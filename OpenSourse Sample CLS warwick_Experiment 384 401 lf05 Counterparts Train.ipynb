{"cells":[{"cell_type":"markdown","metadata":{"id":"-1_HUut4YYm5"},"source":["## This is the official counterparts evaluation script of MIL tasks\n","* Use google colab pro+ (high RAM+GPU)\n","* we use the P100 GPU for the Experiments\n","\n","## The code and Training process along with all record are private\n","* Our github page: https://github.com/sagizty/MIL-SI\n","* The ROSE dataset is not publicly aviliable.\n","* However the MICCAI 2015 chanllenge dataset is avaliable for illustration.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1660558478295,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"},"user_tz":-480},"id":"ZnbrNSoSXFm5","outputId":"ce31fb09-dce9-4327-eb1a-7b8531aacebf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Aug 15 10:14:38 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# check GPU\n","!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":497,"status":"ok","timestamp":1660558478788,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"},"user_tz":-480},"id":"n9GPOn5gcykA","outputId":"1a1e98df-6970-4627-c2da-62d4299d00ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Aug 15 18:14:38 UTC 2022\n"]}],"source":["!date --date='+8 hour'  # CST time zone"]},{"cell_type":"markdown","metadata":{"id":"fbnpeHYUgsJz"},"source":["## Mount Google Drive"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16915,"status":"ok","timestamp":1660558495700,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"},"user_tz":-480},"id":"3obRNrIaffjK","outputId":"67c777dc-ffa0-4304-afe8-519b821014f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"BYevYeMFYmlx"},"source":["## create file-system enviroment\n","* mount your google drive first\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20060,"status":"ok","timestamp":1660558515754,"user":{"displayName":"Rush MSHT","userId":"17678018738220178064"},"user_tz":-480},"id":"ePtQFcQCEPlu","outputId":"dbab1faf-39dc-4ff7-b7b3-b4fb3c6f1cc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Folder Tree Creation completed!\n","Cloning into '/home/MIL_Experiment/code'...\n","warning: redirecting to https://github.com/sagizty/MIL-SI.git/\n","remote: Enumerating objects: 837, done.\u001b[K\n","remote: Counting objects: 100% (39/39), done.\u001b[K\n","remote: Compressing objects: 100% (39/39), done.\u001b[K\n","remote: Total 837 (delta 16), reused 0 (delta 0), pack-reused 798\u001b[K\n","Receiving objects: 100% (837/837), 627.71 MiB | 52.12 MiB/s, done.\n","Resolving deltas: 100% (143/143), done.\n","Checking out files: 100% (624/624), done.\n","code transfer from github completed!\n","data transfer completed!\n"]}],"source":["# create file-system enviroment\n","# mount the google drive first\n","# https://drive.google.com/drive/u/1/my-drive\n","\n","# clear colab path\n","!rm -rf /data\n","!rm -rf /home/MIL_Experiment\n","\n","# create path\n","!mkdir /home/MIL_Experiment\n","!mkdir /home/MIL_Experiment/runs\n","!mkdir /home/MIL_Experiment/code\n","!mkdir /home/MIL_Experiment/saved_models\n","!mkdir /home/MIL_Experiment/imaging_results\n","\n","!mkdir /data\n","!mkdir /data/MIL_Experiment\n","!mkdir /data/MIL_Experiment/dataset\n","\n","print('Folder Tree Creation completed!')\n","\n","# get the latest code from Github MIL-SI official page\n","!git clone https://www.github.com/sagizty/MIL-SI.git /home/MIL_Experiment/code\n","print('code transfer from github completed!')\n","\n","# copy runs if u want to compare\n","# !cp -r /content/drive/MyDrive/MIL_Experiment/runs/* /home/MIL_Experiment/runs\n","# print('tensorboard log transfer completed!')\n","\n","# copy saved_models if u want to compare\n","# !cp -r /content/drive/MyDrive/MIL_Experiment/saved_models/* /home/MIL_Experiment/saved_models\n","# print('saved_models transfer completed!')\n","\n","# get the MIL and CLS dataset from github\n","# by its zip\n","# !cp /home/MIL_Experiment/code/sample_datasets/warwick_MIL.zip /data/MIL_Experiment/dataset/\n","!cp /home/MIL_Experiment/code/sample_datasets/warwick_CLS.zip /data/MIL_Experiment/dataset/\n","# unzip\n","# !unzip -q /data/MIL_Experiment/dataset/warwick_MIL.zip -d /data/MIL_Experiment/dataset/\n","!unzip -q /data/MIL_Experiment/dataset/warwick_CLS.zip -d /data/MIL_Experiment/dataset/\n","\n","# alter the path\n","# !rm -rf /data/MIL_Experiment/dataset/warwick_MIL.zip\n","!rm -rf /data/MIL_Experiment/dataset/warwick_CLS.zip\n","print('data transfer completed!')"]},{"cell_type":"markdown","metadata":{"id":"xLxxHGq_wwwL"},"source":["## Arrange the working enviorment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"K1Yb2b6TGF4r"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/MIL_Experiment/code\n","\u001b[K     |████████████▌                   | 834.1 MB 1.2 MB/s eta 0:17:25tcmalloc: large alloc 1147494400 bytes == 0x38eac000 @  0x7f7317cb7615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |███████████████▉                | 1055.7 MB 137.8 MB/s eta 0:00:08tcmalloc: large alloc 1434370048 bytes == 0x7d502000 @  0x7f7317cb7615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████            | 1336.2 MB 1.2 MB/s eta 0:11:32tcmalloc: large alloc 1792966656 bytes == 0x2334000 @  0x7f7317cb7615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |█████████████████████████▎      | 1691.1 MB 1.2 MB/s eta 0:06:21tcmalloc: large alloc 2241208320 bytes == 0x6d11c000 @  0x7f7317cb7615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x598e3b 0x511f68 0x598e3b 0x511f68 0x598e3b 0x511f68 0x4bc98a 0x532e76 0x594b72 0x515600 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x5118f8 0x593dd7\n","\u001b[K     |████████████████████████████████| 2137.6 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 2137645056 bytes == 0xf2a7e000 @  0x7f7317cb61e7 0x4a3940 0x4a39cc 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9\n","tcmalloc: large alloc 2672058368 bytes == 0x1e65d8000 @  0x7f7317cb7615 0x592b76 0x4df71e 0x59afff 0x515655 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x549576 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576\n","\u001b[K     |████████████████████████████████| 2137.6 MB 379 bytes/s \n","\u001b[K     |████████████████████████████████| 24.5 MB 77.9 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.0+cu111 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0+cu111 which is incompatible.\u001b[0m\n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf\u003c=3.20.1,\u003e=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n","Requirement already satisfied: six\u003e=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf\u003c=3.20.1,\u003e=3.8.0-\u003etensorboardX) (1.15.0)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm==0.5.4\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[K     |████████████████████████████████| 431 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (0.11.1+cu111)\n","Requirement already satisfied: torch\u003e=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.5.4) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=1.4-\u003etimm==0.5.4) (4.1.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm==0.5.4) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.0,\u003e=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-\u003etimm==0.5.4) (7.1.2)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting notifyemail\n","  Downloading notifyemail-1.0.2-py3-none-any.whl (31 kB)\n","Installing collected packages: notifyemail\n","Successfully installed notifyemail-1.0.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Installing collected packages: ttach\n","Successfully installed ttach-0.0.3\n"]}],"source":["# change working dir\n","import os\n","os.chdir(\"/home/MIL_Experiment/code\")\n","!pwd\n","\n","# get packages\n","!pip install -q torch==1.10.0+cu111 torchvision==0.11.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install tensorboardX\n","!pip install timm==0.5.4\n","!pip install notifyemail\n","!pip install ttach\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"87Owjg_pN2yD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.7.13\n"]}],"source":["!python --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"GpEVUWwqK79D"},"outputs":[{"name":"stdout","output_type":"stream","text":["Package                       Version\n","----------------------------- ----------------------------\n","absl-py                       1.2.0\n","aiohttp                       3.8.1\n","aiosignal                     1.2.0\n","alabaster                     0.7.12\n","albumentations                1.2.1\n","altair                        4.2.0\n","appdirs                       1.4.4\n","argon2-cffi                   21.3.0\n","argon2-cffi-bindings          21.2.0\n","arviz                         0.12.1\n","astor                         0.8.1\n","astropy                       4.3.1\n","astunparse                    1.6.3\n","async-timeout                 4.0.2\n","asynctest                     0.13.0\n","atari-py                      0.2.9\n","atomicwrites                  1.4.1\n","attrs                         22.1.0\n","audioread                     2.1.9\n","autograd                      1.4\n","Babel                         2.10.3\n","backcall                      0.2.0\n","beautifulsoup4                4.6.3\n","bleach                        5.0.1\n","blis                          0.7.8\n","bokeh                         2.3.3\n","branca                        0.5.0\n","bs4                           0.0.1\n","CacheControl                  0.12.11\n","cached-property               1.5.2\n","cachetools                    4.2.4\n","catalogue                     2.0.8\n","certifi                       2022.6.15\n","cffi                          1.15.1\n","cftime                        1.6.1\n","chardet                       3.0.4\n","charset-normalizer            2.1.0\n","click                         7.1.2\n","clikit                        0.6.2\n","cloudpickle                   1.3.0\n","cmake                         3.22.6\n","cmdstanpy                     1.0.4\n","colorcet                      3.0.0\n","colorlover                    0.3.0\n","community                     1.0.0b1\n","contextlib2                   0.5.5\n","convertdate                   2.4.0\n","crashtest                     0.3.1\n","crcmod                        1.7\n","cufflinks                     0.17.3\n","cupy-cuda111                  9.4.0\n","cvxopt                        1.3.0\n","cvxpy                         1.2.1\n","cycler                        0.11.0\n","cymem                         2.0.6\n","Cython                        0.29.32\n","daft                          0.0.4\n","dask                          2.12.0\n","datascience                   0.17.5\n","debugpy                       1.0.0\n","decorator                     4.4.2\n","defusedxml                    0.7.1\n","deprecat                      2.1.1\n","descartes                     1.1.0\n","dill                          0.3.5.1\n","distributed                   1.25.3\n","dlib                          19.24.0\n","dm-tree                       0.1.7\n","docutils                      0.17.1\n","dopamine-rl                   1.0.5\n","earthengine-api               0.1.318\n","easydict                      1.9\n","ecos                          2.0.10\n","editdistance                  0.5.3\n","en-core-web-sm                3.4.0\n","entrypoints                   0.4\n","ephem                         4.1.3\n","et-xmlfile                    1.1.0\n","etils                         0.7.1\n","fa2                           0.3.5\n","fastai                        2.7.9\n","fastcore                      1.5.16\n","fastdownload                  0.0.7\n","fastdtw                       0.3.4\n","fastjsonschema                2.16.1\n","fastprogress                  1.0.3\n","fastrlock                     0.8\n","feather-format                0.4.1\n","filelock                      3.7.1\n","firebase-admin                4.4.0\n","fix-yahoo-finance             0.0.22\n","Flask                         1.1.4\n","flatbuffers                   2.0\n","folium                        0.12.1.post1\n","frozenlist                    1.3.1\n","future                        0.16.0\n","gast                          0.5.3\n","GDAL                          2.2.2\n","gdown                         4.4.0\n","gensim                        3.6.0\n","geographiclib                 1.52\n","geopy                         1.17.0\n","gin-config                    0.5.0\n","glob2                         0.7\n","google                        2.0.3\n","google-api-core               1.31.6\n","google-api-python-client      1.12.11\n","google-auth                   1.35.0\n","google-auth-httplib2          0.0.4\n","google-auth-oauthlib          0.4.6\n","google-cloud-bigquery         1.21.0\n","google-cloud-bigquery-storage 1.1.2\n","google-cloud-core             1.0.3\n","google-cloud-datastore        1.8.0\n","google-cloud-firestore        1.7.0\n","google-cloud-language         1.2.0\n","google-cloud-storage          1.18.1\n","google-cloud-translate        1.5.0\n","google-colab                  1.0.0\n","google-pasta                  0.2.0\n","google-resumable-media        0.4.1\n","googleapis-common-protos      1.56.4\n","googledrivedownloader         0.4\n","graphviz                      0.10.1\n","greenlet                      1.1.2\n","grpcio                        1.47.0\n","gspread                       3.4.2\n","gspread-dataframe             3.0.8\n","gym                           0.17.3\n","h5py                          3.1.0\n","HeapDict                      1.0.1\n","hijri-converter               2.2.4\n","holidays                      0.14.2\n","holoviews                     1.14.9\n","html5lib                      1.0.1\n","httpimport                    0.5.18\n","httplib2                      0.17.4\n","httplib2shim                  0.0.3\n","httpstan                      4.6.1\n","humanize                      0.5.1\n","hyperopt                      0.1.2\n","idna                          2.10\n","imageio                       2.9.0\n","imagesize                     1.4.1\n","imbalanced-learn              0.8.1\n","imblearn                      0.0\n","imgaug                        0.4.0\n","importlib-metadata            4.12.0\n","importlib-resources           5.9.0\n","imutils                       0.5.4\n","inflect                       2.1.0\n","intel-openmp                  2022.1.0\n","intervaltree                  2.1.0\n","ipykernel                     4.10.1\n","ipython                       5.5.0\n","ipython-genutils              0.2.0\n","ipython-sql                   0.3.9\n","ipywidgets                    7.7.1\n","itsdangerous                  1.1.0\n","jax                           0.3.14\n","jaxlib                        0.3.14+cuda11.cudnn805\n","jedi                          0.18.1\n","jieba                         0.42.1\n","Jinja2                        2.11.3\n","joblib                        1.1.0\n","jpeg4py                       0.1.4\n","jsonschema                    4.3.3\n","jupyter                       1.0.0\n","jupyter-client                5.3.5\n","jupyter-console               5.2.0\n","jupyter-core                  4.11.1\n","jupyterlab-pygments           0.2.2\n","jupyterlab-widgets            1.1.1\n","kaggle                        1.5.12\n","kapre                         0.3.7\n","keras                         2.8.0\n","Keras-Preprocessing           1.1.2\n","keras-vis                     0.4.1\n","kiwisolver                    1.4.4\n","korean-lunar-calendar         0.2.1\n","langcodes                     3.3.0\n","libclang                      14.0.6\n","librosa                       0.8.1\n","lightgbm                      2.2.3\n","llvmlite                      0.39.0\n","lmdb                          0.99\n","LunarCalendar                 0.0.9\n","lxml                          4.9.1\n","Markdown                      3.4.1\n","MarkupSafe                    2.0.1\n","marshmallow                   3.17.0\n","matplotlib                    3.2.2\n","matplotlib-inline             0.1.3\n","matplotlib-venn               0.11.7\n","missingno                     0.5.1\n","mistune                       0.8.4\n","mizani                        0.6.0\n","mkl                           2019.0\n","mlxtend                       0.14.0\n","more-itertools                8.14.0\n","moviepy                       0.2.3.5\n","mpmath                        1.2.1\n","msgpack                       1.0.4\n","multidict                     6.0.2\n","multitasking                  0.0.11\n","murmurhash                    1.0.7\n","music21                       5.5.0\n","natsort                       5.5.0\n","nbclient                      0.6.6\n","nbconvert                     5.6.1\n","nbformat                      5.4.0\n","nest-asyncio                  1.5.5\n","netCDF4                       1.6.0\n","networkx                      2.6.3\n","nibabel                       3.0.2\n","nltk                          3.7\n","notebook                      5.3.1\n","notifyemail                   1.0.2\n","numba                         0.56.0\n","numexpr                       2.8.3\n","numpy                         1.21.6\n","oauth2client                  4.1.3\n","oauthlib                      3.2.0\n","okgrade                       0.4.3\n","opencv-contrib-python         4.6.0.66\n","opencv-python                 4.6.0.66\n","opencv-python-headless        4.6.0.66\n","openpyxl                      3.0.10\n","opt-einsum                    3.3.0\n","osqp                          0.6.2.post0\n","packaging                     21.3\n","palettable                    3.3.0\n","pandas                        1.3.5\n","pandas-datareader             0.9.0\n","pandas-gbq                    0.13.3\n","pandas-profiling              1.4.1\n","pandocfilters                 1.5.0\n","panel                         0.12.1\n","param                         1.12.2\n","parso                         0.8.3\n","pastel                        0.2.1\n","pathlib                       1.0.1\n","pathy                         0.6.2\n","patsy                         0.5.2\n","pep517                        0.13.0\n","pexpect                       4.8.0\n","pickleshare                   0.7.5\n","Pillow                        7.1.2\n","pip                           21.1.3\n","pip-tools                     6.2.0\n","pkgutil-resolve-name          1.3.10\n","plotly                        5.5.0\n","plotnine                      0.6.0\n","pluggy                        0.7.1\n","pooch                         1.6.0\n","portpicker                    1.3.9\n","prefetch-generator            1.0.1\n","preshed                       3.0.6\n","prettytable                   3.3.0\n","progressbar2                  3.38.0\n","prometheus-client             0.14.1\n","promise                       2.3\n","prompt-toolkit                1.0.18\n","prophet                       1.1\n","protobuf                      3.17.3\n","psutil                        5.4.8\n","psycopg2                      2.9.3\n","ptyprocess                    0.7.0\n","py                            1.11.0\n","pyarrow                       6.0.1\n","pyasn1                        0.4.8\n","pyasn1-modules                0.2.8\n","pycocotools                   2.0.4\n","pycparser                     2.21\n","pyct                          0.4.8\n","pydantic                      1.9.1\n","pydata-google-auth            1.4.0\n","pydot                         1.3.0\n","pydot-ng                      2.0.0\n","pydotplus                     2.0.2\n","PyDrive                       1.3.1\n","pyemd                         0.5.1\n","pyerfa                        2.0.0.1\n","pyglet                        1.5.0\n","Pygments                      2.6.1\n","pygobject                     3.26.1\n","pylev                         1.4.0\n","pymc3                         3.11.5\n","PyMeeus                       0.5.11\n","pymongo                       4.2.0\n","pymystem3                     0.2.0\n","PyOpenGL                      3.1.6\n","pyparsing                     3.0.9\n","pyrsistent                    0.18.1\n","pysimdjson                    3.2.0\n","pysndfile                     1.3.8\n","PySocks                       1.7.1\n","pystan                        3.3.0\n","pytest                        3.6.4\n","python-apt                    0.0.0\n","python-chess                  0.23.11\n","python-dateutil               2.8.2\n","python-louvain                0.16\n","python-slugify                6.1.2\n","python-utils                  3.3.3\n","pytz                          2022.1\n","pyviz-comms                   2.2.0\n","PyWavelets                    1.3.0\n","PyYAML                        3.13\n","pyzmq                         23.2.0\n","qdldl                         0.1.5.post2\n","qtconsole                     5.3.1\n","QtPy                          2.2.0\n","qudida                        0.0.4\n","regex                         2022.6.2\n","requests                      2.23.0\n","requests-oauthlib             1.3.1\n","resampy                       0.4.0\n","rpy2                          3.4.5\n","rsa                           4.9\n","scikit-image                  0.18.3\n","scikit-learn                  1.0.2\n","scipy                         1.7.3\n","screen-resolution-extra       0.0.0\n","scs                           3.2.0\n","seaborn                       0.11.2\n","semver                        2.13.0\n","Send2Trash                    1.8.0\n","setuptools                    57.4.0\n","setuptools-git                1.2\n","Shapely                       1.8.2\n","simplegeneric                 0.8.1\n","six                           1.15.0\n","sklearn                       0.0\n","sklearn-pandas                1.8.0\n","smart-open                    5.2.1\n","snowballstemmer               2.2.0\n","sortedcontainers              2.4.0\n","SoundFile                     0.10.3.post1\n","soupsieve                     2.3.2.post1\n","spacy                         3.4.1\n","spacy-legacy                  3.0.9\n","spacy-loggers                 1.0.3\n","Sphinx                        1.8.6\n","sphinxcontrib-serializinghtml 1.1.5\n","sphinxcontrib-websupport      1.2.4\n","SQLAlchemy                    1.4.40\n","sqlparse                      0.4.2\n","srsly                         2.4.4\n","statsmodels                   0.10.2\n","sympy                         1.7.1\n","tables                        3.7.0\n","tabulate                      0.8.10\n","tblib                         1.7.0\n","tenacity                      8.0.1\n","tensorboard                   2.8.0\n","tensorboard-data-server       0.6.1\n","tensorboard-plugin-wit        1.8.1\n","tensorboardX                  2.5.1\n","tensorflow                    2.8.2+zzzcolab20220719082949\n","tensorflow-datasets           4.6.0\n","tensorflow-estimator          2.8.0\n","tensorflow-gcs-config         2.8.0\n","tensorflow-hub                0.12.0\n","tensorflow-io-gcs-filesystem  0.26.0\n","tensorflow-metadata           1.9.0\n","tensorflow-probability        0.16.0\n","termcolor                     1.1.0\n","terminado                     0.13.3\n","testpath                      0.6.0\n","text-unidecode                1.3\n","textblob                      0.15.3\n","Theano-PyMC                   1.1.2\n","thinc                         8.1.0\n","threadpoolctl                 3.1.0\n","tifffile                      2021.11.2\n","timm                          0.5.4\n","tinycss2                      1.1.1\n","toml                          0.10.2\n","tomli                         2.0.1\n","toolz                         0.12.0\n","torch                         1.10.0+cu111\n","torchaudio                    0.12.1+cu113\n","torchsummary                  1.5.1\n","torchtext                     0.13.1\n","torchvision                   0.11.1+cu111\n","tornado                       5.1.1\n","tqdm                          4.64.0\n","traitlets                     5.1.1\n","ttach                         0.0.3\n","tweepy                        3.10.0\n","typeguard                     2.7.1\n","typer                         0.4.2\n","typing-extensions             4.1.1\n","tzlocal                       1.5.1\n","ujson                         5.4.0\n","uritemplate                   3.0.1\n","urllib3                       1.24.3\n","vega-datasets                 0.9.0\n","wasabi                        0.10.1\n","wcwidth                       0.2.5\n","webargs                       8.2.0\n","webencodings                  0.5.1\n","Werkzeug                      1.0.1\n","wheel                         0.37.1\n","widgetsnbextension            3.6.1\n","wordcloud                     1.8.2.2\n","wrapt                         1.14.1\n","xarray                        0.20.2\n","xarray-einstats               0.2.2\n","xgboost                       0.90\n","xkit                          0.0.0\n","xlrd                          1.1.0\n","xlwt                          1.3.0\n","yarl                          1.8.1\n","yellowbrick                   1.4\n","zict                          2.2.0\n","zipp                          3.8.1\n"]}],"source":["!pip list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PO-IkD6FIO7B"},"outputs":[],"source":["!pip freeze\u003erequirements.txt\n","!cp requirements.txt ../runs"]},{"cell_type":"markdown","metadata":{"id":"h31KAx1ZZEl9"},"source":["## Start Training\n","* by command line\n","* use argparse to set down hyper-parameter\n","\n","* 5-fold experiment is used here"]},{"cell_type":"markdown","metadata":{"id":"--aldMsHOZkP"},"source":["# CLS counterparts"]},{"cell_type":"markdown","metadata":{"id":"QqeBMVh6OjTu"},"source":["* Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fxJcLMKMYeDt"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.7529330253601074\n","minibatch AVG loss: 0.645060946047306\n","Epoch: 1     train index of 5 minibatch: 2      time used: 1.4934320449829102\n","minibatch AVG loss: 1.090901281684637\n","Epoch: 1     train index of 5 minibatch: 3      time used: 1.4927189350128174\n","minibatch AVG loss: 0.5039490010589361\n","\n","Epoch: 1  train \n","Loss: 0.6510  Acc: 72.4638\n","benign precision: 70.0000  recall: 70.0000\n","benign sensitivity: 70.0000  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 76.3158\n","benign TP: 21.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 9.0\n","malignant precision: 76.3158  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 70.0000\n","malignant FPR: 30.0000  NPV: 70.0000\n","malignant TP: 29.0\n","malignant TN: 21.0\n","malignant FP: 9.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.3703  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 1.7119243144989014\n","minibatch AVG loss: 0.053611745871603486\n","Epoch: 2     train index of 5 minibatch: 2      time used: 1.4928863048553467\n","minibatch AVG loss: 0.10996816605329514\n","Epoch: 2     train index of 5 minibatch: 3      time used: 1.4940071105957031\n","minibatch AVG loss: 0.03212673533707857\n","\n","Epoch: 2  train \n","Loss: 0.0580  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.2278  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 1.710463285446167\n","minibatch AVG loss: 0.01743149016983807\n","Epoch: 3     train index of 5 minibatch: 2      time used: 1.4937219619750977\n","minibatch AVG loss: 0.06186955072917044\n","Epoch: 3     train index of 5 minibatch: 3      time used: 1.4928309917449951\n","minibatch AVG loss: 0.03850978924892843\n","\n","Epoch: 3  train \n","Loss: 0.0354  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.0983  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 1.7039539813995361\n","minibatch AVG loss: 0.008715494105126708\n","Epoch: 4     train index of 5 minibatch: 2      time used: 1.4930601119995117\n","minibatch AVG loss: 0.0140923845639918\n","Epoch: 4     train index of 5 minibatch: 3      time used: 1.492661952972412\n","minibatch AVG loss: 0.0020712055717012844\n","\n","Epoch: 4  train \n","Loss: 0.0072  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0858  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 1.7106842994689941\n","minibatch AVG loss: 0.0012363259447738528\n","Epoch: 5     train index of 5 minibatch: 2      time used: 1.4927282333374023\n","minibatch AVG loss: 0.006495520228054374\n","Epoch: 5     train index of 5 minibatch: 3      time used: 1.4936199188232422\n","minibatch AVG loss: 0.001918604224920273\n","\n","Epoch: 5  train \n","Loss: 0.0043  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0715  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.7088086605072021\n","minibatch AVG loss: 0.0023184463192592376\n","Epoch: 6     train index of 5 minibatch: 2      time used: 1.4938430786132812\n","minibatch AVG loss: 0.003119121382769663\n","Epoch: 6     train index of 5 minibatch: 3      time used: 1.4929485321044922\n","minibatch AVG loss: 0.0029100811007083394\n","\n","Epoch: 6  train \n","Loss: 0.0029  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.0923  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.709662675857544\n","minibatch AVG loss: 0.005758618966501672\n","Epoch: 7     train index of 5 minibatch: 2      time used: 1.4926385879516602\n","minibatch AVG loss: 0.0028180498076835645\n","Epoch: 7     train index of 5 minibatch: 3      time used: 1.492868423461914\n","minibatch AVG loss: 0.00042516662215348334\n","\n","Epoch: 7  train \n","Loss: 0.0027  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0580  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.7162673473358154\n","minibatch AVG loss: 0.0002810498626786284\n","Epoch: 8     train index of 5 minibatch: 2      time used: 1.4938979148864746\n","minibatch AVG loss: 0.0006836927786935121\n","Epoch: 8     train index of 5 minibatch: 3      time used: 1.4928603172302246\n","minibatch AVG loss: 0.0010162418300751596\n","\n","Epoch: 8  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.0576  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.704082727432251\n","minibatch AVG loss: 0.001530085373815382\n","Epoch: 9     train index of 5 minibatch: 2      time used: 1.494816541671753\n","minibatch AVG loss: 0.00021408343309303746\n","Epoch: 9     train index of 5 minibatch: 3      time used: 1.492159128189087\n","minibatch AVG loss: 0.010641641536494717\n","\n","Epoch: 9  train \n","Loss: 0.0036  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.0761  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.7084920406341553\n","minibatch AVG loss: 0.0016254443020443433\n","Epoch: 10     train index of 5 minibatch: 2      time used: 1.4917888641357422\n","minibatch AVG loss: 0.015934287069831042\n","Epoch: 10     train index of 5 minibatch: 3      time used: 1.491919755935669\n","minibatch AVG loss: 0.0013327601733180926\n","\n","Epoch: 10  train \n","Loss: 0.0055  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0427  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.7079617977142334\n","minibatch AVG loss: 0.003039122171685449\n","Epoch: 11     train index of 5 minibatch: 2      time used: 1.4927003383636475\n","minibatch AVG loss: 0.0006610413649468683\n","Epoch: 11     train index of 5 minibatch: 3      time used: 1.4925789833068848\n","minibatch AVG loss: 0.0004532920705969445\n","\n","Epoch: 11  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.0393  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.705495834350586\n","minibatch AVG loss: 0.00047092371969483795\n","Epoch: 12     train index of 5 minibatch: 2      time used: 1.4936203956604004\n","minibatch AVG loss: 0.0004266224539605901\n","Epoch: 12     train index of 5 minibatch: 3      time used: 1.4917333126068115\n","minibatch AVG loss: 0.00044331201643217356\n","\n","Epoch: 12  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0454  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.7041082382202148\n","minibatch AVG loss: 0.0002929111746198032\n","Epoch: 13     train index of 5 minibatch: 2      time used: 1.4929323196411133\n","minibatch AVG loss: 0.0008412417460931465\n","Epoch: 13     train index of 5 minibatch: 3      time used: 1.492621660232544\n","minibatch AVG loss: 0.00023737422889098526\n","\n","Epoch: 13  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.0480  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.7094295024871826\n","minibatch AVG loss: 0.0005320474214386195\n","Epoch: 14     train index of 5 minibatch: 2      time used: 1.4923021793365479\n","minibatch AVG loss: 0.00022880917858856265\n","Epoch: 14     train index of 5 minibatch: 3      time used: 1.4920439720153809\n","minibatch AVG loss: 0.0010033941158326342\n","\n","Epoch: 14  train \n","Loss: 0.0005  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0473  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.7086334228515625\n","minibatch AVG loss: 0.0005020555341616273\n","Epoch: 15     train index of 5 minibatch: 2      time used: 1.493154764175415\n","minibatch AVG loss: 0.004452934832079336\n","Epoch: 15     train index of 5 minibatch: 3      time used: 1.4923357963562012\n","minibatch AVG loss: 0.0029187670297687873\n","\n","Epoch: 15  train \n","Loss: 0.0024  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.0677  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.71146559715271\n","minibatch AVG loss: 0.00020047533907927573\n","Epoch: 16     train index of 5 minibatch: 2      time used: 1.4914827346801758\n","minibatch AVG loss: 0.0002625220295158215\n","Epoch: 16     train index of 5 minibatch: 3      time used: 1.4921526908874512\n","minibatch AVG loss: 0.00021777751608169637\n","\n","Epoch: 16  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.0797  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.7010219097137451\n","minibatch AVG loss: 0.0005408095792517997\n","Epoch: 17     train index of 5 minibatch: 2      time used: 1.49184250831604\n","minibatch AVG loss: 0.001555675398412859\n","Epoch: 17     train index of 5 minibatch: 3      time used: 1.492065668106079\n","minibatch AVG loss: 0.00016561801749048755\n","\n","Epoch: 17  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.0610  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 1.7018601894378662\n","minibatch AVG loss: 0.00015781775146024303\n","Epoch: 18     train index of 5 minibatch: 2      time used: 1.4923484325408936\n","minibatch AVG loss: 0.00045509452029364185\n","Epoch: 18     train index of 5 minibatch: 3      time used: 1.491471290588379\n","minibatch AVG loss: 0.0006012871919665486\n","\n","Epoch: 18  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.0550  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.7023954391479492\n","minibatch AVG loss: 0.00020205051259836182\n","Epoch: 19     train index of 5 minibatch: 2      time used: 1.491807222366333\n","minibatch AVG loss: 0.00012503972029662692\n","Epoch: 19     train index of 5 minibatch: 3      time used: 1.4935696125030518\n","minibatch AVG loss: 0.00022437609732151031\n","\n","Epoch: 19  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0576  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.7053217887878418\n","minibatch AVG loss: 0.00025195209545927357\n","Epoch: 20     train index of 5 minibatch: 2      time used: 1.4924561977386475\n","minibatch AVG loss: 0.0006892103891004808\n","Epoch: 20     train index of 5 minibatch: 3      time used: 1.4920682907104492\n","minibatch AVG loss: 0.0002528816527046729\n","\n","Epoch: 20  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0605  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.7150707244873047\n","minibatch AVG loss: 0.00015650957175239455\n","Epoch: 21     train index of 5 minibatch: 2      time used: 1.4922866821289062\n","minibatch AVG loss: 0.0003976772297392017\n","Epoch: 21     train index of 5 minibatch: 3      time used: 1.4937994480133057\n","minibatch AVG loss: 0.0002338427002541721\n","\n","Epoch: 21  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0603  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.7038047313690186\n","minibatch AVG loss: 0.00022147541021695362\n","Epoch: 22     train index of 5 minibatch: 2      time used: 1.492913007736206\n","minibatch AVG loss: 0.00028211933677084743\n","Epoch: 22     train index of 5 minibatch: 3      time used: 1.4912574291229248\n","minibatch AVG loss: 0.00017423856479581445\n","\n","Epoch: 22  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0619  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.7055742740631104\n","minibatch AVG loss: 0.00035944665942224676\n","Epoch: 23     train index of 5 minibatch: 2      time used: 1.491797685623169\n","minibatch AVG loss: 0.0005539001140277833\n","Epoch: 23     train index of 5 minibatch: 3      time used: 1.4920742511749268\n","minibatch AVG loss: 7.739789689367171e-05\n","\n","Epoch: 23  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0656  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.702054738998413\n","minibatch AVG loss: 0.00015238970263453666\n","Epoch: 24     train index of 5 minibatch: 2      time used: 1.4913547039031982\n","minibatch AVG loss: 0.0006207098667800892\n","Epoch: 24     train index of 5 minibatch: 3      time used: 1.4932835102081299\n","minibatch AVG loss: 0.0012565810706291814\n","\n","Epoch: 24  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0659  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.7095928192138672\n","minibatch AVG loss: 0.00030656176095362754\n","Epoch: 25     train index of 5 minibatch: 2      time used: 1.4919815063476562\n","minibatch AVG loss: 0.00015419511255458928\n","Epoch: 25     train index of 5 minibatch: 3      time used: 1.4936635494232178\n","minibatch AVG loss: 0.0004585704609780805\n","\n","Epoch: 25  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0697  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.704406499862671\n","minibatch AVG loss: 8.44198224513093e-05\n","Epoch: 26     train index of 5 minibatch: 2      time used: 1.4935822486877441\n","minibatch AVG loss: 0.0003625401615863666\n","Epoch: 26     train index of 5 minibatch: 3      time used: 1.494685173034668\n","minibatch AVG loss: 0.00015825075934117194\n","\n","Epoch: 26  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0686  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.7039821147918701\n","minibatch AVG loss: 0.00020114707003813237\n","Epoch: 27     train index of 5 minibatch: 2      time used: 1.4929933547973633\n","minibatch AVG loss: 0.00031443576299352574\n","Epoch: 27     train index of 5 minibatch: 3      time used: 1.49379301071167\n","minibatch AVG loss: 0.00027824589633382855\n","\n","Epoch: 27  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0625  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.7068374156951904\n","minibatch AVG loss: 0.0003192219461197965\n","Epoch: 28     train index of 5 minibatch: 2      time used: 1.4924664497375488\n","minibatch AVG loss: 9.050205790117616e-05\n","Epoch: 28     train index of 5 minibatch: 3      time used: 1.4932825565338135\n","minibatch AVG loss: 0.0008390042683458887\n","\n","Epoch: 28  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0631  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 1.704756498336792\n","minibatch AVG loss: 0.00011913350972463377\n","Epoch: 29     train index of 5 minibatch: 2      time used: 1.4927287101745605\n","minibatch AVG loss: 0.00022824033949291332\n","Epoch: 29     train index of 5 minibatch: 3      time used: 1.4922080039978027\n","minibatch AVG loss: 0.00013121515803504735\n","\n","Epoch: 29  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0657  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.7076845169067383\n","minibatch AVG loss: 0.00018415132872178218\n","Epoch: 30     train index of 5 minibatch: 2      time used: 1.4938113689422607\n","minibatch AVG loss: 0.0001972855578060262\n","Epoch: 30     train index of 5 minibatch: 3      time used: 1.4928216934204102\n","minibatch AVG loss: 0.00025769111307454294\n","\n","Epoch: 30  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0677  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.7062180042266846\n","minibatch AVG loss: 0.00027590239624259995\n","Epoch: 31     train index of 5 minibatch: 2      time used: 1.4918818473815918\n","minibatch AVG loss: 0.00026860168582061307\n","Epoch: 31     train index of 5 minibatch: 3      time used: 1.4925503730773926\n","minibatch AVG loss: 0.00018493202878744341\n","\n","Epoch: 31  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0677  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.704869270324707\n","minibatch AVG loss: 0.00018250093344249763\n","Epoch: 32     train index of 5 minibatch: 2      time used: 1.4920237064361572\n","minibatch AVG loss: 0.00023402605402225162\n","Epoch: 32     train index of 5 minibatch: 3      time used: 1.4930663108825684\n","minibatch AVG loss: 0.00028937829338246954\n","\n","Epoch: 32  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0668  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.7076964378356934\n","minibatch AVG loss: 0.00018420508422423154\n","Epoch: 33     train index of 5 minibatch: 2      time used: 1.4918527603149414\n","minibatch AVG loss: 0.00011108024809800555\n","Epoch: 33     train index of 5 minibatch: 3      time used: 1.4931824207305908\n","minibatch AVG loss: 0.0004761909309308976\n","\n","Epoch: 33  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0679  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 1.7041914463043213\n","minibatch AVG loss: 0.0001846356797614135\n","Epoch: 34     train index of 5 minibatch: 2      time used: 1.4923620223999023\n","minibatch AVG loss: 0.0016317767585860565\n","Epoch: 34     train index of 5 minibatch: 3      time used: 1.4929609298706055\n","minibatch AVG loss: 0.0001321561969234608\n","\n","Epoch: 34  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0830  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.701329231262207\n","minibatch AVG loss: 0.00018543310106906575\n","Epoch: 35     train index of 5 minibatch: 2      time used: 1.491544485092163\n","minibatch AVG loss: 0.0016002368734916671\n","Epoch: 35     train index of 5 minibatch: 3      time used: 1.4936363697052002\n","minibatch AVG loss: 0.0003634712362327264\n","\n","Epoch: 35  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0765  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.7004756927490234\n","minibatch AVG loss: 0.0005293931964843069\n","Epoch: 36     train index of 5 minibatch: 2      time used: 1.4920940399169922\n","minibatch AVG loss: 0.00015263754394254648\n","Epoch: 36     train index of 5 minibatch: 3      time used: 1.4920027256011963\n","minibatch AVG loss: 0.0003370279695445788\n","\n","Epoch: 36  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0759  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.7028555870056152\n","minibatch AVG loss: 0.00018084668345181855\n","Epoch: 37     train index of 5 minibatch: 2      time used: 1.492447853088379\n","minibatch AVG loss: 7.257425932039041e-05\n","Epoch: 37     train index of 5 minibatch: 3      time used: 1.4925494194030762\n","minibatch AVG loss: 0.00021640306076733395\n","\n","Epoch: 37  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0757  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 1.7065134048461914\n","minibatch AVG loss: 0.0002018480190599803\n","Epoch: 38     train index of 5 minibatch: 2      time used: 1.493410348892212\n","minibatch AVG loss: 0.00010387071342847776\n","Epoch: 38     train index of 5 minibatch: 3      time used: 1.4932861328125\n","minibatch AVG loss: 0.0003766395806451328\n","\n","Epoch: 38  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0756  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.7021517753601074\n","minibatch AVG loss: 6.767596496501938e-05\n","Epoch: 39     train index of 5 minibatch: 2      time used: 1.4924776554107666\n","minibatch AVG loss: 0.00038897224876563994\n","Epoch: 39     train index of 5 minibatch: 3      time used: 1.4910805225372314\n","minibatch AVG loss: 0.00019126691840938294\n","\n","Epoch: 39  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0757  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.704197883605957\n","minibatch AVG loss: 0.00010999498845194466\n","Epoch: 40     train index of 5 minibatch: 2      time used: 1.4927477836608887\n","minibatch AVG loss: 0.00017538042011437937\n","Epoch: 40     train index of 5 minibatch: 3      time used: 1.4927327632904053\n","minibatch AVG loss: 0.0003720126231200993\n","\n","Epoch: 40  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0764  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 1.703507661819458\n","minibatch AVG loss: 0.00022948388213990257\n","Epoch: 41     train index of 5 minibatch: 2      time used: 1.4921875\n","minibatch AVG loss: 0.0003923806143575348\n","Epoch: 41     train index of 5 minibatch: 3      time used: 1.4924447536468506\n","minibatch AVG loss: 0.00018453636803315022\n","\n","Epoch: 41  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0763  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.7106525897979736\n","minibatch AVG loss: 0.0002766609204627457\n","Epoch: 42     train index of 5 minibatch: 2      time used: 1.492971420288086\n","minibatch AVG loss: 0.0001912407187774079\n","Epoch: 42     train index of 5 minibatch: 3      time used: 1.4927537441253662\n","minibatch AVG loss: 0.0001211201015394181\n","\n","Epoch: 42  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0753  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.7070965766906738\n","minibatch AVG loss: 0.00019394414557609706\n","Epoch: 43     train index of 5 minibatch: 2      time used: 1.4929313659667969\n","minibatch AVG loss: 0.0007439907727530226\n","Epoch: 43     train index of 5 minibatch: 3      time used: 1.494682788848877\n","minibatch AVG loss: 0.00012719215010292828\n","\n","Epoch: 43  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0753  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.702021598815918\n","minibatch AVG loss: 0.00025339865387650205\n","Epoch: 44     train index of 5 minibatch: 2      time used: 1.492666244506836\n","minibatch AVG loss: 0.00044192466448294\n","Epoch: 44     train index of 5 minibatch: 3      time used: 1.4923107624053955\n","minibatch AVG loss: 0.00016364179464289919\n","\n","Epoch: 44  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0748  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.7047107219696045\n","minibatch AVG loss: 0.00022538180310220922\n","Epoch: 45     train index of 5 minibatch: 2      time used: 1.4933545589447021\n","minibatch AVG loss: 8.888668453437276e-05\n","Epoch: 45     train index of 5 minibatch: 3      time used: 1.4924123287200928\n","minibatch AVG loss: 0.00018211003698525018\n","\n","Epoch: 45  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0744  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.7015609741210938\n","minibatch AVG loss: 0.00019161581367370671\n","Epoch: 46     train index of 5 minibatch: 2      time used: 1.4920589923858643\n","minibatch AVG loss: 0.0001052818577591097\n","Epoch: 46     train index of 5 minibatch: 3      time used: 1.493922233581543\n","minibatch AVG loss: 0.0003388492899830453\n","\n","Epoch: 46  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0737  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.7041163444519043\n","minibatch AVG loss: 0.0001544598868349567\n","Epoch: 47     train index of 5 minibatch: 2      time used: 1.493018627166748\n","minibatch AVG loss: 0.0003284434555098414\n","Epoch: 47     train index of 5 minibatch: 3      time used: 1.493656873703003\n","minibatch AVG loss: 9.255586373910774e-05\n","\n","Epoch: 47  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0734  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.7028169631958008\n","minibatch AVG loss: 0.00013114366174704628\n","Epoch: 48     train index of 5 minibatch: 2      time used: 1.4939205646514893\n","minibatch AVG loss: 0.00017542276691528969\n","Epoch: 48     train index of 5 minibatch: 3      time used: 1.4923017024993896\n","minibatch AVG loss: 0.00017604658569325693\n","\n","Epoch: 48  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0728  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.709754228591919\n","minibatch AVG loss: 0.00020512169576250016\n","Epoch: 49     train index of 5 minibatch: 2      time used: 1.493248462677002\n","minibatch AVG loss: 0.0004079985257703811\n","Epoch: 49     train index of 5 minibatch: 3      time used: 1.4938511848449707\n","minibatch AVG loss: 0.0003696130879689008\n","\n","Epoch: 49  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0729  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.7106115818023682\n","minibatch AVG loss: 0.0003618498805735726\n","Epoch: 50     train index of 5 minibatch: 2      time used: 1.4933326244354248\n","minibatch AVG loss: 0.00025114599775406533\n","Epoch: 50     train index of 5 minibatch: 3      time used: 1.4916822910308838\n","minibatch AVG loss: 0.0002971948299091309\n","\n","Epoch: 50  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0726  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 4m 55s\n","Best epoch idx:  14\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QXuVfzuAZ7IB"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name='Cutout', backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.7363812923431396\n","minibatch AVG loss: 0.5541207477450371\n","Epoch: 1     train index of 5 minibatch: 2      time used: 1.4943807125091553\n","minibatch AVG loss: 1.5625587593764068\n","Epoch: 1     train index of 5 minibatch: 3      time used: 1.4962098598480225\n","minibatch AVG loss: 0.43791491985321046\n","\n","Epoch: 1  train \n","Loss: 0.7454  Acc: 75.3623\n","benign precision: 73.3333  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 78.9474\n","benign FPR: 21.0526  NPV: 78.9474\n","benign TP: 22.0\n","benign TN: 30.0\n","benign FP: 8.0\n","benign FN: 8.0\n","malignant precision: 78.9474  recall: 78.9474\n","malignant sensitivity: 78.9474  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 73.3333\n","malignant TP: 30.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.7400  Acc: 75.0000\n","benign precision: 100.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 69.2308\n","benign TP: 3.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 69.2308  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 1.7063603401184082\n","minibatch AVG loss: 0.3058615494519472\n","Epoch: 2     train index of 5 minibatch: 2      time used: 1.4962413311004639\n","minibatch AVG loss: 0.14893070347607135\n","Epoch: 2     train index of 5 minibatch: 3      time used: 1.4950034618377686\n","minibatch AVG loss: 0.1291358015500009\n","\n","Epoch: 2  train \n","Loss: 0.1700  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.3130  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 1.710968017578125\n","minibatch AVG loss: 0.2540212205844\n","Epoch: 3     train index of 5 minibatch: 2      time used: 1.4948406219482422\n","minibatch AVG loss: 0.07464398224838079\n","Epoch: 3     train index of 5 minibatch: 3      time used: 1.4947504997253418\n","minibatch AVG loss: 0.17438411843031645\n","\n","Epoch: 3  train \n","Loss: 0.1483  Acc: 91.3043\n","benign precision: 90.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 94.7368\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 90.0000\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.1645  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 1.7068276405334473\n","minibatch AVG loss: 0.04048201516052359\n","Epoch: 4     train index of 5 minibatch: 2      time used: 1.494997262954712\n","minibatch AVG loss: 0.020550731869298032\n","Epoch: 4     train index of 5 minibatch: 3      time used: 1.4957785606384277\n","minibatch AVG loss: 0.01794472312903963\n","\n","Epoch: 4  train \n","Loss: 0.0229  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0896  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 1.71126389503479\n","minibatch AVG loss: 0.001307731939596124\n","Epoch: 5     train index of 5 minibatch: 2      time used: 1.4959142208099365\n","minibatch AVG loss: 0.009143992341705598\n","Epoch: 5     train index of 5 minibatch: 3      time used: 1.4952945709228516\n","minibatch AVG loss: 0.009370282548479736\n","\n","Epoch: 5  train \n","Loss: 0.0117  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0801  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.7102725505828857\n","minibatch AVG loss: 0.009638831578195095\n","Epoch: 6     train index of 5 minibatch: 2      time used: 1.4956941604614258\n","minibatch AVG loss: 0.003686896173894638\n","Epoch: 6     train index of 5 minibatch: 3      time used: 1.4955253601074219\n","minibatch AVG loss: 0.005522286901032203\n","\n","Epoch: 6  train \n","Loss: 0.0107  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.1593  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.7153675556182861\n","minibatch AVG loss: 0.016800837707341996\n","Epoch: 7     train index of 5 minibatch: 2      time used: 1.4950673580169678\n","minibatch AVG loss: 0.002538238249690039\n","Epoch: 7     train index of 5 minibatch: 3      time used: 1.4950196743011475\n","minibatch AVG loss: 0.01406942803878337\n","\n","Epoch: 7  train \n","Loss: 0.0105  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0648  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.71126389503479\n","minibatch AVG loss: 0.00024183314126275947\n","Epoch: 8     train index of 5 minibatch: 2      time used: 1.4946751594543457\n","minibatch AVG loss: 0.0033871892694151027\n","Epoch: 8     train index of 5 minibatch: 3      time used: 1.4959979057312012\n","minibatch AVG loss: 0.12100553414493334\n","\n","Epoch: 8  train \n","Loss: 0.0362  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.1082  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.7116451263427734\n","minibatch AVG loss: 0.008077786147259758\n","Epoch: 9     train index of 5 minibatch: 2      time used: 1.4955649375915527\n","minibatch AVG loss: 0.185902646521572\n","Epoch: 9     train index of 5 minibatch: 3      time used: 1.4950473308563232\n","minibatch AVG loss: 0.0038032948563341052\n","\n","Epoch: 9  train \n","Loss: 0.0578  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.3954  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.7152643203735352\n","minibatch AVG loss: 0.15816194683575305\n","Epoch: 10     train index of 5 minibatch: 2      time used: 1.49650239944458\n","minibatch AVG loss: 0.02466010792122688\n","Epoch: 10     train index of 5 minibatch: 3      time used: 1.4959137439727783\n","minibatch AVG loss: 0.13264822154305875\n","\n","Epoch: 10  train \n","Loss: 0.0915  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0228  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.7090468406677246\n","minibatch AVG loss: 0.0009370587176817935\n","Epoch: 11     train index of 5 minibatch: 2      time used: 1.4955766201019287\n","minibatch AVG loss: 0.00011830074508907273\n","Epoch: 11     train index of 5 minibatch: 3      time used: 1.494109869003296\n","minibatch AVG loss: 0.003084427787689492\n","\n","Epoch: 11  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.1559  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.7116127014160156\n","minibatch AVG loss: 0.0946250865032198\n","Epoch: 12     train index of 5 minibatch: 2      time used: 1.4945216178894043\n","minibatch AVG loss: 0.004005663085263222\n","Epoch: 12     train index of 5 minibatch: 3      time used: 1.495194435119629\n","minibatch AVG loss: 0.005070887075271457\n","\n","Epoch: 12  train \n","Loss: 0.0525  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0275  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.7082374095916748\n","minibatch AVG loss: 0.0034150502004195006\n","Epoch: 13     train index of 5 minibatch: 2      time used: 1.4961891174316406\n","minibatch AVG loss: 0.00810872134170495\n","Epoch: 13     train index of 5 minibatch: 3      time used: 1.4954955577850342\n","minibatch AVG loss: 0.0068276043879450295\n","\n","Epoch: 13  train \n","Loss: 0.0056  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.2826  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.7102668285369873\n","minibatch AVG loss: 0.016320655771414748\n","Epoch: 14     train index of 5 minibatch: 2      time used: 1.4951746463775635\n","minibatch AVG loss: 0.00023959838890732498\n","Epoch: 14     train index of 5 minibatch: 3      time used: 1.494048833847046\n","minibatch AVG loss: 0.0030623998445662437\n","\n","Epoch: 14  train \n","Loss: 0.0057  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0617  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.706697940826416\n","minibatch AVG loss: 0.00017342057108180598\n","Epoch: 15     train index of 5 minibatch: 2      time used: 1.4945507049560547\n","minibatch AVG loss: 0.016875598737533436\n","Epoch: 15     train index of 5 minibatch: 3      time used: 1.4955129623413086\n","minibatch AVG loss: 6.761903205187991e-05\n","\n","Epoch: 15  train \n","Loss: 0.0050  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.2172  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.7127621173858643\n","minibatch AVG loss: 0.00207688022946968\n","Epoch: 16     train index of 5 minibatch: 2      time used: 1.4942107200622559\n","minibatch AVG loss: 0.0006379062588166562\n","Epoch: 16     train index of 5 minibatch: 3      time used: 1.494415283203125\n","minibatch AVG loss: 0.000678842412389713\n","\n","Epoch: 16  train \n","Loss: 0.0011  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.2576  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.7197191715240479\n","minibatch AVG loss: 0.0005320450949511723\n","Epoch: 17     train index of 5 minibatch: 2      time used: 1.494978427886963\n","minibatch AVG loss: 0.0033888118756749464\n","Epoch: 17     train index of 5 minibatch: 3      time used: 1.4949822425842285\n","minibatch AVG loss: 0.00018966978495882358\n","\n","Epoch: 17  train \n","Loss: 0.0099  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.1902  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 1.71085524559021\n","minibatch AVG loss: 5.897821483813459e-05\n","Epoch: 18     train index of 5 minibatch: 2      time used: 1.495887279510498\n","minibatch AVG loss: 0.03408694912845931\n","Epoch: 18     train index of 5 minibatch: 3      time used: 1.4965693950653076\n","minibatch AVG loss: 0.0020758768157975284\n","\n","Epoch: 18  train \n","Loss: 0.0105  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.1603  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.7216594219207764\n","minibatch AVG loss: 7.758024403301534e-05\n","Epoch: 19     train index of 5 minibatch: 2      time used: 1.497084379196167\n","minibatch AVG loss: 0.0014303724006822448\n","Epoch: 19     train index of 5 minibatch: 3      time used: 1.4955263137817383\n","minibatch AVG loss: 0.09131005369827108\n","\n","Epoch: 19  train \n","Loss: 0.0269  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0107  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.7086408138275146\n","minibatch AVG loss: 0.0032374668238844607\n","Epoch: 20     train index of 5 minibatch: 2      time used: 1.495760440826416\n","minibatch AVG loss: 0.07043657359754434\n","Epoch: 20     train index of 5 minibatch: 3      time used: 1.4947009086608887\n","minibatch AVG loss: 0.0014724914680300572\n","\n","Epoch: 20  train \n","Loss: 0.0218  Acc: 97.1014\n","benign precision: 96.6667  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.6667\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0136  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.7080106735229492\n","minibatch AVG loss: 0.00036822286892856937\n","Epoch: 21     train index of 5 minibatch: 2      time used: 1.4942748546600342\n","minibatch AVG loss: 0.00017962173956220795\n","Epoch: 21     train index of 5 minibatch: 3      time used: 1.495051383972168\n","minibatch AVG loss: 0.00046624637416243787\n","\n","Epoch: 21  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0706  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.7095584869384766\n","minibatch AVG loss: 0.0001525430674519157\n","Epoch: 22     train index of 5 minibatch: 2      time used: 1.4943993091583252\n","minibatch AVG loss: 0.00013665448996107442\n","Epoch: 22     train index of 5 minibatch: 3      time used: 1.4955353736877441\n","minibatch AVG loss: 0.003625748424747144\n","\n","Epoch: 22  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0586  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.704038143157959\n","minibatch AVG loss: 0.0009870224965197849\n","Epoch: 23     train index of 5 minibatch: 2      time used: 1.49644136428833\n","minibatch AVG loss: 5.4891708532522895e-05\n","Epoch: 23     train index of 5 minibatch: 3      time used: 1.4956188201904297\n","minibatch AVG loss: 0.0020355417818336717\n","\n","Epoch: 23  train \n","Loss: 0.0009  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0428  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.7086553573608398\n","minibatch AVG loss: 1.8601638930704212e-05\n","Epoch: 24     train index of 5 minibatch: 2      time used: 1.4951767921447754\n","minibatch AVG loss: 0.0004282283011320942\n","Epoch: 24     train index of 5 minibatch: 3      time used: 1.4919168949127197\n","minibatch AVG loss: 0.0001174802390323748\n","\n","Epoch: 24  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0466  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.7046339511871338\n","minibatch AVG loss: 0.0006917780840012711\n","Epoch: 25     train index of 5 minibatch: 2      time used: 1.49483323097229\n","minibatch AVG loss: 5.1031301336479376e-05\n","Epoch: 25     train index of 5 minibatch: 3      time used: 1.4952270984649658\n","minibatch AVG loss: 0.0006391152533979039\n","\n","Epoch: 25  train \n","Loss: 0.0004  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0402  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.706049919128418\n","minibatch AVG loss: 8.947427518251061e-05\n","Epoch: 26     train index of 5 minibatch: 2      time used: 1.4949994087219238\n","minibatch AVG loss: 0.0015151141423302762\n","Epoch: 26     train index of 5 minibatch: 3      time used: 1.4955778121948242\n","minibatch AVG loss: 0.00040868711371899735\n","\n","Epoch: 26  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0289  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.7143304347991943\n","minibatch AVG loss: 0.002221627882499888\n","Epoch: 27     train index of 5 minibatch: 2      time used: 1.4947776794433594\n","minibatch AVG loss: 0.0003493946836897521\n","Epoch: 27     train index of 5 minibatch: 3      time used: 1.4937260150909424\n","minibatch AVG loss: 3.359579604875762e-05\n","\n","Epoch: 27  train \n","Loss: 0.0008  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0327  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.705587387084961\n","minibatch AVG loss: 0.0023612448308995226\n","Epoch: 28     train index of 5 minibatch: 2      time used: 1.494809627532959\n","minibatch AVG loss: 4.081225013123913e-05\n","Epoch: 28     train index of 5 minibatch: 3      time used: 1.4945006370544434\n","minibatch AVG loss: 0.00023087135765536003\n","\n","Epoch: 28  train \n","Loss: 0.0008  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0248  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 1.7080349922180176\n","minibatch AVG loss: 0.0013388645638315211\n","Epoch: 29     train index of 5 minibatch: 2      time used: 1.4955098628997803\n","minibatch AVG loss: 0.0001290267276999657\n","Epoch: 29     train index of 5 minibatch: 3      time used: 1.4938559532165527\n","minibatch AVG loss: 0.012840124131253105\n","\n","Epoch: 29  train \n","Loss: 0.0041  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0119  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.7081172466278076\n","minibatch AVG loss: 2.891294238906994e-05\n","Epoch: 30     train index of 5 minibatch: 2      time used: 1.4959330558776855\n","minibatch AVG loss: 0.0003276255075434165\n","Epoch: 30     train index of 5 minibatch: 3      time used: 1.4951121807098389\n","minibatch AVG loss: 0.00012774452470694088\n","\n","Epoch: 30  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0065  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.7111899852752686\n","minibatch AVG loss: 0.00016162948593319016\n","Epoch: 31     train index of 5 minibatch: 2      time used: 1.4949965476989746\n","minibatch AVG loss: 0.009844239465382997\n","Epoch: 31     train index of 5 minibatch: 3      time used: 1.495534896850586\n","minibatch AVG loss: 0.00027904443067541254\n","\n","Epoch: 31  train \n","Loss: 0.0030  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0124  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.7109007835388184\n","minibatch AVG loss: 0.0009936034486599965\n","Epoch: 32     train index of 5 minibatch: 2      time used: 1.4956998825073242\n","minibatch AVG loss: 3.708530929316112e-05\n","Epoch: 32     train index of 5 minibatch: 3      time used: 1.4950196743011475\n","minibatch AVG loss: 3.890218572450976e-05\n","\n","Epoch: 32  train \n","Loss: 0.0003  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0152  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.7114710807800293\n","minibatch AVG loss: 0.0020724095423247492\n","Epoch: 33     train index of 5 minibatch: 2      time used: 1.497176170349121\n","minibatch AVG loss: 6.115378732829413e-05\n","Epoch: 33     train index of 5 minibatch: 3      time used: 1.4959895610809326\n","minibatch AVG loss: 0.0005255782574749901\n","\n","Epoch: 33  train \n","Loss: 0.0008  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0120  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 1.7126398086547852\n","minibatch AVG loss: 0.00011588955940169398\n","Epoch: 34     train index of 5 minibatch: 2      time used: 1.4938700199127197\n","minibatch AVG loss: 0.000940256644571491\n","Epoch: 34     train index of 5 minibatch: 3      time used: 1.4943218231201172\n","minibatch AVG loss: 0.002288741574739106\n","\n","Epoch: 34  train \n","Loss: 0.0010  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0104  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.7154395580291748\n","minibatch AVG loss: 1.5288055737983087e-05\n","Epoch: 35     train index of 5 minibatch: 2      time used: 1.4953444004058838\n","minibatch AVG loss: 5.287702911118686e-05\n","Epoch: 35     train index of 5 minibatch: 3      time used: 1.4944419860839844\n","minibatch AVG loss: 0.0003093019426131605\n","\n","Epoch: 35  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0096  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.709362506866455\n","minibatch AVG loss: 4.640112360903004e-05\n","Epoch: 36     train index of 5 minibatch: 2      time used: 1.4950001239776611\n","minibatch AVG loss: 3.0601500384364044e-05\n","Epoch: 36     train index of 5 minibatch: 3      time used: 1.4951725006103516\n","minibatch AVG loss: 3.6380615347297864e-05\n","\n","Epoch: 36  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0096  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.7058749198913574\n","minibatch AVG loss: 0.0112876536935687\n","Epoch: 37     train index of 5 minibatch: 2      time used: 1.4944956302642822\n","minibatch AVG loss: 1.125913470332307e-05\n","Epoch: 37     train index of 5 minibatch: 3      time used: 1.4935052394866943\n","minibatch AVG loss: 0.001236121808506141\n","\n","Epoch: 37  train \n","Loss: 0.0037  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0259  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 1.710871696472168\n","minibatch AVG loss: 3.152539834445633e-05\n","Epoch: 38     train index of 5 minibatch: 2      time used: 1.4951093196868896\n","minibatch AVG loss: 1.4519231672238676e-05\n","Epoch: 38     train index of 5 minibatch: 3      time used: 1.4931557178497314\n","minibatch AVG loss: 4.1659449379949365e-05\n","\n","Epoch: 38  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0313  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.706554889678955\n","minibatch AVG loss: 3.37804918672191e-05\n","Epoch: 39     train index of 5 minibatch: 2      time used: 1.4951424598693848\n","minibatch AVG loss: 0.0010559978278706695\n","Epoch: 39     train index of 5 minibatch: 3      time used: 1.4951801300048828\n","minibatch AVG loss: 0.0007085020160047861\n","\n","Epoch: 39  train \n","Loss: 0.0007  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0296  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.7082738876342773\n","minibatch AVG loss: 4.307110039007967e-05\n","Epoch: 40     train index of 5 minibatch: 2      time used: 1.4957940578460693\n","minibatch AVG loss: 0.00019885691071976907\n","Epoch: 40     train index of 5 minibatch: 3      time used: 1.4943029880523682\n","minibatch AVG loss: 0.001983082797403313\n","\n","Epoch: 40  train \n","Loss: 0.0006  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0289  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 1.7107415199279785\n","minibatch AVG loss: 0.00010318373665541003\n","Epoch: 41     train index of 5 minibatch: 2      time used: 1.495682954788208\n","minibatch AVG loss: 7.755297622225044e-05\n","Epoch: 41     train index of 5 minibatch: 3      time used: 1.4948556423187256\n","minibatch AVG loss: 0.011828623652490933\n","\n","Epoch: 41  train \n","Loss: 0.0035  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0220  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.7110421657562256\n","minibatch AVG loss: 1.554385982558415e-05\n","Epoch: 42     train index of 5 minibatch: 2      time used: 1.4951012134552002\n","minibatch AVG loss: 3.6149430115983705e-05\n","Epoch: 42     train index of 5 minibatch: 3      time used: 1.496290922164917\n","minibatch AVG loss: 1.985295839403989e-05\n","\n","Epoch: 42  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0168  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.7134714126586914\n","minibatch AVG loss: 0.009136288227387013\n","Epoch: 43     train index of 5 minibatch: 2      time used: 1.4954099655151367\n","minibatch AVG loss: 0.007510733926574175\n","Epoch: 43     train index of 5 minibatch: 3      time used: 1.4955554008483887\n","minibatch AVG loss: 0.0004441887919711007\n","\n","Epoch: 43  train \n","Loss: 0.0050  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0182  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.7139766216278076\n","minibatch AVG loss: 6.559747107530711e-05\n","Epoch: 44     train index of 5 minibatch: 2      time used: 1.4956574440002441\n","minibatch AVG loss: 5.114972991577815e-05\n","Epoch: 44     train index of 5 minibatch: 3      time used: 1.4940216541290283\n","minibatch AVG loss: 2.5174665404392725e-05\n","\n","Epoch: 44  train \n","Loss: 0.0000  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0194  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.7146592140197754\n","minibatch AVG loss: 0.0006203951739735203\n","Epoch: 45     train index of 5 minibatch: 2      time used: 1.4960412979125977\n","minibatch AVG loss: 3.791758972511161e-05\n","Epoch: 45     train index of 5 minibatch: 3      time used: 1.494074821472168\n","minibatch AVG loss: 2.918643012890243e-05\n","\n","Epoch: 45  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0190  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.7169256210327148\n","minibatch AVG loss: 3.5984672967970256e-05\n","Epoch: 46     train index of 5 minibatch: 2      time used: 1.4949510097503662\n","minibatch AVG loss: 2.6103514710484887e-05\n","Epoch: 46     train index of 5 minibatch: 3      time used: 1.4937446117401123\n","minibatch AVG loss: 0.010845208857051602\n","\n","Epoch: 46  train \n","Loss: 0.0032  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0230  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.7107105255126953\n","minibatch AVG loss: 9.875266841845587e-05\n","Epoch: 47     train index of 5 minibatch: 2      time used: 1.4941980838775635\n","minibatch AVG loss: 3.951721532757802e-06\n","Epoch: 47     train index of 5 minibatch: 3      time used: 1.4942378997802734\n","minibatch AVG loss: 2.044925913651241e-05\n","\n","Epoch: 47  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0269  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.7094602584838867\n","minibatch AVG loss: 2.6021143867183127e-05\n","Epoch: 48     train index of 5 minibatch: 2      time used: 1.494889736175537\n","minibatch AVG loss: 9.323294216301292e-05\n","Epoch: 48     train index of 5 minibatch: 3      time used: 1.4945199489593506\n","minibatch AVG loss: 4.9144154309033185e-05\n","\n","Epoch: 48  train \n","Loss: 0.0001  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0274  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.7089753150939941\n","minibatch AVG loss: 0.00026289426944003935\n","Epoch: 49     train index of 5 minibatch: 2      time used: 1.4931068420410156\n","minibatch AVG loss: 5.1172917665098794e-05\n","Epoch: 49     train index of 5 minibatch: 3      time used: 1.494689702987671\n","minibatch AVG loss: 0.001428390569435578\n","\n","Epoch: 49  train \n","Loss: 0.0005  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0269  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.7116844654083252\n","minibatch AVG loss: 1.2652865302698047e-05\n","Epoch: 50     train index of 5 minibatch: 2      time used: 1.4942407608032227\n","minibatch AVG loss: 0.0006842158328197456\n","Epoch: 50     train index of 5 minibatch: 3      time used: 1.4935111999511719\n","minibatch AVG loss: 0.0001440733181880205\n","\n","Epoch: 50  train \n","Loss: 0.0002  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0255  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 4m 56s\n","Best epoch idx:  50\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS --augmentation_name Cutout --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nQ3MJZGaaIRc"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name='CutMix', backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.7394683361053467\n","minibatch AVG loss: 1.7453938245773315\n","Epoch: 1     train index of 5 minibatch: 2      time used: 1.4979767799377441\n","minibatch AVG loss: 2.1522363007068632\n","Epoch: 1     train index of 5 minibatch: 3      time used: 1.4968276023864746\n","minibatch AVG loss: 1.5341726422309876\n","\n","Epoch: 1  train \n","Loss: 1.7826  Acc: 78.2609\n","benign precision: 71.4286  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 74.3590\n","benign FPR: 25.6410  NPV: 87.8788\n","benign TP: 25.0\n","benign TN: 29.0\n","benign FP: 10.0\n","benign FN: 4.0\n","malignant precision: 87.8788  recall: 74.3590\n","malignant sensitivity: 74.3590  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 71.4286\n","malignant TP: 29.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 10.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 1.1911  Acc: 81.2500\n","benign precision: 75.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 87.5000\n","benign TP: 6.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 87.5000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 75.0000\n","malignant TP: 7.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 1.708765983581543\n","minibatch AVG loss: 4.095122361183167\n","Epoch: 2     train index of 5 minibatch: 2      time used: 1.4970195293426514\n","minibatch AVG loss: 1.0252360105514526\n","Epoch: 2     train index of 5 minibatch: 3      time used: 1.4954454898834229\n","minibatch AVG loss: 0.8637114048004151\n","\n","Epoch: 2  train \n","Loss: 1.8842  Acc: 88.4058\n","benign precision: 89.6552  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 89.7436\n","benign TP: 26.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 89.7436  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 89.6552\n","malignant TP: 35.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.9829  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 1.716644048690796\n","minibatch AVG loss: 0.9537281513214111\n","Epoch: 3     train index of 5 minibatch: 2      time used: 1.49932861328125\n","minibatch AVG loss: 1.5467109739780427\n","Epoch: 3     train index of 5 minibatch: 3      time used: 1.496450424194336\n","minibatch AVG loss: 1.3651013135910035\n","\n","Epoch: 3  train \n","Loss: 1.3009  Acc: 91.3043\n","benign precision: 100.0000  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 88.6364\n","benign TP: 24.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 88.6364  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.4870  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 1.7116360664367676\n","minibatch AVG loss: 0.9457129962742329\n","Epoch: 4     train index of 5 minibatch: 2      time used: 1.4959328174591064\n","minibatch AVG loss: 0.9849015034735202\n","Epoch: 4     train index of 5 minibatch: 3      time used: 1.4970190525054932\n","minibatch AVG loss: 0.9998949781060219\n","\n","Epoch: 4  train \n","Loss: 0.8525  Acc: 94.2029\n","benign precision: 90.3226  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 92.5000\n","benign FPR: 7.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 3.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 92.5000\n","malignant sensitivity: 92.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 90.3226\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.6058  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 1.7027721405029297\n","minibatch AVG loss: 0.5073957376182079\n","Epoch: 5     train index of 5 minibatch: 2      time used: 1.4967107772827148\n","minibatch AVG loss: 1.471051573753357\n","Epoch: 5     train index of 5 minibatch: 3      time used: 1.4984169006347656\n","minibatch AVG loss: 0.8636421263217926\n","\n","Epoch: 5  train \n","Loss: 1.0379  Acc: 91.3043\n","benign precision: 92.8571  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 92.5000\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.5000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 92.8571\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.8420  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.7098970413208008\n","minibatch AVG loss: 0.44435029476881027\n","Epoch: 6     train index of 5 minibatch: 2      time used: 1.4973804950714111\n","minibatch AVG loss: 0.3157290633767843\n","Epoch: 6     train index of 5 minibatch: 3      time used: 1.4977705478668213\n","minibatch AVG loss: 0.8611812487244606\n","\n","Epoch: 6  train \n","Loss: 0.4878  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.4359\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.3910  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.711251974105835\n","minibatch AVG loss: 0.8862473599612712\n","Epoch: 7     train index of 5 minibatch: 2      time used: 1.49735426902771\n","minibatch AVG loss: 0.8318699479103089\n","Epoch: 7     train index of 5 minibatch: 3      time used: 1.4992923736572266\n","minibatch AVG loss: 0.1232252299785614\n","\n","Epoch: 7  train \n","Loss: 0.5368  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.4219  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.7099299430847168\n","minibatch AVG loss: 0.9708884939551353\n","Epoch: 8     train index of 5 minibatch: 2      time used: 1.498969554901123\n","minibatch AVG loss: 1.3880123391747474\n","Epoch: 8     train index of 5 minibatch: 3      time used: 1.500673770904541\n","minibatch AVG loss: 1.3329811573028565\n","\n","Epoch: 8  train \n","Loss: 1.0822  Acc: 92.7536\n","benign precision: 96.6667  recall: 90.6250\n","benign sensitivity: 90.6250  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 92.1053\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 90.6250\n","malignant FPR: 9.3750  NPV: 96.6667\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.4465  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.7129888534545898\n","minibatch AVG loss: 0.4849799878895283\n","Epoch: 9     train index of 5 minibatch: 2      time used: 1.4980783462524414\n","minibatch AVG loss: 1.153375332057476\n","Epoch: 9     train index of 5 minibatch: 3      time used: 1.497056245803833\n","minibatch AVG loss: 1.4324990689754487\n","\n","Epoch: 9  train \n","Loss: 1.0504  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.1220\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.1220  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.6707  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.7136194705963135\n","minibatch AVG loss: 1.4561392307281493\n","Epoch: 10     train index of 5 minibatch: 2      time used: 1.4986655712127686\n","minibatch AVG loss: 1.219714766740799\n","Epoch: 10     train index of 5 minibatch: 3      time used: 1.4981451034545898\n","minibatch AVG loss: 0.7727363407611847\n","\n","Epoch: 10  train \n","Loss: 1.1533  Acc: 89.8551\n","benign precision: 89.6552  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 92.3077\n","benign TP: 26.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 92.3077  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 89.6552\n","malignant TP: 36.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.3666  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.708364725112915\n","minibatch AVG loss: 0.33389543294906615\n","Epoch: 11     train index of 5 minibatch: 2      time used: 1.4982218742370605\n","minibatch AVG loss: 0.6949259445071221\n","Epoch: 11     train index of 5 minibatch: 3      time used: 1.4981658458709717\n","minibatch AVG loss: 0.7570346772670746\n","\n","Epoch: 11  train \n","Loss: 0.5299  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 40.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 40.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.2171  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.7079877853393555\n","minibatch AVG loss: 0.8272240221500397\n","Epoch: 12     train index of 5 minibatch: 2      time used: 1.4969089031219482\n","minibatch AVG loss: 1.132021876424551\n","Epoch: 12     train index of 5 minibatch: 3      time used: 1.497809886932373\n","minibatch AVG loss: 0.8528352022171021\n","\n","Epoch: 12  train \n","Loss: 0.9002  Acc: 92.7536\n","benign precision: 93.5484  recall: 93.5484\n","benign sensitivity: 93.5484  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 94.5946\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 93.5484\n","malignant FPR: 6.4516  NPV: 93.5484\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.5609  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.7064554691314697\n","minibatch AVG loss: 0.7178728252649307\n","Epoch: 13     train index of 5 minibatch: 2      time used: 1.497509479522705\n","minibatch AVG loss: 0.7851883009076118\n","Epoch: 13     train index of 5 minibatch: 3      time used: 1.4967074394226074\n","minibatch AVG loss: 1.7756519883871078\n","\n","Epoch: 13  train \n","Loss: 1.0612  Acc: 91.3043\n","benign precision: 89.6552  recall: 92.8571\n","benign sensitivity: 92.8571  specificity: 92.5000\n","benign FPR: 7.5000  NPV: 94.8718\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 92.5000\n","malignant sensitivity: 92.5000  specificity: 92.8571\n","malignant FPR: 7.1429  NPV: 89.6552\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.7767  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.715895414352417\n","minibatch AVG loss: 1.930119800567627\n","Epoch: 14     train index of 5 minibatch: 2      time used: 1.4976727962493896\n","minibatch AVG loss: 1.0914869129657745\n","Epoch: 14     train index of 5 minibatch: 3      time used: 1.4976279735565186\n","minibatch AVG loss: 0.8253265872597695\n","\n","Epoch: 14  train \n","Loss: 1.2300  Acc: 86.9565\n","benign precision: 87.0968  recall: 87.0968\n","benign sensitivity: 87.0968  specificity: 89.1892\n","benign FPR: 10.8108  NPV: 89.1892\n","benign TP: 27.0\n","benign TN: 33.0\n","benign FP: 4.0\n","benign FN: 4.0\n","malignant precision: 89.1892  recall: 89.1892\n","malignant sensitivity: 89.1892  specificity: 87.0968\n","malignant FPR: 12.9032  NPV: 87.0968\n","malignant TP: 33.0\n","malignant TN: 27.0\n","malignant FP: 4.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.4306  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.7112774848937988\n","minibatch AVG loss: 0.5538337498903274\n","Epoch: 15     train index of 5 minibatch: 2      time used: 1.4984586238861084\n","minibatch AVG loss: 0.8593796133995056\n","Epoch: 15     train index of 5 minibatch: 3      time used: 1.496659517288208\n","minibatch AVG loss: 1.10695381462574\n","\n","Epoch: 15  train \n","Loss: 0.8543  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.3220  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.7122244834899902\n","minibatch AVG loss: 0.8812287777662278\n","Epoch: 16     train index of 5 minibatch: 2      time used: 1.4975981712341309\n","minibatch AVG loss: 0.49582336843013763\n","Epoch: 16     train index of 5 minibatch: 3      time used: 1.4955990314483643\n","minibatch AVG loss: 0.571318106353283\n","\n","Epoch: 16  train \n","Loss: 0.5794  Acc: 94.2029\n","benign precision: 93.3333  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 97.3684\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 93.3333\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.3116  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.7100756168365479\n","minibatch AVG loss: 0.44612730853259563\n","Epoch: 17     train index of 5 minibatch: 2      time used: 1.496412754058838\n","minibatch AVG loss: 0.056209447979927066\n","Epoch: 17     train index of 5 minibatch: 3      time used: 1.4969861507415771\n","minibatch AVG loss: 0.5181007958948612\n","\n","Epoch: 17  train \n","Loss: 0.3043  Acc: 95.6522\n","benign precision: 96.8750  recall: 96.8750\n","benign sensitivity: 96.8750  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 97.2222\n","benign TP: 31.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 96.8750\n","malignant FPR: 3.1250  NPV: 96.8750\n","malignant TP: 35.0\n","malignant TN: 31.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.2531  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 1.7138924598693848\n","minibatch AVG loss: 0.7847254309803248\n","Epoch: 18     train index of 5 minibatch: 2      time used: 1.4993884563446045\n","minibatch AVG loss: 0.9664933893829584\n","Epoch: 18     train index of 5 minibatch: 3      time used: 1.4988734722137451\n","minibatch AVG loss: 1.0808693587779998\n","\n","Epoch: 18  train \n","Loss: 0.9326  Acc: 91.3043\n","benign precision: 87.0968  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 90.0000\n","benign FPR: 10.0000  NPV: 97.2973\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 4.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 90.0000\n","malignant sensitivity: 90.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 87.0968\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.2574  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.7143173217773438\n","minibatch AVG loss: 1.920077347755432\n","Epoch: 19     train index of 5 minibatch: 2      time used: 1.4970619678497314\n","minibatch AVG loss: 1.1930688165128231\n","Epoch: 19     train index of 5 minibatch: 3      time used: 1.4966368675231934\n","minibatch AVG loss: 0.10221308209002018\n","\n","Epoch: 19  train \n","Loss: 0.9507  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.3941  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.7108852863311768\n","minibatch AVG loss: 0.7514770478010178\n","Epoch: 20     train index of 5 minibatch: 2      time used: 1.4966576099395752\n","minibatch AVG loss: 0.7184586502611637\n","Epoch: 20     train index of 5 minibatch: 3      time used: 1.496915340423584\n","minibatch AVG loss: 0.7803838819265365\n","\n","Epoch: 20  train \n","Loss: 0.8098  Acc: 95.6522\n","benign precision: 93.1034  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.1220\n","benign FPR: 4.8780  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.1220\n","malignant sensitivity: 95.1220  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.1034\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.5176  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.7164669036865234\n","minibatch AVG loss: 0.6932253852486611\n","Epoch: 21     train index of 5 minibatch: 2      time used: 1.4986376762390137\n","minibatch AVG loss: 1.4423709094524384\n","Epoch: 21     train index of 5 minibatch: 3      time used: 1.4986696243286133\n","minibatch AVG loss: 1.0802820414304732\n","\n","Epoch: 21  train \n","Loss: 1.0034  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.3314  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.715764045715332\n","minibatch AVG loss: 0.42041505724191663\n","Epoch: 22     train index of 5 minibatch: 2      time used: 1.497286319732666\n","minibatch AVG loss: 0.5117927066981792\n","Epoch: 22     train index of 5 minibatch: 3      time used: 1.4982457160949707\n","minibatch AVG loss: 0.3648364398628473\n","\n","Epoch: 22  train \n","Loss: 0.4425  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.3529  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.7063260078430176\n","minibatch AVG loss: 1.567348265647888\n","Epoch: 23     train index of 5 minibatch: 2      time used: 1.4991223812103271\n","minibatch AVG loss: 0.9671211875975132\n","Epoch: 23     train index of 5 minibatch: 3      time used: 1.4981358051300049\n","minibatch AVG loss: 0.7784718319773674\n","\n","Epoch: 23  train \n","Loss: 0.9914  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.7500\n","benign sensitivity: 93.7500  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 94.7368\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.7500\n","malignant FPR: 6.2500  NPV: 100.0000\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.3524  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.7114965915679932\n","minibatch AVG loss: 0.809127277508378\n","Epoch: 24     train index of 5 minibatch: 2      time used: 1.4980521202087402\n","minibatch AVG loss: 0.8315449550747871\n","Epoch: 24     train index of 5 minibatch: 3      time used: 1.4954769611358643\n","minibatch AVG loss: 1.232284951210022\n","\n","Epoch: 24  train \n","Loss: 0.9197  Acc: 94.2029\n","benign precision: 93.7500  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 97.2222\n","benign TP: 30.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 93.7500\n","malignant TP: 35.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.3408  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.7059731483459473\n","minibatch AVG loss: 0.5122439689934254\n","Epoch: 25     train index of 5 minibatch: 2      time used: 1.4957571029663086\n","minibatch AVG loss: 0.7124097242951393\n","Epoch: 25     train index of 5 minibatch: 3      time used: 1.497056007385254\n","minibatch AVG loss: 1.5244728326797485\n","\n","Epoch: 25  train \n","Loss: 0.8689  Acc: 88.4058\n","benign precision: 90.0000  recall: 87.0968\n","benign sensitivity: 87.0968  specificity: 91.8919\n","benign FPR: 8.1081  NPV: 89.4737\n","benign TP: 27.0\n","benign TN: 34.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 91.8919\n","malignant sensitivity: 91.8919  specificity: 87.0968\n","malignant FPR: 12.9032  NPV: 90.0000\n","malignant TP: 34.0\n","malignant TN: 27.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.2574  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.7097501754760742\n","minibatch AVG loss: 0.8712688386440277\n","Epoch: 26     train index of 5 minibatch: 2      time used: 1.4975776672363281\n","minibatch AVG loss: 1.1383886814117432\n","Epoch: 26     train index of 5 minibatch: 3      time used: 1.4974169731140137\n","minibatch AVG loss: 1.0191778652369976\n","\n","Epoch: 26  train \n","Loss: 1.0151  Acc: 95.6522\n","benign precision: 100.0000  recall: 92.5926\n","benign sensitivity: 92.5926  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.3488\n","benign TP: 25.0\n","benign TN: 41.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.3488  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 92.5926\n","malignant FPR: 7.4074  NPV: 100.0000\n","malignant TP: 41.0\n","malignant TN: 25.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.3474  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.7142367362976074\n","minibatch AVG loss: 0.8200696282088756\n","Epoch: 27     train index of 5 minibatch: 2      time used: 1.4983034133911133\n","minibatch AVG loss: 0.43289255499839785\n","Epoch: 27     train index of 5 minibatch: 3      time used: 1.499100685119629\n","minibatch AVG loss: 0.20799148231744766\n","\n","Epoch: 27  train \n","Loss: 0.5166  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.2408  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.7089884281158447\n","minibatch AVG loss: 0.670639232546091\n","Epoch: 28     train index of 5 minibatch: 2      time used: 1.4981920719146729\n","minibatch AVG loss: 0.7418859124183654\n","Epoch: 28     train index of 5 minibatch: 3      time used: 1.4975314140319824\n","minibatch AVG loss: 0.7630244106054306\n","\n","Epoch: 28  train \n","Loss: 0.7488  Acc: 95.6522\n","benign precision: 96.9697  recall: 96.9697\n","benign sensitivity: 96.9697  specificity: 97.1429\n","benign FPR: 2.8571  NPV: 97.1429\n","benign TP: 32.0\n","benign TN: 34.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.1429  recall: 97.1429\n","malignant sensitivity: 97.1429  specificity: 96.9697\n","malignant FPR: 3.0303  NPV: 96.9697\n","malignant TP: 34.0\n","malignant TN: 32.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.2814  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 1.7088215351104736\n","minibatch AVG loss: 1.4064474791288375\n","Epoch: 29     train index of 5 minibatch: 2      time used: 1.4992868900299072\n","minibatch AVG loss: 0.3956480771303177\n","Epoch: 29     train index of 5 minibatch: 3      time used: 1.4990215301513672\n","minibatch AVG loss: 0.7237876817584038\n","\n","Epoch: 29  train \n","Loss: 0.8310  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.9697\n","benign sensitivity: 96.9697  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.2222\n","benign TP: 32.0\n","benign TN: 35.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.9697\n","malignant FPR: 3.0303  NPV: 100.0000\n","malignant TP: 35.0\n","malignant TN: 32.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.2556  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.7067148685455322\n","minibatch AVG loss: 0.8668461818248033\n","Epoch: 30     train index of 5 minibatch: 2      time used: 1.4964344501495361\n","minibatch AVG loss: 0.7986583970487118\n","Epoch: 30     train index of 5 minibatch: 3      time used: 1.4987707138061523\n","minibatch AVG loss: 0.5482242606580258\n","\n","Epoch: 30  train \n","Loss: 0.7271  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.2530  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.7103521823883057\n","minibatch AVG loss: 1.1846228145062923\n","Epoch: 31     train index of 5 minibatch: 2      time used: 1.4979307651519775\n","minibatch AVG loss: 0.5489548280835151\n","Epoch: 31     train index of 5 minibatch: 3      time used: 1.4981141090393066\n","minibatch AVG loss: 0.7885788828134537\n","\n","Epoch: 31  train \n","Loss: 0.8841  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.2224  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.709437370300293\n","minibatch AVG loss: 0.3830487817525864\n","Epoch: 32     train index of 5 minibatch: 2      time used: 1.4970777034759521\n","minibatch AVG loss: 0.9142822355031968\n","Epoch: 32     train index of 5 minibatch: 3      time used: 1.4980547428131104\n","minibatch AVG loss: 0.5445849612355232\n","\n","Epoch: 32  train \n","Loss: 0.6768  Acc: 97.1014\n","benign precision: 96.8750  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.8750\n","malignant TP: 36.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.2559  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.7114653587341309\n","minibatch AVG loss: 1.2797178506851197\n","Epoch: 33     train index of 5 minibatch: 2      time used: 1.4977703094482422\n","minibatch AVG loss: 0.7594113752245903\n","Epoch: 33     train index of 5 minibatch: 3      time used: 1.4961793422698975\n","minibatch AVG loss: 0.599992960691452\n","\n","Epoch: 33  train \n","Loss: 0.7720  Acc: 92.7536\n","benign precision: 89.2857  recall: 96.1538\n","benign sensitivity: 96.1538  specificity: 92.8571\n","benign FPR: 7.1429  NPV: 97.5000\n","benign TP: 25.0\n","benign TN: 39.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 92.8571\n","malignant sensitivity: 92.8571  specificity: 96.1538\n","malignant FPR: 3.8462  NPV: 89.2857\n","malignant TP: 39.0\n","malignant TN: 25.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.2143  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 1.7181341648101807\n","minibatch AVG loss: 0.4282756485044956\n","Epoch: 34     train index of 5 minibatch: 2      time used: 1.4987034797668457\n","minibatch AVG loss: 0.8658869840204716\n","Epoch: 34     train index of 5 minibatch: 3      time used: 1.497795820236206\n","minibatch AVG loss: 1.1768649458885192\n","\n","Epoch: 34  train \n","Loss: 0.7192  Acc: 95.6522\n","benign precision: 93.7500  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.7500\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.3048  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.710465669631958\n","minibatch AVG loss: 0.5438224907964468\n","Epoch: 35     train index of 5 minibatch: 2      time used: 1.497995138168335\n","minibatch AVG loss: 1.706219846010208\n","Epoch: 35     train index of 5 minibatch: 3      time used: 1.497840404510498\n","minibatch AVG loss: 0.32179763317108157\n","\n","Epoch: 35  train \n","Loss: 0.8771  Acc: 91.3043\n","benign precision: 93.1034  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 92.3077\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.3077  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 93.1034\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.1951  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.7081143856048584\n","minibatch AVG loss: 0.2048317302018404\n","Epoch: 36     train index of 5 minibatch: 2      time used: 1.4977450370788574\n","minibatch AVG loss: 0.8004062958061695\n","Epoch: 36     train index of 5 minibatch: 3      time used: 1.4964046478271484\n","minibatch AVG loss: 1.3097286581993104\n","\n","Epoch: 36  train \n","Loss: 0.8189  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.2362  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.7053823471069336\n","minibatch AVG loss: 0.49669312182813885\n","Epoch: 37     train index of 5 minibatch: 2      time used: 1.4962103366851807\n","minibatch AVG loss: 0.43210783004760744\n","Epoch: 37     train index of 5 minibatch: 3      time used: 1.498072624206543\n","minibatch AVG loss: 0.2894198909401894\n","\n","Epoch: 37  train \n","Loss: 0.4534  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.1974  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 1.71240234375\n","minibatch AVG loss: 1.1149005867540835\n","Epoch: 38     train index of 5 minibatch: 2      time used: 1.497365951538086\n","minibatch AVG loss: 0.5666566213592887\n","Epoch: 38     train index of 5 minibatch: 3      time used: 1.4980502128601074\n","minibatch AVG loss: 0.5942000888288022\n","\n","Epoch: 38  train \n","Loss: 0.7552  Acc: 91.3043\n","benign precision: 90.9091  recall: 93.7500\n","benign sensitivity: 93.7500  specificity: 91.6667\n","benign FPR: 8.3333  NPV: 94.2857\n","benign TP: 30.0\n","benign TN: 33.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.2857  recall: 91.6667\n","malignant sensitivity: 91.6667  specificity: 93.7500\n","malignant FPR: 6.2500  NPV: 90.9091\n","malignant TP: 33.0\n","malignant TN: 30.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.2271  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.7175333499908447\n","minibatch AVG loss: 0.8159531895071268\n","Epoch: 39     train index of 5 minibatch: 2      time used: 1.497225284576416\n","minibatch AVG loss: 0.8179995119571686\n","Epoch: 39     train index of 5 minibatch: 3      time used: 1.4980618953704834\n","minibatch AVG loss: 0.5716785453259945\n","\n","Epoch: 39  train \n","Loss: 0.6422  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.1833  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.7047877311706543\n","minibatch AVG loss: 1.279023861885071\n","Epoch: 40     train index of 5 minibatch: 2      time used: 1.4968385696411133\n","minibatch AVG loss: 0.9810205712914467\n","Epoch: 40     train index of 5 minibatch: 3      time used: 1.496746301651001\n","minibatch AVG loss: 0.2693488482385874\n","\n","Epoch: 40  train \n","Loss: 0.8423  Acc: 92.7536\n","benign precision: 93.7500  recall: 93.7500\n","benign sensitivity: 93.7500  specificity: 94.4444\n","benign FPR: 5.5556  NPV: 94.4444\n","benign TP: 30.0\n","benign TN: 34.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.4444  recall: 94.4444\n","malignant sensitivity: 94.4444  specificity: 93.7500\n","malignant FPR: 6.2500  NPV: 93.7500\n","malignant TP: 34.0\n","malignant TN: 30.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.2143  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 1.7105529308319092\n","minibatch AVG loss: 0.17552022747695445\n","Epoch: 41     train index of 5 minibatch: 2      time used: 1.4963839054107666\n","minibatch AVG loss: 0.12854443658143283\n","Epoch: 41     train index of 5 minibatch: 3      time used: 1.4974548816680908\n","minibatch AVG loss: 0.33308214172720907\n","\n","Epoch: 41  train \n","Loss: 0.2718  Acc: 97.1014\n","benign precision: 96.4286  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.4286\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.1848  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.70839524269104\n","minibatch AVG loss: 0.9239962577819825\n","Epoch: 42     train index of 5 minibatch: 2      time used: 1.4975426197052002\n","minibatch AVG loss: 0.7085279690101742\n","Epoch: 42     train index of 5 minibatch: 3      time used: 1.4975364208221436\n","minibatch AVG loss: 0.7561857655644417\n","\n","Epoch: 42  train \n","Loss: 0.7737  Acc: 92.7536\n","benign precision: 89.2857  recall: 96.1538\n","benign sensitivity: 96.1538  specificity: 92.8571\n","benign FPR: 7.1429  NPV: 97.5000\n","benign TP: 25.0\n","benign TN: 39.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 92.8571\n","malignant sensitivity: 92.8571  specificity: 96.1538\n","malignant FPR: 3.8462  NPV: 89.2857\n","malignant TP: 39.0\n","malignant TN: 25.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.1765  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.7080836296081543\n","minibatch AVG loss: 0.9566648513078689\n","Epoch: 43     train index of 5 minibatch: 2      time used: 1.497309684753418\n","minibatch AVG loss: 0.630563897639513\n","Epoch: 43     train index of 5 minibatch: 3      time used: 1.4979302883148193\n","minibatch AVG loss: 0.4752705104649067\n","\n","Epoch: 43  train \n","Loss: 0.6438  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.1815  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.7059361934661865\n","minibatch AVG loss: 0.4815478682518005\n","Epoch: 44     train index of 5 minibatch: 2      time used: 1.4996607303619385\n","minibatch AVG loss: 0.8261978045105934\n","Epoch: 44     train index of 5 minibatch: 3      time used: 1.4963183403015137\n","minibatch AVG loss: 0.4396385237574577\n","\n","Epoch: 44  train \n","Loss: 0.5977  Acc: 94.2029\n","benign precision: 96.6667  recall: 93.5484\n","benign sensitivity: 93.5484  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 94.7368\n","benign TP: 29.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 93.5484\n","malignant FPR: 6.4516  NPV: 96.6667\n","malignant TP: 36.0\n","malignant TN: 29.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.1710  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.7101874351501465\n","minibatch AVG loss: 0.36493164002895356\n","Epoch: 45     train index of 5 minibatch: 2      time used: 1.4965691566467285\n","minibatch AVG loss: 0.8468333967961371\n","Epoch: 45     train index of 5 minibatch: 3      time used: 1.497136116027832\n","minibatch AVG loss: 0.2488155798986554\n","\n","Epoch: 45  train \n","Loss: 0.4635  Acc: 97.1014\n","benign precision: 96.8750  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.8750\n","malignant TP: 36.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.1625  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.7083640098571777\n","minibatch AVG loss: 0.17092258892953396\n","Epoch: 46     train index of 5 minibatch: 2      time used: 1.4980411529541016\n","minibatch AVG loss: 0.7236716505140066\n","Epoch: 46     train index of 5 minibatch: 3      time used: 1.496070146560669\n","minibatch AVG loss: 0.7791279949247837\n","\n","Epoch: 46  train \n","Loss: 0.5614  Acc: 97.1014\n","benign precision: 96.4286  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.4286\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.1598  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.7104222774505615\n","minibatch AVG loss: 0.73856908865273\n","Epoch: 47     train index of 5 minibatch: 2      time used: 1.4979572296142578\n","minibatch AVG loss: 0.9044553861021996\n","Epoch: 47     train index of 5 minibatch: 3      time used: 1.496286153793335\n","minibatch AVG loss: 0.9566571850329637\n","\n","Epoch: 47  train \n","Loss: 0.8284  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.1547  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.7103939056396484\n","minibatch AVG loss: 0.7008832946419716\n","Epoch: 48     train index of 5 minibatch: 2      time used: 1.4980010986328125\n","minibatch AVG loss: 0.530615758895874\n","Epoch: 48     train index of 5 minibatch: 3      time used: 1.49761962890625\n","minibatch AVG loss: 0.6223939832299947\n","\n","Epoch: 48  train \n","Loss: 0.6253  Acc: 91.3043\n","benign precision: 87.5000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 97.2222\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 87.5000\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.1559  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.7106719017028809\n","minibatch AVG loss: 0.5495834678411484\n","Epoch: 49     train index of 5 minibatch: 2      time used: 1.4995512962341309\n","minibatch AVG loss: 0.261155067384243\n","Epoch: 49     train index of 5 minibatch: 3      time used: 1.4998908042907715\n","minibatch AVG loss: 0.6753845289349556\n","\n","Epoch: 49  train \n","Loss: 0.4911  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.1572  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.7236456871032715\n","minibatch AVG loss: 0.9113868355751038\n","Epoch: 50     train index of 5 minibatch: 2      time used: 1.4991710186004639\n","minibatch AVG loss: 0.33187765553593634\n","Epoch: 50     train index of 5 minibatch: 3      time used: 1.4981825351715088\n","minibatch AVG loss: 0.5070764154195786\n","\n","Epoch: 50  train \n","Loss: 0.5741  Acc: 95.6522\n","benign precision: 96.5517  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 97.4359\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 96.5517\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.1515  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 4m 56s\n","Best epoch idx:  49\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS --augmentation_name CutMix --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7YKeUdYjaOx3"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name='Mixup', backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 2.3224, -1.6767]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 768, 24, 24]         590,592\n","          Identity-2             [-1, 576, 768]               0\n","        PatchEmbed-3             [-1, 576, 768]               0\n","           Dropout-4             [-1, 577, 768]               0\n","         LayerNorm-5             [-1, 577, 768]           1,536\n","            Linear-6            [-1, 577, 2304]       1,771,776\n","           Dropout-7         [-1, 12, 577, 577]               0\n","            Linear-8             [-1, 577, 768]         590,592\n","           Dropout-9             [-1, 577, 768]               0\n","        Attention-10             [-1, 577, 768]               0\n","         Identity-11             [-1, 577, 768]               0\n","        LayerNorm-12             [-1, 577, 768]           1,536\n","           Linear-13            [-1, 577, 3072]       2,362,368\n","             GELU-14            [-1, 577, 3072]               0\n","          Dropout-15            [-1, 577, 3072]               0\n","           Linear-16             [-1, 577, 768]       2,360,064\n","          Dropout-17             [-1, 577, 768]               0\n","              Mlp-18             [-1, 577, 768]               0\n","         Identity-19             [-1, 577, 768]               0\n","            Block-20             [-1, 577, 768]               0\n","        LayerNorm-21             [-1, 577, 768]           1,536\n","           Linear-22            [-1, 577, 2304]       1,771,776\n","          Dropout-23         [-1, 12, 577, 577]               0\n","           Linear-24             [-1, 577, 768]         590,592\n","          Dropout-25             [-1, 577, 768]               0\n","        Attention-26             [-1, 577, 768]               0\n","         Identity-27             [-1, 577, 768]               0\n","        LayerNorm-28             [-1, 577, 768]           1,536\n","           Linear-29            [-1, 577, 3072]       2,362,368\n","             GELU-30            [-1, 577, 3072]               0\n","          Dropout-31            [-1, 577, 3072]               0\n","           Linear-32             [-1, 577, 768]       2,360,064\n","          Dropout-33             [-1, 577, 768]               0\n","              Mlp-34             [-1, 577, 768]               0\n","         Identity-35             [-1, 577, 768]               0\n","            Block-36             [-1, 577, 768]               0\n","        LayerNorm-37             [-1, 577, 768]           1,536\n","           Linear-38            [-1, 577, 2304]       1,771,776\n","          Dropout-39         [-1, 12, 577, 577]               0\n","           Linear-40             [-1, 577, 768]         590,592\n","          Dropout-41             [-1, 577, 768]               0\n","        Attention-42             [-1, 577, 768]               0\n","         Identity-43             [-1, 577, 768]               0\n","        LayerNorm-44             [-1, 577, 768]           1,536\n","           Linear-45            [-1, 577, 3072]       2,362,368\n","             GELU-46            [-1, 577, 3072]               0\n","          Dropout-47            [-1, 577, 3072]               0\n","           Linear-48             [-1, 577, 768]       2,360,064\n","          Dropout-49             [-1, 577, 768]               0\n","              Mlp-50             [-1, 577, 768]               0\n","         Identity-51             [-1, 577, 768]               0\n","            Block-52             [-1, 577, 768]               0\n","        LayerNorm-53             [-1, 577, 768]           1,536\n","           Linear-54            [-1, 577, 2304]       1,771,776\n","          Dropout-55         [-1, 12, 577, 577]               0\n","           Linear-56             [-1, 577, 768]         590,592\n","          Dropout-57             [-1, 577, 768]               0\n","        Attention-58             [-1, 577, 768]               0\n","         Identity-59             [-1, 577, 768]               0\n","        LayerNorm-60             [-1, 577, 768]           1,536\n","           Linear-61            [-1, 577, 3072]       2,362,368\n","             GELU-62            [-1, 577, 3072]               0\n","          Dropout-63            [-1, 577, 3072]               0\n","           Linear-64             [-1, 577, 768]       2,360,064\n","          Dropout-65             [-1, 577, 768]               0\n","              Mlp-66             [-1, 577, 768]               0\n","         Identity-67             [-1, 577, 768]               0\n","            Block-68             [-1, 577, 768]               0\n","        LayerNorm-69             [-1, 577, 768]           1,536\n","           Linear-70            [-1, 577, 2304]       1,771,776\n","          Dropout-71         [-1, 12, 577, 577]               0\n","           Linear-72             [-1, 577, 768]         590,592\n","          Dropout-73             [-1, 577, 768]               0\n","        Attention-74             [-1, 577, 768]               0\n","         Identity-75             [-1, 577, 768]               0\n","        LayerNorm-76             [-1, 577, 768]           1,536\n","           Linear-77            [-1, 577, 3072]       2,362,368\n","             GELU-78            [-1, 577, 3072]               0\n","          Dropout-79            [-1, 577, 3072]               0\n","           Linear-80             [-1, 577, 768]       2,360,064\n","          Dropout-81             [-1, 577, 768]               0\n","              Mlp-82             [-1, 577, 768]               0\n","         Identity-83             [-1, 577, 768]               0\n","            Block-84             [-1, 577, 768]               0\n","        LayerNorm-85             [-1, 577, 768]           1,536\n","           Linear-86            [-1, 577, 2304]       1,771,776\n","          Dropout-87         [-1, 12, 577, 577]               0\n","           Linear-88             [-1, 577, 768]         590,592\n","          Dropout-89             [-1, 577, 768]               0\n","        Attention-90             [-1, 577, 768]               0\n","         Identity-91             [-1, 577, 768]               0\n","        LayerNorm-92             [-1, 577, 768]           1,536\n","           Linear-93            [-1, 577, 3072]       2,362,368\n","             GELU-94            [-1, 577, 3072]               0\n","          Dropout-95            [-1, 577, 3072]               0\n","           Linear-96             [-1, 577, 768]       2,360,064\n","          Dropout-97             [-1, 577, 768]               0\n","              Mlp-98             [-1, 577, 768]               0\n","         Identity-99             [-1, 577, 768]               0\n","           Block-100             [-1, 577, 768]               0\n","       LayerNorm-101             [-1, 577, 768]           1,536\n","          Linear-102            [-1, 577, 2304]       1,771,776\n","         Dropout-103         [-1, 12, 577, 577]               0\n","          Linear-104             [-1, 577, 768]         590,592\n","         Dropout-105             [-1, 577, 768]               0\n","       Attention-106             [-1, 577, 768]               0\n","        Identity-107             [-1, 577, 768]               0\n","       LayerNorm-108             [-1, 577, 768]           1,536\n","          Linear-109            [-1, 577, 3072]       2,362,368\n","            GELU-110            [-1, 577, 3072]               0\n","         Dropout-111            [-1, 577, 3072]               0\n","          Linear-112             [-1, 577, 768]       2,360,064\n","         Dropout-113             [-1, 577, 768]               0\n","             Mlp-114             [-1, 577, 768]               0\n","        Identity-115             [-1, 577, 768]               0\n","           Block-116             [-1, 577, 768]               0\n","       LayerNorm-117             [-1, 577, 768]           1,536\n","          Linear-118            [-1, 577, 2304]       1,771,776\n","         Dropout-119         [-1, 12, 577, 577]               0\n","          Linear-120             [-1, 577, 768]         590,592\n","         Dropout-121             [-1, 577, 768]               0\n","       Attention-122             [-1, 577, 768]               0\n","        Identity-123             [-1, 577, 768]               0\n","       LayerNorm-124             [-1, 577, 768]           1,536\n","          Linear-125            [-1, 577, 3072]       2,362,368\n","            GELU-126            [-1, 577, 3072]               0\n","         Dropout-127            [-1, 577, 3072]               0\n","          Linear-128             [-1, 577, 768]       2,360,064\n","         Dropout-129             [-1, 577, 768]               0\n","             Mlp-130             [-1, 577, 768]               0\n","        Identity-131             [-1, 577, 768]               0\n","           Block-132             [-1, 577, 768]               0\n","       LayerNorm-133             [-1, 577, 768]           1,536\n","          Linear-134            [-1, 577, 2304]       1,771,776\n","         Dropout-135         [-1, 12, 577, 577]               0\n","          Linear-136             [-1, 577, 768]         590,592\n","         Dropout-137             [-1, 577, 768]               0\n","       Attention-138             [-1, 577, 768]               0\n","        Identity-139             [-1, 577, 768]               0\n","       LayerNorm-140             [-1, 577, 768]           1,536\n","          Linear-141            [-1, 577, 3072]       2,362,368\n","            GELU-142            [-1, 577, 3072]               0\n","         Dropout-143            [-1, 577, 3072]               0\n","          Linear-144             [-1, 577, 768]       2,360,064\n","         Dropout-145             [-1, 577, 768]               0\n","             Mlp-146             [-1, 577, 768]               0\n","        Identity-147             [-1, 577, 768]               0\n","           Block-148             [-1, 577, 768]               0\n","       LayerNorm-149             [-1, 577, 768]           1,536\n","          Linear-150            [-1, 577, 2304]       1,771,776\n","         Dropout-151         [-1, 12, 577, 577]               0\n","          Linear-152             [-1, 577, 768]         590,592\n","         Dropout-153             [-1, 577, 768]               0\n","       Attention-154             [-1, 577, 768]               0\n","        Identity-155             [-1, 577, 768]               0\n","       LayerNorm-156             [-1, 577, 768]           1,536\n","          Linear-157            [-1, 577, 3072]       2,362,368\n","            GELU-158            [-1, 577, 3072]               0\n","         Dropout-159            [-1, 577, 3072]               0\n","          Linear-160             [-1, 577, 768]       2,360,064\n","         Dropout-161             [-1, 577, 768]               0\n","             Mlp-162             [-1, 577, 768]               0\n","        Identity-163             [-1, 577, 768]               0\n","           Block-164             [-1, 577, 768]               0\n","       LayerNorm-165             [-1, 577, 768]           1,536\n","          Linear-166            [-1, 577, 2304]       1,771,776\n","         Dropout-167         [-1, 12, 577, 577]               0\n","          Linear-168             [-1, 577, 768]         590,592\n","         Dropout-169             [-1, 577, 768]               0\n","       Attention-170             [-1, 577, 768]               0\n","        Identity-171             [-1, 577, 768]               0\n","       LayerNorm-172             [-1, 577, 768]           1,536\n","          Linear-173            [-1, 577, 3072]       2,362,368\n","            GELU-174            [-1, 577, 3072]               0\n","         Dropout-175            [-1, 577, 3072]               0\n","          Linear-176             [-1, 577, 768]       2,360,064\n","         Dropout-177             [-1, 577, 768]               0\n","             Mlp-178             [-1, 577, 768]               0\n","        Identity-179             [-1, 577, 768]               0\n","           Block-180             [-1, 577, 768]               0\n","       LayerNorm-181             [-1, 577, 768]           1,536\n","          Linear-182            [-1, 577, 2304]       1,771,776\n","         Dropout-183         [-1, 12, 577, 577]               0\n","          Linear-184             [-1, 577, 768]         590,592\n","         Dropout-185             [-1, 577, 768]               0\n","       Attention-186             [-1, 577, 768]               0\n","        Identity-187             [-1, 577, 768]               0\n","       LayerNorm-188             [-1, 577, 768]           1,536\n","          Linear-189            [-1, 577, 3072]       2,362,368\n","            GELU-190            [-1, 577, 3072]               0\n","         Dropout-191            [-1, 577, 3072]               0\n","          Linear-192             [-1, 577, 768]       2,360,064\n","         Dropout-193             [-1, 577, 768]               0\n","             Mlp-194             [-1, 577, 768]               0\n","        Identity-195             [-1, 577, 768]               0\n","           Block-196             [-1, 577, 768]               0\n","       LayerNorm-197             [-1, 577, 768]           1,536\n","        Identity-198                  [-1, 768]               0\n","          Linear-199                    [-1, 2]           1,538\n","================================================================\n","Total params: 85,648,130\n","Trainable params: 85,648,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1437.49\n","Params size (MB): 326.72\n","Estimated Total Size (MB): 1765.90\n","----------------------------------------------------------------\n","model : ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.7441401481628418\n","minibatch AVG loss: 3.6687774032354357\n","Epoch: 1     train index of 5 minibatch: 2      time used: 1.4956018924713135\n","minibatch AVG loss: 7.571217727661133\n","Epoch: 1     train index of 5 minibatch: 3      time used: 1.4971075057983398\n","minibatch AVG loss: 3.727121591567993\n","\n","Epoch: 1  train \n","Loss: 4.5737  Acc: 56.5217\n","benign precision: 53.3333  recall: 51.6129\n","benign sensitivity: 51.6129  specificity: 62.1622\n","benign FPR: 37.8378  NPV: 60.5263\n","benign TP: 16.0\n","benign TN: 23.0\n","benign FP: 14.0\n","benign FN: 15.0\n","malignant precision: 60.5263  recall: 62.1622\n","malignant sensitivity: 62.1622  specificity: 51.6129\n","malignant FPR: 48.3871  NPV: 53.3333\n","malignant TP: 23.0\n","malignant TN: 16.0\n","malignant FP: 15.0\n","malignant FN: 14.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 2.2580  Acc: 68.7500\n","benign precision: 100.0000  recall: 28.5714\n","benign sensitivity: 28.5714  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 64.2857\n","benign TP: 2.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 64.2857  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 28.5714\n","malignant FPR: 71.4286  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 2.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 1.7077031135559082\n","minibatch AVG loss: 1.0907865762710571\n","Epoch: 2     train index of 5 minibatch: 2      time used: 1.4962131977081299\n","minibatch AVG loss: 1.5192825436592101\n","Epoch: 2     train index of 5 minibatch: 3      time used: 1.4955356121063232\n","minibatch AVG loss: 1.0511570274829865\n","\n","Epoch: 2  train \n","Loss: 1.2288  Acc: 97.1014\n","benign precision: 96.5517  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.5517\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.9775  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 1.7114362716674805\n","minibatch AVG loss: 1.5082713842391968\n","Epoch: 3     train index of 5 minibatch: 2      time used: 1.496807336807251\n","minibatch AVG loss: 1.3993288367986678\n","Epoch: 3     train index of 5 minibatch: 3      time used: 1.4954562187194824\n","minibatch AVG loss: 1.5923651576042175\n","\n","Epoch: 3  train \n","Loss: 1.6835  Acc: 91.3043\n","benign precision: 92.0000  recall: 88.4615\n","benign sensitivity: 88.4615  specificity: 95.2381\n","benign FPR: 4.7619  NPV: 93.0233\n","benign TP: 23.0\n","benign TN: 40.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 93.0233  recall: 95.2381\n","malignant sensitivity: 95.2381  specificity: 88.4615\n","malignant FPR: 11.5385  NPV: 92.0000\n","malignant TP: 40.0\n","malignant TN: 23.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 1.5446  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 1.7071442604064941\n","minibatch AVG loss: 0.9646833539009094\n","Epoch: 4     train index of 5 minibatch: 2      time used: 1.4965085983276367\n","minibatch AVG loss: 2.229083979129791\n","Epoch: 4     train index of 5 minibatch: 3      time used: 1.4961066246032715\n","minibatch AVG loss: 0.9764763057231903\n","\n","Epoch: 4  train \n","Loss: 1.2248  Acc: 92.7536\n","benign precision: 92.5926  recall: 92.5926\n","benign sensitivity: 92.5926  specificity: 95.1220\n","benign FPR: 4.8780  NPV: 95.1220\n","benign TP: 25.0\n","benign TN: 39.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 95.1220  recall: 95.1220\n","malignant sensitivity: 95.1220  specificity: 92.5926\n","malignant FPR: 7.4074  NPV: 92.5926\n","malignant TP: 39.0\n","malignant TN: 25.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.8709  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 1.7053272724151611\n","minibatch AVG loss: 1.4362481713294983\n","Epoch: 5     train index of 5 minibatch: 2      time used: 1.4949710369110107\n","minibatch AVG loss: 1.3231420300900936\n","Epoch: 5     train index of 5 minibatch: 3      time used: 1.496140956878662\n","minibatch AVG loss: 1.1753409266471864\n","\n","Epoch: 5  train \n","Loss: 1.3619  Acc: 91.3043\n","benign precision: 90.3226  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 94.5946\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 90.3226\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 1.1209  Acc: 81.2500\n","benign precision: 70.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 70.0000\n","malignant TP: 6.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.707092523574829\n","minibatch AVG loss: 0.9035302996635437\n","Epoch: 6     train index of 5 minibatch: 2      time used: 1.4960551261901855\n","minibatch AVG loss: 0.938644153624773\n","Epoch: 6     train index of 5 minibatch: 3      time used: 1.495938777923584\n","minibatch AVG loss: 0.8000663228332996\n","\n","Epoch: 6  train \n","Loss: 0.8082  Acc: 95.6522\n","benign precision: 96.2963  recall: 96.2963\n","benign sensitivity: 96.2963  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 97.5610\n","benign TP: 26.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.5610  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 96.2963\n","malignant FPR: 3.7037  NPV: 96.2963\n","malignant TP: 40.0\n","malignant TN: 26.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.4153  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.7095472812652588\n","minibatch AVG loss: 1.90158109664917\n","Epoch: 7     train index of 5 minibatch: 2      time used: 1.496429204940796\n","minibatch AVG loss: 1.9632385492324829\n","Epoch: 7     train index of 5 minibatch: 3      time used: 1.4964900016784668\n","minibatch AVG loss: 0.41130131632089617\n","\n","Epoch: 7  train \n","Loss: 1.2671  Acc: 92.7536\n","benign precision: 93.5484  recall: 93.5484\n","benign sensitivity: 93.5484  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 94.5946\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 93.5484\n","malignant FPR: 6.4516  NPV: 93.5484\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.7010  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.7101914882659912\n","minibatch AVG loss: 1.158671647310257\n","Epoch: 8     train index of 5 minibatch: 2      time used: 1.4967997074127197\n","minibatch AVG loss: 1.8141955614089966\n","Epoch: 8     train index of 5 minibatch: 3      time used: 1.4982264041900635\n","minibatch AVG loss: 1.5985376119613648\n","\n","Epoch: 8  train \n","Loss: 1.3398  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5610\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5610  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 100.0000\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.4826  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.7080836296081543\n","minibatch AVG loss: 0.8927179127931595\n","Epoch: 9     train index of 5 minibatch: 2      time used: 1.4976017475128174\n","minibatch AVG loss: 1.6933664947748184\n","Epoch: 9     train index of 5 minibatch: 3      time used: 1.4956514835357666\n","minibatch AVG loss: 1.5838239908218383\n","\n","Epoch: 9  train \n","Loss: 1.3546  Acc: 88.4058\n","benign precision: 85.1852  recall: 88.4615\n","benign sensitivity: 88.4615  specificity: 90.4762\n","benign FPR: 9.5238  NPV: 92.6829\n","benign TP: 23.0\n","benign TN: 38.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 92.6829  recall: 90.4762\n","malignant sensitivity: 90.4762  specificity: 88.4615\n","malignant FPR: 11.5385  NPV: 85.1852\n","malignant TP: 38.0\n","malignant TN: 23.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.5805  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.711416244506836\n","minibatch AVG loss: 1.664270031452179\n","Epoch: 10     train index of 5 minibatch: 2      time used: 1.4970860481262207\n","minibatch AVG loss: 1.0593623757362365\n","Epoch: 10     train index of 5 minibatch: 3      time used: 1.4962072372436523\n","minibatch AVG loss: 0.8202298821881413\n","\n","Epoch: 10  train \n","Loss: 1.1283  Acc: 89.8551\n","benign precision: 90.9091  recall: 90.9091\n","benign sensitivity: 90.9091  specificity: 91.4286\n","benign FPR: 8.5714  NPV: 91.4286\n","benign TP: 30.0\n","benign TN: 32.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 91.4286  recall: 91.4286\n","malignant sensitivity: 91.4286  specificity: 90.9091\n","malignant FPR: 9.0909  NPV: 90.9091\n","malignant TP: 32.0\n","malignant TN: 30.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.4039  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.7085809707641602\n","minibatch AVG loss: 1.000584234111011\n","Epoch: 11     train index of 5 minibatch: 2      time used: 1.494513988494873\n","minibatch AVG loss: 0.6999397248029708\n","Epoch: 11     train index of 5 minibatch: 3      time used: 1.4975857734680176\n","minibatch AVG loss: 1.4028523832559585\n","\n","Epoch: 11  train \n","Loss: 0.9128  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.5000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.4652  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.7152926921844482\n","minibatch AVG loss: 1.427860403060913\n","Epoch: 12     train index of 5 minibatch: 2      time used: 1.4957895278930664\n","minibatch AVG loss: 1.5015159264206885\n","Epoch: 12     train index of 5 minibatch: 3      time used: 1.495297908782959\n","minibatch AVG loss: 1.5806774258613587\n","\n","Epoch: 12  train \n","Loss: 1.4228  Acc: 91.3043\n","benign precision: 82.1429  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 23.0\n","benign TN: 40.0\n","benign FP: 5.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 82.1429\n","malignant TP: 40.0\n","malignant TN: 23.0\n","malignant FP: 0.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.6028  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.7101285457611084\n","minibatch AVG loss: 0.7386524677276611\n","Epoch: 13     train index of 5 minibatch: 2      time used: 1.495790719985962\n","minibatch AVG loss: 1.0250412821769714\n","Epoch: 13     train index of 5 minibatch: 3      time used: 1.4964745044708252\n","minibatch AVG loss: 1.4224775552749633\n","\n","Epoch: 13  train \n","Loss: 1.0296  Acc: 95.6522\n","benign precision: 96.4286  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 97.5000\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 96.4286\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.5844  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.7136688232421875\n","minibatch AVG loss: 1.2958178222179413\n","Epoch: 14     train index of 5 minibatch: 2      time used: 1.4955666065216064\n","minibatch AVG loss: 1.1347618259489536\n","Epoch: 14     train index of 5 minibatch: 3      time used: 1.496201753616333\n","minibatch AVG loss: 1.1760519862174987\n","\n","Epoch: 14  train \n","Loss: 1.0898  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.3692  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.7117252349853516\n","minibatch AVG loss: 0.4489162042737007\n","Epoch: 15     train index of 5 minibatch: 2      time used: 1.494816780090332\n","minibatch AVG loss: 0.7547044545412064\n","Epoch: 15     train index of 5 minibatch: 3      time used: 1.4946603775024414\n","minibatch AVG loss: 1.2271460115909576\n","\n","Epoch: 15  train \n","Loss: 0.9297  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.3097  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.705366611480713\n","minibatch AVG loss: 1.1056346222758293\n","Epoch: 16     train index of 5 minibatch: 2      time used: 1.495795726776123\n","minibatch AVG loss: 1.167544674873352\n","Epoch: 16     train index of 5 minibatch: 3      time used: 1.498145580291748\n","minibatch AVG loss: 1.3321020543575286\n","\n","Epoch: 16  train \n","Loss: 1.1014  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.4191  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.7045979499816895\n","minibatch AVG loss: 1.061396563053131\n","Epoch: 17     train index of 5 minibatch: 2      time used: 1.4959373474121094\n","minibatch AVG loss: 0.5993124574422837\n","Epoch: 17     train index of 5 minibatch: 3      time used: 1.4952847957611084\n","minibatch AVG loss: 0.6163909643888473\n","\n","Epoch: 17  train \n","Loss: 0.6766  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.3108  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 1.706275224685669\n","minibatch AVG loss: 0.7146566316485405\n","Epoch: 18     train index of 5 minibatch: 2      time used: 1.495452880859375\n","minibatch AVG loss: 0.9125404089689255\n","Epoch: 18     train index of 5 minibatch: 3      time used: 1.4968528747558594\n","minibatch AVG loss: 1.296741744875908\n","\n","Epoch: 18  train \n","Loss: 0.9903  Acc: 97.1014\n","benign precision: 96.5517  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.5517\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.3579  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.708430290222168\n","minibatch AVG loss: 1.7460872650146484\n","Epoch: 19     train index of 5 minibatch: 2      time used: 1.4971003532409668\n","minibatch AVG loss: 1.0387409329414368\n","Epoch: 19     train index of 5 minibatch: 3      time used: 1.4959337711334229\n","minibatch AVG loss: 0.7041118770837784\n","\n","Epoch: 19  train \n","Loss: 1.0943  Acc: 95.6522\n","benign precision: 96.8750  recall: 96.8750\n","benign sensitivity: 96.8750  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 97.2222\n","benign TP: 31.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 96.8750\n","malignant FPR: 3.1250  NPV: 96.8750\n","malignant TP: 35.0\n","malignant TN: 31.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.4990  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.7090380191802979\n","minibatch AVG loss: 0.7111306637525558\n","Epoch: 20     train index of 5 minibatch: 2      time used: 1.4953014850616455\n","minibatch AVG loss: 0.8985775172710418\n","Epoch: 20     train index of 5 minibatch: 3      time used: 1.4963736534118652\n","minibatch AVG loss: 1.1230088472366333\n","\n","Epoch: 20  train \n","Loss: 0.9779  Acc: 92.7536\n","benign precision: 90.3226  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 97.2973\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 90.3226\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.3881  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.70908784866333\n","minibatch AVG loss: 0.5936555959284305\n","Epoch: 21     train index of 5 minibatch: 2      time used: 1.4947586059570312\n","minibatch AVG loss: 1.05779709815979\n","Epoch: 21     train index of 5 minibatch: 3      time used: 1.4950182437896729\n","minibatch AVG loss: 1.3037840589880942\n","\n","Epoch: 21  train \n","Loss: 0.9349  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.3684\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 100.0000\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.2994  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.7077605724334717\n","minibatch AVG loss: 0.8470809444785118\n","Epoch: 22     train index of 5 minibatch: 2      time used: 1.495513677597046\n","minibatch AVG loss: 1.1265375413000585\n","Epoch: 22     train index of 5 minibatch: 3      time used: 1.495187520980835\n","minibatch AVG loss: 0.3089657202363014\n","\n","Epoch: 22  train \n","Loss: 0.8844  Acc: 97.1014\n","benign precision: 96.9697  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.2222\n","benign FPR: 2.7778  NPV: 100.0000\n","benign TP: 32.0\n","benign TN: 35.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.2222\n","malignant sensitivity: 97.2222  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.9697\n","malignant TP: 35.0\n","malignant TN: 32.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.6426  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.710118055343628\n","minibatch AVG loss: 1.8754859924316407\n","Epoch: 23     train index of 5 minibatch: 2      time used: 1.4973211288452148\n","minibatch AVG loss: 1.6339555263519288\n","Epoch: 23     train index of 5 minibatch: 3      time used: 1.4971625804901123\n","minibatch AVG loss: 1.0902320623397828\n","\n","Epoch: 23  train \n","Loss: 1.4392  Acc: 89.8551\n","benign precision: 90.0000  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 92.1053\n","benign TP: 27.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 90.0000\n","malignant TP: 35.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.3711  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.708498239517212\n","minibatch AVG loss: 0.37358173727989197\n","Epoch: 24     train index of 5 minibatch: 2      time used: 1.4961419105529785\n","minibatch AVG loss: 1.2293566763401031\n","Epoch: 24     train index of 5 minibatch: 3      time used: 1.4962959289550781\n","minibatch AVG loss: 1.3259607076644897\n","\n","Epoch: 24  train \n","Loss: 0.9397  Acc: 92.7536\n","benign precision: 90.6250  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 97.2222\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 90.6250\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.2481  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.7079780101776123\n","minibatch AVG loss: 0.6296772360801697\n","Epoch: 25     train index of 5 minibatch: 2      time used: 1.496399164199829\n","minibatch AVG loss: 1.0016276627779006\n","Epoch: 25     train index of 5 minibatch: 3      time used: 1.4972474575042725\n","minibatch AVG loss: 1.6168649077415467\n","\n","Epoch: 25  train \n","Loss: 1.0153  Acc: 92.7536\n","benign precision: 93.1034  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 94.8718\n","benign TP: 27.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 93.1034\n","malignant TP: 37.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.3077  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.7058484554290771\n","minibatch AVG loss: 1.1841131411492825\n","Epoch: 26     train index of 5 minibatch: 2      time used: 1.4952924251556396\n","minibatch AVG loss: 1.1981160759925842\n","Epoch: 26     train index of 5 minibatch: 3      time used: 1.494645595550537\n","minibatch AVG loss: 1.3858074307441712\n","\n","Epoch: 26  train \n","Loss: 1.1650  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.5903  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.7087080478668213\n","minibatch AVG loss: 0.9735501319169998\n","Epoch: 27     train index of 5 minibatch: 2      time used: 1.4964146614074707\n","minibatch AVG loss: 0.5318513974547386\n","Epoch: 27     train index of 5 minibatch: 3      time used: 1.49635648727417\n","minibatch AVG loss: 0.3787068009376526\n","\n","Epoch: 27  train \n","Loss: 0.5957  Acc: 92.7536\n","benign precision: 100.0000  recall: 87.5000\n","benign sensitivity: 87.5000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 87.5000\n","malignant FPR: 12.5000  NPV: 100.0000\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.3074  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.708458423614502\n","minibatch AVG loss: 0.8913140416145324\n","Epoch: 28     train index of 5 minibatch: 2      time used: 1.497025728225708\n","minibatch AVG loss: 1.0808092296123504\n","Epoch: 28     train index of 5 minibatch: 3      time used: 1.4972143173217773\n","minibatch AVG loss: 0.9563512921333313\n","\n","Epoch: 28  train \n","Loss: 1.0256  Acc: 91.3043\n","benign precision: 92.8571  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 92.5000\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.5000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 92.8571\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.4261  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 1.7031009197235107\n","minibatch AVG loss: 0.7708495065569878\n","Epoch: 29     train index of 5 minibatch: 2      time used: 1.4955174922943115\n","minibatch AVG loss: 0.5432354658842087\n","Epoch: 29     train index of 5 minibatch: 3      time used: 1.4971771240234375\n","minibatch AVG loss: 0.5794022068381309\n","\n","Epoch: 29  train \n","Loss: 0.6920  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.2447  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.7050659656524658\n","minibatch AVG loss: 1.7282089561223983\n","Epoch: 30     train index of 5 minibatch: 2      time used: 1.495821475982666\n","minibatch AVG loss: 0.9984079092741013\n","Epoch: 30     train index of 5 minibatch: 3      time used: 1.4957010746002197\n","minibatch AVG loss: 1.0485459387302398\n","\n","Epoch: 30  train \n","Loss: 1.2384  Acc: 92.7536\n","benign precision: 96.9697  recall: 91.4286\n","benign sensitivity: 91.4286  specificity: 96.9697\n","benign FPR: 3.0303  NPV: 91.4286\n","benign TP: 32.0\n","benign TN: 32.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 91.4286  recall: 96.9697\n","malignant sensitivity: 96.9697  specificity: 91.4286\n","malignant FPR: 8.5714  NPV: 96.9697\n","malignant TP: 32.0\n","malignant TN: 32.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.3962  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.714233160018921\n","minibatch AVG loss: 0.9030541002750396\n","Epoch: 31     train index of 5 minibatch: 2      time used: 1.496659517288208\n","minibatch AVG loss: 0.9394805133342743\n","Epoch: 31     train index of 5 minibatch: 3      time used: 1.4966835975646973\n","minibatch AVG loss: 1.3854050993919373\n","\n","Epoch: 31  train \n","Loss: 1.0316  Acc: 89.8551\n","benign precision: 93.5484  recall: 87.8788\n","benign sensitivity: 87.8788  specificity: 94.2857\n","benign FPR: 5.7143  NPV: 89.1892\n","benign TP: 29.0\n","benign TN: 33.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 89.1892  recall: 94.2857\n","malignant sensitivity: 94.2857  specificity: 87.8788\n","malignant FPR: 12.1212  NPV: 93.5484\n","malignant TP: 33.0\n","malignant TN: 29.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.3679  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.7089500427246094\n","minibatch AVG loss: 0.6926990509033203\n","Epoch: 32     train index of 5 minibatch: 2      time used: 1.4965569972991943\n","minibatch AVG loss: 0.5776252716779708\n","Epoch: 32     train index of 5 minibatch: 3      time used: 1.4969508647918701\n","minibatch AVG loss: 0.8851897686719894\n","\n","Epoch: 32  train \n","Loss: 0.7311  Acc: 95.6522\n","benign precision: 93.5484  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.5484\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.3293  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.7073960304260254\n","minibatch AVG loss: 1.6174114108085633\n","Epoch: 33     train index of 5 minibatch: 2      time used: 1.4982266426086426\n","minibatch AVG loss: 1.1675823658704758\n","Epoch: 33     train index of 5 minibatch: 3      time used: 1.4959375858306885\n","minibatch AVG loss: 0.7817776650190353\n","\n","Epoch: 33  train \n","Loss: 1.1401  Acc: 95.6522\n","benign precision: 92.5926  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.3488\n","benign FPR: 4.6512  NPV: 100.0000\n","benign TP: 25.0\n","benign TN: 41.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.3488\n","malignant sensitivity: 95.3488  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 92.5926\n","malignant TP: 41.0\n","malignant TN: 25.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.2966  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 1.7084918022155762\n","minibatch AVG loss: 0.7681393086910248\n","Epoch: 34     train index of 5 minibatch: 2      time used: 1.4967856407165527\n","minibatch AVG loss: 0.47784054577350615\n","Epoch: 34     train index of 5 minibatch: 3      time used: 1.4961566925048828\n","minibatch AVG loss: 1.3586915612220765\n","\n","Epoch: 34  train \n","Loss: 0.7667  Acc: 97.1014\n","benign precision: 96.4286  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5610\n","benign FPR: 2.4390  NPV: 100.0000\n","benign TP: 27.0\n","benign TN: 40.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5610\n","malignant sensitivity: 97.5610  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.4286\n","malignant TP: 40.0\n","malignant TN: 27.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.3289  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.7074782848358154\n","minibatch AVG loss: 0.584180311858654\n","Epoch: 35     train index of 5 minibatch: 2      time used: 1.4947800636291504\n","minibatch AVG loss: 1.324773019552231\n","Epoch: 35     train index of 5 minibatch: 3      time used: 1.4950213432312012\n","minibatch AVG loss: 0.1700167790055275\n","\n","Epoch: 35  train \n","Loss: 0.6685  Acc: 95.6522\n","benign precision: 96.7742  recall: 96.7742\n","benign sensitivity: 96.7742  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 97.2973\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 96.7742\n","malignant FPR: 3.2258  NPV: 96.7742\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.2338  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.7048752307891846\n","minibatch AVG loss: 0.8551933489739895\n","Epoch: 36     train index of 5 minibatch: 2      time used: 1.4955270290374756\n","minibatch AVG loss: 0.9814208716154098\n","Epoch: 36     train index of 5 minibatch: 3      time used: 1.4958560466766357\n","minibatch AVG loss: 0.949674054980278\n","\n","Epoch: 36  train \n","Loss: 0.9851  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.2413  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.7053139209747314\n","minibatch AVG loss: 0.9246410876512527\n","Epoch: 37     train index of 5 minibatch: 2      time used: 1.495924472808838\n","minibatch AVG loss: 0.29297863245010375\n","Epoch: 37     train index of 5 minibatch: 3      time used: 1.495007038116455\n","minibatch AVG loss: 0.7290314882993698\n","\n","Epoch: 37  train \n","Loss: 0.6114  Acc: 95.6522\n","benign precision: 93.3333  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.3333\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.2611  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 1.7093782424926758\n","minibatch AVG loss: 1.4000482082366943\n","Epoch: 38     train index of 5 minibatch: 2      time used: 1.495743751525879\n","minibatch AVG loss: 0.9395031616091728\n","Epoch: 38     train index of 5 minibatch: 3      time used: 1.4964714050292969\n","minibatch AVG loss: 0.9431250289082527\n","\n","Epoch: 38  train \n","Loss: 1.1245  Acc: 95.6522\n","benign precision: 96.4286  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 97.5000\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.5000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 96.4286\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.2420  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.7069363594055176\n","minibatch AVG loss: 0.8294199556112289\n","Epoch: 39     train index of 5 minibatch: 2      time used: 1.4963245391845703\n","minibatch AVG loss: 1.1733127176761626\n","Epoch: 39     train index of 5 minibatch: 3      time used: 1.4961938858032227\n","minibatch AVG loss: 0.7858169615268707\n","\n","Epoch: 39  train \n","Loss: 0.9177  Acc: 95.6522\n","benign precision: 93.9394  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.5946\n","benign FPR: 5.4054  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 35.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.5946\n","malignant sensitivity: 94.5946  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.9394\n","malignant TP: 35.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.2741  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.7074663639068604\n","minibatch AVG loss: 1.2215978860855103\n","Epoch: 40     train index of 5 minibatch: 2      time used: 1.4953973293304443\n","minibatch AVG loss: 0.997480833530426\n","Epoch: 40     train index of 5 minibatch: 3      time used: 1.4959745407104492\n","minibatch AVG loss: 0.8274666234850884\n","\n","Epoch: 40  train \n","Loss: 1.0039  Acc: 97.1014\n","benign precision: 100.0000  recall: 97.1429\n","benign sensitivity: 97.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.0588\n","benign TP: 34.0\n","benign TN: 33.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.0588  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.1429\n","malignant FPR: 2.8571  NPV: 100.0000\n","malignant TP: 33.0\n","malignant TN: 34.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.2898  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 1.7090325355529785\n","minibatch AVG loss: 0.3696776509284973\n","Epoch: 41     train index of 5 minibatch: 2      time used: 1.4970436096191406\n","minibatch AVG loss: 0.675983864068985\n","Epoch: 41     train index of 5 minibatch: 3      time used: 1.496135950088501\n","minibatch AVG loss: 0.5827428251504898\n","\n","Epoch: 41  train \n","Loss: 0.5769  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 25.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 25.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.2494  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.7070400714874268\n","minibatch AVG loss: 1.0301907509565353\n","Epoch: 42     train index of 5 minibatch: 2      time used: 1.4958512783050537\n","minibatch AVG loss: 0.7359848357737064\n","Epoch: 42     train index of 5 minibatch: 3      time used: 1.4973716735839844\n","minibatch AVG loss: 0.9770910859107971\n","\n","Epoch: 42  train \n","Loss: 0.9248  Acc: 92.7536\n","benign precision: 90.6250  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 97.2222\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 90.6250\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.2276  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.7089483737945557\n","minibatch AVG loss: 0.7840575948357582\n","Epoch: 43     train index of 5 minibatch: 2      time used: 1.4959416389465332\n","minibatch AVG loss: 0.741027170419693\n","Epoch: 43     train index of 5 minibatch: 3      time used: 1.496997594833374\n","minibatch AVG loss: 1.4412461310625075\n","\n","Epoch: 43  train \n","Loss: 0.9274  Acc: 92.7536\n","benign precision: 86.6667  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 90.4762\n","benign FPR: 9.5238  NPV: 100.0000\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 4.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 90.4762\n","malignant sensitivity: 90.4762  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 86.6667\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 0.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.2537  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.7078361511230469\n","minibatch AVG loss: 0.7353893294930458\n","Epoch: 44     train index of 5 minibatch: 2      time used: 1.4964096546173096\n","minibatch AVG loss: 0.6749537050724029\n","Epoch: 44     train index of 5 minibatch: 3      time used: 1.4945638179779053\n","minibatch AVG loss: 1.145674431324005\n","\n","Epoch: 44  train \n","Loss: 0.8543  Acc: 94.2029\n","benign precision: 96.6667  recall: 93.5484\n","benign sensitivity: 93.5484  specificity: 97.2973\n","benign FPR: 2.7027  NPV: 94.7368\n","benign TP: 29.0\n","benign TN: 36.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 97.2973\n","malignant sensitivity: 97.2973  specificity: 93.5484\n","malignant FPR: 6.4516  NPV: 96.6667\n","malignant TP: 36.0\n","malignant TN: 29.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.2737  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.7110881805419922\n","minibatch AVG loss: 0.7784850314259529\n","Epoch: 45     train index of 5 minibatch: 2      time used: 1.495504379272461\n","minibatch AVG loss: 0.9406803138554096\n","Epoch: 45     train index of 5 minibatch: 3      time used: 1.4943199157714844\n","minibatch AVG loss: 0.18372918367385865\n","\n","Epoch: 45  train \n","Loss: 0.5946  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 31.0\n","benign TN: 37.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 37.0\n","malignant TN: 31.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.2739  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.7068510055541992\n","minibatch AVG loss: 0.7951881550252438\n","Epoch: 46     train index of 5 minibatch: 2      time used: 1.4952616691589355\n","minibatch AVG loss: 0.7708646558225155\n","Epoch: 46     train index of 5 minibatch: 3      time used: 1.4950308799743652\n","minibatch AVG loss: 0.7410605922341347\n","\n","Epoch: 46  train \n","Loss: 0.7618  Acc: 92.7536\n","benign precision: 89.6552  recall: 96.2963\n","benign sensitivity: 96.2963  specificity: 92.6829\n","benign FPR: 7.3171  NPV: 97.4359\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 92.6829\n","malignant sensitivity: 92.6829  specificity: 96.2963\n","malignant FPR: 3.7037  NPV: 89.6552\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.2279  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.7087876796722412\n","minibatch AVG loss: 1.0186659321188927\n","Epoch: 47     train index of 5 minibatch: 2      time used: 1.4942152500152588\n","minibatch AVG loss: 0.5137719169259072\n","Epoch: 47     train index of 5 minibatch: 3      time used: 1.4956903457641602\n","minibatch AVG loss: 0.8730425864458085\n","\n","Epoch: 47  train \n","Loss: 0.7360  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 32.0\n","benign TN: 36.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 36.0\n","malignant TN: 32.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.2155  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.70969557762146\n","minibatch AVG loss: 1.629587459564209\n","Epoch: 48     train index of 5 minibatch: 2      time used: 1.4954204559326172\n","minibatch AVG loss: 0.624584986269474\n","Epoch: 48     train index of 5 minibatch: 3      time used: 1.4963171482086182\n","minibatch AVG loss: 0.5070672199130059\n","\n","Epoch: 48  train \n","Loss: 0.8604  Acc: 97.1014\n","benign precision: 96.5517  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.5000\n","benign FPR: 2.5000  NPV: 100.0000\n","benign TP: 28.0\n","benign TN: 39.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.5000\n","malignant sensitivity: 97.5000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.5517\n","malignant TP: 39.0\n","malignant TN: 28.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.2278  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.7076306343078613\n","minibatch AVG loss: 0.7871403284370899\n","Epoch: 49     train index of 5 minibatch: 2      time used: 1.4959793090820312\n","minibatch AVG loss: 0.7711777210235595\n","Epoch: 49     train index of 5 minibatch: 3      time used: 1.4962055683135986\n","minibatch AVG loss: 0.8142141878604889\n","\n","Epoch: 49  train \n","Loss: 0.8281  Acc: 94.2029\n","benign precision: 93.1034  recall: 96.4286\n","benign sensitivity: 96.4286  specificity: 95.0000\n","benign FPR: 5.0000  NPV: 97.4359\n","benign TP: 27.0\n","benign TN: 38.0\n","benign FP: 2.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 95.0000\n","malignant sensitivity: 95.0000  specificity: 96.4286\n","malignant FPR: 3.5714  NPV: 93.1034\n","malignant TP: 38.0\n","malignant TN: 27.0\n","malignant FP: 1.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.2358  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.7086915969848633\n","minibatch AVG loss: 0.47063594982028006\n","Epoch: 50     train index of 5 minibatch: 2      time used: 1.4969685077667236\n","minibatch AVG loss: 0.3751295879483223\n","Epoch: 50     train index of 5 minibatch: 3      time used: 1.4941847324371338\n","minibatch AVG loss: 0.5799851715564728\n","\n","Epoch: 50  train \n","Loss: 0.5311  Acc: 94.2029\n","benign precision: 89.6552  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 92.8571\n","benign FPR: 7.1429  NPV: 100.0000\n","benign TP: 26.0\n","benign TN: 39.0\n","benign FP: 3.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 92.8571\n","malignant sensitivity: 92.8571  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 89.6552\n","malignant TP: 39.0\n","malignant TN: 26.0\n","malignant FP: 0.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.2518  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 4m 56s\n","Best epoch idx:  47\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS.pth\n"]}],"source":["!python Train.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS --augmentation_name Mixup --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2FSe1lRILyYK"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ResNet50_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","100% 97.8M/97.8M [00:00\u003c00:00, 252MB/s]\n","test model output： tensor([[0.1968, 0.2936]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 192, 192]           9,408\n","       BatchNorm2d-2         [-1, 64, 192, 192]             128\n","              ReLU-3         [-1, 64, 192, 192]               0\n","         MaxPool2d-4           [-1, 64, 96, 96]               0\n","            Conv2d-5           [-1, 64, 96, 96]           4,096\n","       BatchNorm2d-6           [-1, 64, 96, 96]             128\n","              ReLU-7           [-1, 64, 96, 96]               0\n","            Conv2d-8           [-1, 64, 96, 96]          36,864\n","       BatchNorm2d-9           [-1, 64, 96, 96]             128\n","             ReLU-10           [-1, 64, 96, 96]               0\n","           Conv2d-11          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-12          [-1, 256, 96, 96]             512\n","           Conv2d-13          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-14          [-1, 256, 96, 96]             512\n","             ReLU-15          [-1, 256, 96, 96]               0\n","       Bottleneck-16          [-1, 256, 96, 96]               0\n","           Conv2d-17           [-1, 64, 96, 96]          16,384\n","      BatchNorm2d-18           [-1, 64, 96, 96]             128\n","             ReLU-19           [-1, 64, 96, 96]               0\n","           Conv2d-20           [-1, 64, 96, 96]          36,864\n","      BatchNorm2d-21           [-1, 64, 96, 96]             128\n","             ReLU-22           [-1, 64, 96, 96]               0\n","           Conv2d-23          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-24          [-1, 256, 96, 96]             512\n","             ReLU-25          [-1, 256, 96, 96]               0\n","       Bottleneck-26          [-1, 256, 96, 96]               0\n","           Conv2d-27           [-1, 64, 96, 96]          16,384\n","      BatchNorm2d-28           [-1, 64, 96, 96]             128\n","             ReLU-29           [-1, 64, 96, 96]               0\n","           Conv2d-30           [-1, 64, 96, 96]          36,864\n","      BatchNorm2d-31           [-1, 64, 96, 96]             128\n","             ReLU-32           [-1, 64, 96, 96]               0\n","           Conv2d-33          [-1, 256, 96, 96]          16,384\n","      BatchNorm2d-34          [-1, 256, 96, 96]             512\n","             ReLU-35          [-1, 256, 96, 96]               0\n","       Bottleneck-36          [-1, 256, 96, 96]               0\n","           Conv2d-37          [-1, 128, 96, 96]          32,768\n","      BatchNorm2d-38          [-1, 128, 96, 96]             256\n","             ReLU-39          [-1, 128, 96, 96]               0\n","           Conv2d-40          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-41          [-1, 128, 48, 48]             256\n","             ReLU-42          [-1, 128, 48, 48]               0\n","           Conv2d-43          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-44          [-1, 512, 48, 48]           1,024\n","           Conv2d-45          [-1, 512, 48, 48]         131,072\n","      BatchNorm2d-46          [-1, 512, 48, 48]           1,024\n","             ReLU-47          [-1, 512, 48, 48]               0\n","       Bottleneck-48          [-1, 512, 48, 48]               0\n","           Conv2d-49          [-1, 128, 48, 48]          65,536\n","      BatchNorm2d-50          [-1, 128, 48, 48]             256\n","             ReLU-51          [-1, 128, 48, 48]               0\n","           Conv2d-52          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-53          [-1, 128, 48, 48]             256\n","             ReLU-54          [-1, 128, 48, 48]               0\n","           Conv2d-55          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-56          [-1, 512, 48, 48]           1,024\n","             ReLU-57          [-1, 512, 48, 48]               0\n","       Bottleneck-58          [-1, 512, 48, 48]               0\n","           Conv2d-59          [-1, 128, 48, 48]          65,536\n","      BatchNorm2d-60          [-1, 128, 48, 48]             256\n","             ReLU-61          [-1, 128, 48, 48]               0\n","           Conv2d-62          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-63          [-1, 128, 48, 48]             256\n","             ReLU-64          [-1, 128, 48, 48]               0\n","           Conv2d-65          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-66          [-1, 512, 48, 48]           1,024\n","             ReLU-67          [-1, 512, 48, 48]               0\n","       Bottleneck-68          [-1, 512, 48, 48]               0\n","           Conv2d-69          [-1, 128, 48, 48]          65,536\n","      BatchNorm2d-70          [-1, 128, 48, 48]             256\n","             ReLU-71          [-1, 128, 48, 48]               0\n","           Conv2d-72          [-1, 128, 48, 48]         147,456\n","      BatchNorm2d-73          [-1, 128, 48, 48]             256\n","             ReLU-74          [-1, 128, 48, 48]               0\n","           Conv2d-75          [-1, 512, 48, 48]          65,536\n","      BatchNorm2d-76          [-1, 512, 48, 48]           1,024\n","             ReLU-77          [-1, 512, 48, 48]               0\n","       Bottleneck-78          [-1, 512, 48, 48]               0\n","           Conv2d-79          [-1, 256, 48, 48]         131,072\n","      BatchNorm2d-80          [-1, 256, 48, 48]             512\n","             ReLU-81          [-1, 256, 48, 48]               0\n","           Conv2d-82          [-1, 256, 24, 24]         589,824\n","      BatchNorm2d-83          [-1, 256, 24, 24]             512\n","             ReLU-84          [-1, 256, 24, 24]               0\n","           Conv2d-85         [-1, 1024, 24, 24]         262,144\n","      BatchNorm2d-86         [-1, 1024, 24, 24]           2,048\n","           Conv2d-87         [-1, 1024, 24, 24]         524,288\n","      BatchNorm2d-88         [-1, 1024, 24, 24]           2,048\n","             ReLU-89         [-1, 1024, 24, 24]               0\n","       Bottleneck-90         [-1, 1024, 24, 24]               0\n","           Conv2d-91          [-1, 256, 24, 24]         262,144\n","      BatchNorm2d-92          [-1, 256, 24, 24]             512\n","             ReLU-93          [-1, 256, 24, 24]               0\n","           Conv2d-94          [-1, 256, 24, 24]         589,824\n","      BatchNorm2d-95          [-1, 256, 24, 24]             512\n","             ReLU-96          [-1, 256, 24, 24]               0\n","           Conv2d-97         [-1, 1024, 24, 24]         262,144\n","      BatchNorm2d-98         [-1, 1024, 24, 24]           2,048\n","             ReLU-99         [-1, 1024, 24, 24]               0\n","      Bottleneck-100         [-1, 1024, 24, 24]               0\n","          Conv2d-101          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-102          [-1, 256, 24, 24]             512\n","            ReLU-103          [-1, 256, 24, 24]               0\n","          Conv2d-104          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-105          [-1, 256, 24, 24]             512\n","            ReLU-106          [-1, 256, 24, 24]               0\n","          Conv2d-107         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-108         [-1, 1024, 24, 24]           2,048\n","            ReLU-109         [-1, 1024, 24, 24]               0\n","      Bottleneck-110         [-1, 1024, 24, 24]               0\n","          Conv2d-111          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-112          [-1, 256, 24, 24]             512\n","            ReLU-113          [-1, 256, 24, 24]               0\n","          Conv2d-114          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-115          [-1, 256, 24, 24]             512\n","            ReLU-116          [-1, 256, 24, 24]               0\n","          Conv2d-117         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-118         [-1, 1024, 24, 24]           2,048\n","            ReLU-119         [-1, 1024, 24, 24]               0\n","      Bottleneck-120         [-1, 1024, 24, 24]               0\n","          Conv2d-121          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-122          [-1, 256, 24, 24]             512\n","            ReLU-123          [-1, 256, 24, 24]               0\n","          Conv2d-124          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-125          [-1, 256, 24, 24]             512\n","            ReLU-126          [-1, 256, 24, 24]               0\n","          Conv2d-127         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-128         [-1, 1024, 24, 24]           2,048\n","            ReLU-129         [-1, 1024, 24, 24]               0\n","      Bottleneck-130         [-1, 1024, 24, 24]               0\n","          Conv2d-131          [-1, 256, 24, 24]         262,144\n","     BatchNorm2d-132          [-1, 256, 24, 24]             512\n","            ReLU-133          [-1, 256, 24, 24]               0\n","          Conv2d-134          [-1, 256, 24, 24]         589,824\n","     BatchNorm2d-135          [-1, 256, 24, 24]             512\n","            ReLU-136          [-1, 256, 24, 24]               0\n","          Conv2d-137         [-1, 1024, 24, 24]         262,144\n","     BatchNorm2d-138         [-1, 1024, 24, 24]           2,048\n","            ReLU-139         [-1, 1024, 24, 24]               0\n","      Bottleneck-140         [-1, 1024, 24, 24]               0\n","          Conv2d-141          [-1, 512, 24, 24]         524,288\n","     BatchNorm2d-142          [-1, 512, 24, 24]           1,024\n","            ReLU-143          [-1, 512, 24, 24]               0\n","          Conv2d-144          [-1, 512, 12, 12]       2,359,296\n","     BatchNorm2d-145          [-1, 512, 12, 12]           1,024\n","            ReLU-146          [-1, 512, 12, 12]               0\n","          Conv2d-147         [-1, 2048, 12, 12]       1,048,576\n","     BatchNorm2d-148         [-1, 2048, 12, 12]           4,096\n","          Conv2d-149         [-1, 2048, 12, 12]       2,097,152\n","     BatchNorm2d-150         [-1, 2048, 12, 12]           4,096\n","            ReLU-151         [-1, 2048, 12, 12]               0\n","      Bottleneck-152         [-1, 2048, 12, 12]               0\n","          Conv2d-153          [-1, 512, 12, 12]       1,048,576\n","     BatchNorm2d-154          [-1, 512, 12, 12]           1,024\n","            ReLU-155          [-1, 512, 12, 12]               0\n","          Conv2d-156          [-1, 512, 12, 12]       2,359,296\n","     BatchNorm2d-157          [-1, 512, 12, 12]           1,024\n","            ReLU-158          [-1, 512, 12, 12]               0\n","          Conv2d-159         [-1, 2048, 12, 12]       1,048,576\n","     BatchNorm2d-160         [-1, 2048, 12, 12]           4,096\n","            ReLU-161         [-1, 2048, 12, 12]               0\n","      Bottleneck-162         [-1, 2048, 12, 12]               0\n","          Conv2d-163          [-1, 512, 12, 12]       1,048,576\n","     BatchNorm2d-164          [-1, 512, 12, 12]           1,024\n","            ReLU-165          [-1, 512, 12, 12]               0\n","          Conv2d-166          [-1, 512, 12, 12]       2,359,296\n","     BatchNorm2d-167          [-1, 512, 12, 12]           1,024\n","            ReLU-168          [-1, 512, 12, 12]               0\n","          Conv2d-169         [-1, 2048, 12, 12]       1,048,576\n","     BatchNorm2d-170         [-1, 2048, 12, 12]           4,096\n","            ReLU-171         [-1, 2048, 12, 12]               0\n","      Bottleneck-172         [-1, 2048, 12, 12]               0\n","AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n","          Linear-174                    [-1, 2]           4,098\n","================================================================\n","Total params: 23,512,130\n","Trainable params: 23,512,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 842.08\n","Params size (MB): 89.69\n","Estimated Total Size (MB): 933.46\n","----------------------------------------------------------------\n","model : ResNet50_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 0.7714395523071289\n","minibatch AVG loss: 0.6562294840812684\n","Epoch: 1     train index of 5 minibatch: 2      time used: 0.532381534576416\n","minibatch AVG loss: 0.6783202886581421\n","Epoch: 1     train index of 5 minibatch: 3      time used: 0.5316786766052246\n","minibatch AVG loss: 0.6549890041351318\n","\n","Epoch: 1  train \n","Loss: 0.6451  Acc: 71.0145\n","benign precision: 76.1905  recall: 53.3333\n","benign sensitivity: 53.3333  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 70.2128\n","benign TP: 16.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 14.0\n","malignant precision: 70.2128  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 53.3333\n","malignant FPR: 46.6667  NPV: 76.1905\n","malignant TP: 33.0\n","malignant TN: 16.0\n","malignant FP: 14.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.6879  Acc: 56.2500\n","benign precision: 0.0000  recall: 0.0000\n","benign sensitivity: 0.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 56.2500\n","benign TP: 0.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 7.0\n","malignant precision: 56.2500  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 0.0000\n","malignant FPR: 100.0000  NPV: 0.0000\n","malignant TP: 9.0\n","malignant TN: 0.0\n","malignant FP: 7.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 0.7409040927886963\n","minibatch AVG loss: 0.566865611076355\n","Epoch: 2     train index of 5 minibatch: 2      time used: 0.5341739654541016\n","minibatch AVG loss: 0.6134220004081726\n","Epoch: 2     train index of 5 minibatch: 3      time used: 0.5331964492797852\n","minibatch AVG loss: 0.5467191100120544\n","\n","Epoch: 2  train \n","Loss: 0.5701  Acc: 82.6087\n","benign precision: 87.5000  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 81.8182\n","benign TP: 21.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 8.0\n","malignant precision: 81.8182  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 87.5000\n","malignant TP: 36.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.6533  Acc: 62.5000\n","benign precision: 100.0000  recall: 14.2857\n","benign sensitivity: 14.2857  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 60.0000\n","benign TP: 1.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 6.0\n","malignant precision: 60.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 14.2857\n","malignant FPR: 85.7143  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 1.0\n","malignant FP: 6.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 0.7443699836730957\n","minibatch AVG loss: 0.5246207118034363\n","Epoch: 3     train index of 5 minibatch: 2      time used: 0.5334174633026123\n","minibatch AVG loss: 0.5611652135848999\n","Epoch: 3     train index of 5 minibatch: 3      time used: 0.5327091217041016\n","minibatch AVG loss: 0.48119449615478516\n","\n","Epoch: 3  train \n","Loss: 0.5041  Acc: 84.0580\n","benign precision: 85.1852  recall: 79.3103\n","benign sensitivity: 79.3103  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 85.3659\n","benign TP: 23.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 6.0\n","malignant precision: 85.3659  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 79.3103\n","malignant FPR: 20.6897  NPV: 85.1852\n","malignant TP: 35.0\n","malignant TN: 23.0\n","malignant FP: 6.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.6328  Acc: 62.5000\n","benign precision: 100.0000  recall: 14.2857\n","benign sensitivity: 14.2857  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 60.0000\n","benign TP: 1.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 6.0\n","malignant precision: 60.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 14.2857\n","malignant FPR: 85.7143  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 1.0\n","malignant FP: 6.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 0.7477960586547852\n","minibatch AVG loss: 0.3989856123924255\n","Epoch: 4     train index of 5 minibatch: 2      time used: 0.5327267646789551\n","minibatch AVG loss: 0.4535239636898041\n","Epoch: 4     train index of 5 minibatch: 3      time used: 0.5318019390106201\n","minibatch AVG loss: 0.48679528534412386\n","\n","Epoch: 4  train \n","Loss: 0.4418  Acc: 86.9565\n","benign precision: 95.8333  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 84.0909\n","benign TP: 23.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 7.0\n","malignant precision: 84.0909  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 95.8333\n","malignant TP: 37.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.5749  Acc: 68.7500\n","benign precision: 100.0000  recall: 28.5714\n","benign sensitivity: 28.5714  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 64.2857\n","benign TP: 2.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 64.2857  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 28.5714\n","malignant FPR: 71.4286  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 2.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 0.7405643463134766\n","minibatch AVG loss: 0.5053507089614868\n","Epoch: 5     train index of 5 minibatch: 2      time used: 0.5325663089752197\n","minibatch AVG loss: 0.34340018928050997\n","Epoch: 5     train index of 5 minibatch: 3      time used: 0.5322911739349365\n","minibatch AVG loss: 0.5252901613712311\n","\n","Epoch: 5  train \n","Loss: 0.4713  Acc: 76.8116\n","benign precision: 80.0000  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 76.7442\n","benign TP: 20.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 10.0\n","malignant precision: 76.7442  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 80.0000\n","malignant TP: 33.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.4822  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 0.7475619316101074\n","minibatch AVG loss: 0.35577417612075807\n","Epoch: 6     train index of 5 minibatch: 2      time used: 0.5336105823516846\n","minibatch AVG loss: 0.33589729070663454\n","Epoch: 6     train index of 5 minibatch: 3      time used: 0.5322620868682861\n","minibatch AVG loss: 0.22234565019607544\n","\n","Epoch: 6  train \n","Loss: 0.2945  Acc: 89.8551\n","benign precision: 96.1538  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 88.0952\n","benign TP: 25.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 5.0\n","malignant precision: 88.0952  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 96.1538\n","malignant TP: 37.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.4052  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 0.740767240524292\n","minibatch AVG loss: 0.3238190710544586\n","Epoch: 7     train index of 5 minibatch: 2      time used: 0.5334062576293945\n","minibatch AVG loss: 0.24930836260318756\n","Epoch: 7     train index of 5 minibatch: 3      time used: 0.532834529876709\n","minibatch AVG loss: 0.46507923007011415\n","\n","Epoch: 7  train \n","Loss: 0.3296  Acc: 84.0580\n","benign precision: 88.0000  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 83.7209\n","benign TP: 22.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 7.0\n","malignant precision: 83.7209  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 88.0000\n","malignant TP: 36.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.3955  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 0.7494096755981445\n","minibatch AVG loss: 0.255033141374588\n","Epoch: 8     train index of 5 minibatch: 2      time used: 0.5327250957489014\n","minibatch AVG loss: 0.5569072544574738\n","Epoch: 8     train index of 5 minibatch: 3      time used: 0.5330376625061035\n","minibatch AVG loss: 0.2770630717277527\n","\n","Epoch: 8  train \n","Loss: 0.3561  Acc: 78.2609\n","benign precision: 80.0000  recall: 68.9655\n","benign sensitivity: 68.9655  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 79.0698\n","benign TP: 20.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 9.0\n","malignant precision: 79.0698  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 68.9655\n","malignant FPR: 31.0345  NPV: 80.0000\n","malignant TP: 34.0\n","malignant TN: 20.0\n","malignant FP: 9.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.4340  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 0.7462902069091797\n","minibatch AVG loss: 0.3441623657941818\n","Epoch: 9     train index of 5 minibatch: 2      time used: 0.5318446159362793\n","minibatch AVG loss: 0.20116932690143585\n","Epoch: 9     train index of 5 minibatch: 3      time used: 0.533980131149292\n","minibatch AVG loss: 0.2548902928829193\n","\n","Epoch: 9  train \n","Loss: 0.2686  Acc: 88.4058\n","benign precision: 87.0968  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 91.8919\n","benign TP: 27.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 91.8919  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 87.0968\n","malignant TP: 34.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.3746  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 0.7436683177947998\n","minibatch AVG loss: 0.3725244030356407\n","Epoch: 10     train index of 5 minibatch: 2      time used: 0.5325558185577393\n","minibatch AVG loss: 0.11407045125961304\n","Epoch: 10     train index of 5 minibatch: 3      time used: 0.5324959754943848\n","minibatch AVG loss: 0.20151303708553314\n","\n","Epoch: 10  train \n","Loss: 0.2226  Acc: 89.8551\n","benign precision: 84.8485  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 97.1429\n","benign TP: 28.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 1.0\n","malignant precision: 97.1429  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 84.8485\n","malignant TP: 34.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.3438  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 0.7506744861602783\n","minibatch AVG loss: 0.21096485406160354\n","Epoch: 11     train index of 5 minibatch: 2      time used: 0.5330812931060791\n","minibatch AVG loss: 0.12819915562868117\n","Epoch: 11     train index of 5 minibatch: 3      time used: 0.5317041873931885\n","minibatch AVG loss: 0.3073341310024261\n","\n","Epoch: 11  train \n","Loss: 0.1965  Acc: 94.2029\n","benign precision: 96.5517  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 94.8718\n","benign TP: 28.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 94.8718  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 96.5517\n","malignant TP: 37.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.3190  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 0.7416296005249023\n","minibatch AVG loss: 0.11092791929841042\n","Epoch: 12     train index of 5 minibatch: 2      time used: 0.5322787761688232\n","minibatch AVG loss: 0.4237965777516365\n","Epoch: 12     train index of 5 minibatch: 3      time used: 0.5317933559417725\n","minibatch AVG loss: 0.18542249798774718\n","\n","Epoch: 12  train \n","Loss: 0.2409  Acc: 85.5072\n","benign precision: 92.0000  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 83.7209\n","benign TP: 23.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 7.0\n","malignant precision: 83.7209  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 92.0000\n","malignant TP: 36.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.3841  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 0.7416098117828369\n","minibatch AVG loss: 0.28473923429846765\n","Epoch: 13     train index of 5 minibatch: 2      time used: 0.5336899757385254\n","minibatch AVG loss: 0.19968022555112838\n","Epoch: 13     train index of 5 minibatch: 3      time used: 0.5318937301635742\n","minibatch AVG loss: 0.22483398467302323\n","\n","Epoch: 13  train \n","Loss: 0.2160  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.2736  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 0.7481613159179688\n","minibatch AVG loss: 0.6309147775173187\n","Epoch: 14     train index of 5 minibatch: 2      time used: 0.5326080322265625\n","minibatch AVG loss: 0.16866954416036606\n","Epoch: 14     train index of 5 minibatch: 3      time used: 0.5327165126800537\n","minibatch AVG loss: 0.1795218899846077\n","\n","Epoch: 14  train \n","Loss: 0.3047  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.2578  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 0.7400567531585693\n","minibatch AVG loss: 0.27591161131858827\n","Epoch: 15     train index of 5 minibatch: 2      time used: 0.5333762168884277\n","minibatch AVG loss: 0.1424531526863575\n","Epoch: 15     train index of 5 minibatch: 3      time used: 0.5320794582366943\n","minibatch AVG loss: 0.08282303661108018\n","\n","Epoch: 15  train \n","Loss: 0.1703  Acc: 92.7536\n","benign precision: 90.6250  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 97.2222\n","benign TP: 29.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 90.6250\n","malignant TP: 35.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.2121  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 0.7421774864196777\n","minibatch AVG loss: 0.12985946238040924\n","Epoch: 16     train index of 5 minibatch: 2      time used: 0.53236985206604\n","minibatch AVG loss: 0.3939065754413605\n","Epoch: 16     train index of 5 minibatch: 3      time used: 0.531541109085083\n","minibatch AVG loss: 0.26143098026514056\n","\n","Epoch: 16  train \n","Loss: 0.2382  Acc: 86.9565\n","benign precision: 84.3750  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 91.6667\n","benign TP: 27.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 3.0\n","malignant precision: 91.6667  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 84.3750\n","malignant TP: 33.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.2457  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 0.7388663291931152\n","minibatch AVG loss: 0.22119905948638915\n","Epoch: 17     train index of 5 minibatch: 2      time used: 0.5321645736694336\n","minibatch AVG loss: 0.13670201674103738\n","Epoch: 17     train index of 5 minibatch: 3      time used: 0.5327060222625732\n","minibatch AVG loss: 0.12561143934726715\n","\n","Epoch: 17  train \n","Loss: 0.1441  Acc: 92.7536\n","benign precision: 90.3226  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 97.2973\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 90.3226\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.2596  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 0.7385811805725098\n","minibatch AVG loss: 0.0763196550309658\n","Epoch: 18     train index of 5 minibatch: 2      time used: 0.5329005718231201\n","minibatch AVG loss: 0.385254193097353\n","Epoch: 18     train index of 5 minibatch: 3      time used: 0.534111499786377\n","minibatch AVG loss: 0.268424104899168\n","\n","Epoch: 18  train \n","Loss: 0.2215  Acc: 88.4058\n","benign precision: 92.5926  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 87.8049\n","benign TP: 25.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 5.0\n","malignant precision: 87.8049  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 92.5926\n","malignant TP: 36.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.2469  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 0.7458183765411377\n","minibatch AVG loss: 0.31059515066444876\n","Epoch: 19     train index of 5 minibatch: 2      time used: 0.5323867797851562\n","minibatch AVG loss: 0.1157656718045473\n","Epoch: 19     train index of 5 minibatch: 3      time used: 0.5324313640594482\n","minibatch AVG loss: 0.09837935827672481\n","\n","Epoch: 19  train \n","Loss: 0.1552  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.0000\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.2207  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 0.7469820976257324\n","minibatch AVG loss: 0.46602662801742556\n","Epoch: 20     train index of 5 minibatch: 2      time used: 0.5321915149688721\n","minibatch AVG loss: 0.10513865575194359\n","Epoch: 20     train index of 5 minibatch: 3      time used: 0.5330631732940674\n","minibatch AVG loss: 0.4573727633804083\n","\n","Epoch: 20  train \n","Loss: 0.3260  Acc: 86.9565\n","benign precision: 80.0000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 96.9697\n","benign TP: 28.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 1.0\n","malignant precision: 96.9697  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 80.0000\n","malignant TP: 32.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.2426  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 0.7523882389068604\n","minibatch AVG loss: 0.0828912228345871\n","Epoch: 21     train index of 5 minibatch: 2      time used: 0.534670352935791\n","minibatch AVG loss: 0.3327727824449539\n","Epoch: 21     train index of 5 minibatch: 3      time used: 0.5329735279083252\n","minibatch AVG loss: 0.2660911146551371\n","\n","Epoch: 21  train \n","Loss: 0.2552  Acc: 88.4058\n","benign precision: 86.6667  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 92.1053\n","benign TP: 26.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 86.6667\n","malignant TP: 35.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.1964  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 0.7436790466308594\n","minibatch AVG loss: 0.039948811382055284\n","Epoch: 22     train index of 5 minibatch: 2      time used: 0.5333199501037598\n","minibatch AVG loss: 0.05975924655795097\n","Epoch: 22     train index of 5 minibatch: 3      time used: 0.5347089767456055\n","minibatch AVG loss: 0.14313486963510513\n","\n","Epoch: 22  train \n","Loss: 0.0821  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.4359\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.2308  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 0.7430152893066406\n","minibatch AVG loss: 0.3128074467182159\n","Epoch: 23     train index of 5 minibatch: 2      time used: 0.5329501628875732\n","minibatch AVG loss: 0.0723893765360117\n","Epoch: 23     train index of 5 minibatch: 3      time used: 0.5322790145874023\n","minibatch AVG loss: 0.44177031591534616\n","\n","Epoch: 23  train \n","Loss: 0.3018  Acc: 82.6087\n","benign precision: 88.0000  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 81.3953\n","benign TP: 22.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 8.0\n","malignant precision: 81.3953  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 88.0000\n","malignant TP: 35.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.2670  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 0.7468316555023193\n","minibatch AVG loss: 0.05030340477824211\n","Epoch: 24     train index of 5 minibatch: 2      time used: 0.5321986675262451\n","minibatch AVG loss: 0.037133195996284486\n","Epoch: 24     train index of 5 minibatch: 3      time used: 0.53163743019104\n","minibatch AVG loss: 0.1894661858677864\n","\n","Epoch: 24  train \n","Loss: 0.1292  Acc: 91.3043\n","benign precision: 92.8571  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 94.8718\n","benign FPR: 5.1282  NPV: 92.5000\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.5000  recall: 94.8718\n","malignant sensitivity: 94.8718  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 92.8571\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.2802  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 0.7440829277038574\n","minibatch AVG loss: 0.21678645871579647\n","Epoch: 25     train index of 5 minibatch: 2      time used: 0.5324974060058594\n","minibatch AVG loss: 0.3564576208591461\n","Epoch: 25     train index of 5 minibatch: 3      time used: 0.5307374000549316\n","minibatch AVG loss: 0.18164136223495006\n","\n","Epoch: 25  train \n","Loss: 0.2550  Acc: 88.4058\n","benign precision: 82.3529  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 97.0588\n","benign TP: 28.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 1.0\n","malignant precision: 97.0588  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 82.3529\n","malignant TP: 33.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.2663  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 0.7404084205627441\n","minibatch AVG loss: 0.36355602703988554\n","Epoch: 26     train index of 5 minibatch: 2      time used: 0.5323076248168945\n","minibatch AVG loss: 0.19500987976789474\n","Epoch: 26     train index of 5 minibatch: 3      time used: 0.532820463180542\n","minibatch AVG loss: 0.23186140581965448\n","\n","Epoch: 26  train \n","Loss: 0.2362  Acc: 86.9565\n","benign precision: 88.8889  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 87.8049\n","benign TP: 24.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.8049  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 88.8889\n","malignant TP: 36.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.2859  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 0.7401292324066162\n","minibatch AVG loss: 0.07654882892966271\n","Epoch: 27     train index of 5 minibatch: 2      time used: 0.532376766204834\n","minibatch AVG loss: 0.3153944447636604\n","Epoch: 27     train index of 5 minibatch: 3      time used: 0.5330929756164551\n","minibatch AVG loss: 0.29805212616920473\n","\n","Epoch: 27  train \n","Loss: 0.2345  Acc: 89.8551\n","benign precision: 84.8485  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 97.1429\n","benign TP: 28.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 1.0\n","malignant precision: 97.1429  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 84.8485\n","malignant TP: 34.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.2597  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 0.7441296577453613\n","minibatch AVG loss: 0.15136253014206885\n","Epoch: 28     train index of 5 minibatch: 2      time used: 0.5344109535217285\n","minibatch AVG loss: 0.23817138113081454\n","Epoch: 28     train index of 5 minibatch: 3      time used: 0.5310416221618652\n","minibatch AVG loss: 0.0905059739947319\n","\n","Epoch: 28  train \n","Loss: 0.2063  Acc: 89.8551\n","benign precision: 92.8571  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 90.0000\n","benign TP: 26.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 90.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 92.8571\n","malignant TP: 36.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.2345  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 0.7408299446105957\n","minibatch AVG loss: 0.3509204849600792\n","Epoch: 29     train index of 5 minibatch: 2      time used: 0.531688928604126\n","minibatch AVG loss: 0.23076480850577355\n","Epoch: 29     train index of 5 minibatch: 3      time used: 0.5316753387451172\n","minibatch AVG loss: 0.04636804647743702\n","\n","Epoch: 29  train \n","Loss: 0.1870  Acc: 91.3043\n","benign precision: 93.1034  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 92.3077\n","benign TP: 27.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 92.3077  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 93.1034\n","malignant TP: 36.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.2193  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 0.7410433292388916\n","minibatch AVG loss: 0.4293786946684122\n","Epoch: 30     train index of 5 minibatch: 2      time used: 0.5360145568847656\n","minibatch AVG loss: 0.07917903512716293\n","Epoch: 30     train index of 5 minibatch: 3      time used: 0.5322082042694092\n","minibatch AVG loss: 0.16437391247600316\n","\n","Epoch: 30  train \n","Loss: 0.2042  Acc: 88.4058\n","benign precision: 87.0968  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 91.8919\n","benign TP: 27.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 91.8919  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 87.0968\n","malignant TP: 34.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.2519  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 0.7407388687133789\n","minibatch AVG loss: 0.05600064508616924\n","Epoch: 31     train index of 5 minibatch: 2      time used: 0.5325496196746826\n","minibatch AVG loss: 0.4230651505291462\n","Epoch: 31     train index of 5 minibatch: 3      time used: 0.5322427749633789\n","minibatch AVG loss: 0.18921630028635264\n","\n","Epoch: 31  train \n","Loss: 0.2014  Acc: 88.4058\n","benign precision: 89.6552  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 89.7436\n","benign TP: 26.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 89.7436  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 89.6552\n","malignant TP: 35.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.2570  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 0.7407433986663818\n","minibatch AVG loss: 0.03482345044612885\n","Epoch: 32     train index of 5 minibatch: 2      time used: 0.5329928398132324\n","minibatch AVG loss: 0.28662922251969575\n","Epoch: 32     train index of 5 minibatch: 3      time used: 0.5329058170318604\n","minibatch AVG loss: 0.08445463255047798\n","\n","Epoch: 32  train \n","Loss: 0.1210  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.2338  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 0.7433526515960693\n","minibatch AVG loss: 0.2762218866497278\n","Epoch: 33     train index of 5 minibatch: 2      time used: 0.5330057144165039\n","minibatch AVG loss: 0.054224048554897306\n","Epoch: 33     train index of 5 minibatch: 3      time used: 0.5328700542449951\n","minibatch AVG loss: 0.17591461166739464\n","\n","Epoch: 33  train \n","Loss: 0.1500  Acc: 92.7536\n","benign precision: 96.2963  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 92.6829\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 92.6829  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 96.2963\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.2145  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 0.7477102279663086\n","minibatch AVG loss: 0.1581151932477951\n","Epoch: 34     train index of 5 minibatch: 2      time used: 0.5326945781707764\n","minibatch AVG loss: 0.03459792695939541\n","Epoch: 34     train index of 5 minibatch: 3      time used: 0.5333478450775146\n","minibatch AVG loss: 0.18299762345850468\n","\n","Epoch: 34  train \n","Loss: 0.1147  Acc: 92.7536\n","benign precision: 100.0000  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.4762\n","benign TP: 26.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 90.4762  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.2170  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 0.740685224533081\n","minibatch AVG loss: 0.41242001615464685\n","Epoch: 35     train index of 5 minibatch: 2      time used: 0.5324137210845947\n","minibatch AVG loss: 0.22987658828496932\n","Epoch: 35     train index of 5 minibatch: 3      time used: 0.5354244709014893\n","minibatch AVG loss: 0.1210784139111638\n","\n","Epoch: 35  train \n","Loss: 0.2249  Acc: 88.4058\n","benign precision: 82.8571  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 96.9697\n","benign TP: 29.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 1.0\n","malignant precision: 96.9697  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 82.8571\n","malignant TP: 32.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.2315  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 0.7434840202331543\n","minibatch AVG loss: 0.0402033481746912\n","Epoch: 36     train index of 5 minibatch: 2      time used: 0.5333156585693359\n","minibatch AVG loss: 0.09530314523726702\n","Epoch: 36     train index of 5 minibatch: 3      time used: 0.5327565670013428\n","minibatch AVG loss: 0.05928438790142536\n","\n","Epoch: 36  train \n","Loss: 0.1107  Acc: 95.6522\n","benign precision: 96.5517  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 97.4359\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 96.5517\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.2128  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 0.7531700134277344\n","minibatch AVG loss: 0.04987830892205238\n","Epoch: 37     train index of 5 minibatch: 2      time used: 0.5330502986907959\n","minibatch AVG loss: 0.19803204163908958\n","Epoch: 37     train index of 5 minibatch: 3      time used: 0.5324769020080566\n","minibatch AVG loss: 0.23915128074586392\n","\n","Epoch: 37  train \n","Loss: 0.2091  Acc: 88.4058\n","benign precision: 89.2857  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 90.0000\n","benign TP: 25.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 4.0\n","malignant precision: 90.0000  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 89.2857\n","malignant TP: 36.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.2249  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 0.7423341274261475\n","minibatch AVG loss: 0.15446698665618896\n","Epoch: 38     train index of 5 minibatch: 2      time used: 0.5329382419586182\n","minibatch AVG loss: 0.202911926060915\n","Epoch: 38     train index of 5 minibatch: 3      time used: 0.5317294597625732\n","minibatch AVG loss: 0.14567307084798814\n","\n","Epoch: 38  train \n","Loss: 0.1532  Acc: 92.7536\n","benign precision: 93.3333  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 94.7368\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 94.7368  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 93.3333\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.2618  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 0.7433187961578369\n","minibatch AVG loss: 0.2088665034621954\n","Epoch: 39     train index of 5 minibatch: 2      time used: 0.5329005718231201\n","minibatch AVG loss: 0.09865460358560085\n","Epoch: 39     train index of 5 minibatch: 3      time used: 0.5333828926086426\n","minibatch AVG loss: 0.17911642268300057\n","\n","Epoch: 39  train \n","Loss: 0.1503  Acc: 91.3043\n","benign precision: 96.2963  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 90.2439\n","benign TP: 26.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 90.2439  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 96.2963\n","malignant TP: 37.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.2281  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 0.7514955997467041\n","minibatch AVG loss: 0.2559894725680351\n","Epoch: 40     train index of 5 minibatch: 2      time used: 0.5328171253204346\n","minibatch AVG loss: 0.229701878875494\n","Epoch: 40     train index of 5 minibatch: 3      time used: 0.5323410034179688\n","minibatch AVG loss: 0.05208939239382744\n","\n","Epoch: 40  train \n","Loss: 0.1601  Acc: 95.6522\n","benign precision: 93.7500  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.7500\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.2257  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 0.7461421489715576\n","minibatch AVG loss: 0.06872605606913566\n","Epoch: 41     train index of 5 minibatch: 2      time used: 0.5323166847229004\n","minibatch AVG loss: 0.03857320323586464\n","Epoch: 41     train index of 5 minibatch: 3      time used: 0.5338332653045654\n","minibatch AVG loss: 0.11731620095670223\n","\n","Epoch: 41  train \n","Loss: 0.0712  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.2269  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 0.7451221942901611\n","minibatch AVG loss: 0.285674861446023\n","Epoch: 42     train index of 5 minibatch: 2      time used: 0.5326497554779053\n","minibatch AVG loss: 0.19787918142974376\n","Epoch: 42     train index of 5 minibatch: 3      time used: 0.5316641330718994\n","minibatch AVG loss: 0.115858493745327\n","\n","Epoch: 42  train \n","Loss: 0.1950  Acc: 91.3043\n","benign precision: 87.5000  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 97.2222\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 1.0\n","malignant precision: 97.2222  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 87.5000\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.2311  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 0.7380921840667725\n","minibatch AVG loss: 0.09836996234953403\n","Epoch: 43     train index of 5 minibatch: 2      time used: 0.5333540439605713\n","minibatch AVG loss: 0.31389482915401457\n","Epoch: 43     train index of 5 minibatch: 3      time used: 0.5327732563018799\n","minibatch AVG loss: 0.07999241054058075\n","\n","Epoch: 43  train \n","Loss: 0.1540  Acc: 95.6522\n","benign precision: 96.5517  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 97.4359\n","benign TP: 28.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 96.5517\n","malignant TP: 38.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.1598  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 0.7444074153900146\n","minibatch AVG loss: 0.07220147401094437\n","Epoch: 44     train index of 5 minibatch: 2      time used: 0.5331482887268066\n","minibatch AVG loss: 0.05564273297786713\n","Epoch: 44     train index of 5 minibatch: 3      time used: 0.531834602355957\n","minibatch AVG loss: 0.0361944392323494\n","\n","Epoch: 44  train \n","Loss: 0.1166  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.1220\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.1220  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.1993  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 0.7409377098083496\n","minibatch AVG loss: 0.04284389391541481\n","Epoch: 45     train index of 5 minibatch: 2      time used: 0.532893180847168\n","minibatch AVG loss: 0.5912990290671587\n","Epoch: 45     train index of 5 minibatch: 3      time used: 0.531674861907959\n","minibatch AVG loss: 0.35079858973622324\n","\n","Epoch: 45  train \n","Loss: 0.2903  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.2070  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 0.7520556449890137\n","minibatch AVG loss: 0.043441086262464526\n","Epoch: 46     train index of 5 minibatch: 2      time used: 0.5332012176513672\n","minibatch AVG loss: 0.08842800185084343\n","Epoch: 46     train index of 5 minibatch: 3      time used: 0.534130334854126\n","minibatch AVG loss: 0.15963192395865916\n","\n","Epoch: 46  train \n","Loss: 0.0916  Acc: 95.6522\n","benign precision: 96.6667  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 97.3684\n","benign TP: 29.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.3684  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 96.6667\n","malignant TP: 37.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.2185  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 0.7404332160949707\n","minibatch AVG loss: 0.21300516799092292\n","Epoch: 47     train index of 5 minibatch: 2      time used: 0.5329065322875977\n","minibatch AVG loss: 0.36848136931657793\n","Epoch: 47     train index of 5 minibatch: 3      time used: 0.5326781272888184\n","minibatch AVG loss: 0.29195025861263274\n","\n","Epoch: 47  train \n","Loss: 0.2600  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.2276  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 0.7421038150787354\n","minibatch AVG loss: 0.2616929993033409\n","Epoch: 48     train index of 5 minibatch: 2      time used: 0.5319817066192627\n","minibatch AVG loss: 0.1914691373705864\n","Epoch: 48     train index of 5 minibatch: 3      time used: 0.5350570678710938\n","minibatch AVG loss: 0.0830763377249241\n","\n","Epoch: 48  train \n","Loss: 0.1656  Acc: 92.7536\n","benign precision: 90.3226  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 92.3077\n","benign FPR: 7.6923  NPV: 97.2973\n","benign TP: 28.0\n","benign TN: 36.0\n","benign FP: 3.0\n","benign FN: 1.0\n","malignant precision: 97.2973  recall: 92.3077\n","malignant sensitivity: 92.3077  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 90.3226\n","malignant TP: 36.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.2049  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 0.7352840900421143\n","minibatch AVG loss: 0.040435613319277766\n","Epoch: 49     train index of 5 minibatch: 2      time used: 0.5326018333435059\n","minibatch AVG loss: 0.05479500330984592\n","Epoch: 49     train index of 5 minibatch: 3      time used: 0.5320298671722412\n","minibatch AVG loss: 0.06568980645388364\n","\n","Epoch: 49  train \n","Loss: 0.0505  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.1988  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 0.7449953556060791\n","minibatch AVG loss: 0.5136769324541092\n","Epoch: 50     train index of 5 minibatch: 2      time used: 0.5332999229431152\n","minibatch AVG loss: 0.14383230805397035\n","Epoch: 50     train index of 5 minibatch: 3      time used: 0.5335490703582764\n","minibatch AVG loss: 0.40881505347788333\n","\n","Epoch: 50  train \n","Loss: 0.3120  Acc: 85.5072\n","benign precision: 86.2069  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 87.1795\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 87.1795  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 86.2069\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.2174  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 1m 58s\n","Best epoch idx:  49\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 93.750000\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ResNet50_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx ResNet50_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ruKV2IJrSKij"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224']\n","Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_resnet50_384-9fd3c705.pth\" to /root/.cache/torch/hub/checkpoints/jx_vit_base_resnet50_384-9fd3c705.pth\n","test model output： tensor([[-0.5185,  0.0907]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","     StdConv2dSame-1         [-1, 64, 192, 192]           9,408\n","              ReLU-2         [-1, 64, 192, 192]               0\n","      GroupNormAct-3         [-1, 64, 192, 192]             128\n","     MaxPool2dSame-4           [-1, 64, 96, 96]               0\n","     StdConv2dSame-5          [-1, 256, 96, 96]          16,384\n","          Identity-6          [-1, 256, 96, 96]               0\n","      GroupNormAct-7          [-1, 256, 96, 96]             512\n","    DownsampleConv-8          [-1, 256, 96, 96]               0\n","     StdConv2dSame-9           [-1, 64, 96, 96]           4,096\n","             ReLU-10           [-1, 64, 96, 96]               0\n","     GroupNormAct-11           [-1, 64, 96, 96]             128\n","    StdConv2dSame-12           [-1, 64, 96, 96]          36,864\n","             ReLU-13           [-1, 64, 96, 96]               0\n","     GroupNormAct-14           [-1, 64, 96, 96]             128\n","    StdConv2dSame-15          [-1, 256, 96, 96]          16,384\n","         Identity-16          [-1, 256, 96, 96]               0\n","     GroupNormAct-17          [-1, 256, 96, 96]             512\n","         Identity-18          [-1, 256, 96, 96]               0\n","             ReLU-19          [-1, 256, 96, 96]               0\n","       Bottleneck-20          [-1, 256, 96, 96]               0\n","    StdConv2dSame-21           [-1, 64, 96, 96]          16,384\n","             ReLU-22           [-1, 64, 96, 96]               0\n","     GroupNormAct-23           [-1, 64, 96, 96]             128\n","    StdConv2dSame-24           [-1, 64, 96, 96]          36,864\n","             ReLU-25           [-1, 64, 96, 96]               0\n","     GroupNormAct-26           [-1, 64, 96, 96]             128\n","    StdConv2dSame-27          [-1, 256, 96, 96]          16,384\n","         Identity-28          [-1, 256, 96, 96]               0\n","     GroupNormAct-29          [-1, 256, 96, 96]             512\n","         Identity-30          [-1, 256, 96, 96]               0\n","             ReLU-31          [-1, 256, 96, 96]               0\n","       Bottleneck-32          [-1, 256, 96, 96]               0\n","    StdConv2dSame-33           [-1, 64, 96, 96]          16,384\n","             ReLU-34           [-1, 64, 96, 96]               0\n","     GroupNormAct-35           [-1, 64, 96, 96]             128\n","    StdConv2dSame-36           [-1, 64, 96, 96]          36,864\n","             ReLU-37           [-1, 64, 96, 96]               0\n","     GroupNormAct-38           [-1, 64, 96, 96]             128\n","    StdConv2dSame-39          [-1, 256, 96, 96]          16,384\n","         Identity-40          [-1, 256, 96, 96]               0\n","     GroupNormAct-41          [-1, 256, 96, 96]             512\n","         Identity-42          [-1, 256, 96, 96]               0\n","             ReLU-43          [-1, 256, 96, 96]               0\n","       Bottleneck-44          [-1, 256, 96, 96]               0\n","      ResNetStage-45          [-1, 256, 96, 96]               0\n","    StdConv2dSame-46          [-1, 512, 48, 48]         131,072\n","         Identity-47          [-1, 512, 48, 48]               0\n","     GroupNormAct-48          [-1, 512, 48, 48]           1,024\n","   DownsampleConv-49          [-1, 512, 48, 48]               0\n","    StdConv2dSame-50          [-1, 128, 96, 96]          32,768\n","             ReLU-51          [-1, 128, 96, 96]               0\n","     GroupNormAct-52          [-1, 128, 96, 96]             256\n","    StdConv2dSame-53          [-1, 128, 48, 48]         147,456\n","             ReLU-54          [-1, 128, 48, 48]               0\n","     GroupNormAct-55          [-1, 128, 48, 48]             256\n","    StdConv2dSame-56          [-1, 512, 48, 48]          65,536\n","         Identity-57          [-1, 512, 48, 48]               0\n","     GroupNormAct-58          [-1, 512, 48, 48]           1,024\n","         Identity-59          [-1, 512, 48, 48]               0\n","             ReLU-60          [-1, 512, 48, 48]               0\n","       Bottleneck-61          [-1, 512, 48, 48]               0\n","    StdConv2dSame-62          [-1, 128, 48, 48]          65,536\n","             ReLU-63          [-1, 128, 48, 48]               0\n","     GroupNormAct-64          [-1, 128, 48, 48]             256\n","    StdConv2dSame-65          [-1, 128, 48, 48]         147,456\n","             ReLU-66          [-1, 128, 48, 48]               0\n","     GroupNormAct-67          [-1, 128, 48, 48]             256\n","    StdConv2dSame-68          [-1, 512, 48, 48]          65,536\n","         Identity-69          [-1, 512, 48, 48]               0\n","     GroupNormAct-70          [-1, 512, 48, 48]           1,024\n","         Identity-71          [-1, 512, 48, 48]               0\n","             ReLU-72          [-1, 512, 48, 48]               0\n","       Bottleneck-73          [-1, 512, 48, 48]               0\n","    StdConv2dSame-74          [-1, 128, 48, 48]          65,536\n","             ReLU-75          [-1, 128, 48, 48]               0\n","     GroupNormAct-76          [-1, 128, 48, 48]             256\n","    StdConv2dSame-77          [-1, 128, 48, 48]         147,456\n","             ReLU-78          [-1, 128, 48, 48]               0\n","     GroupNormAct-79          [-1, 128, 48, 48]             256\n","    StdConv2dSame-80          [-1, 512, 48, 48]          65,536\n","         Identity-81          [-1, 512, 48, 48]               0\n","     GroupNormAct-82          [-1, 512, 48, 48]           1,024\n","         Identity-83          [-1, 512, 48, 48]               0\n","             ReLU-84          [-1, 512, 48, 48]               0\n","       Bottleneck-85          [-1, 512, 48, 48]               0\n","    StdConv2dSame-86          [-1, 128, 48, 48]          65,536\n","             ReLU-87          [-1, 128, 48, 48]               0\n","     GroupNormAct-88          [-1, 128, 48, 48]             256\n","    StdConv2dSame-89          [-1, 128, 48, 48]         147,456\n","             ReLU-90          [-1, 128, 48, 48]               0\n","     GroupNormAct-91          [-1, 128, 48, 48]             256\n","    StdConv2dSame-92          [-1, 512, 48, 48]          65,536\n","         Identity-93          [-1, 512, 48, 48]               0\n","     GroupNormAct-94          [-1, 512, 48, 48]           1,024\n","         Identity-95          [-1, 512, 48, 48]               0\n","             ReLU-96          [-1, 512, 48, 48]               0\n","       Bottleneck-97          [-1, 512, 48, 48]               0\n","      ResNetStage-98          [-1, 512, 48, 48]               0\n","    StdConv2dSame-99         [-1, 1024, 24, 24]         524,288\n","        Identity-100         [-1, 1024, 24, 24]               0\n","    GroupNormAct-101         [-1, 1024, 24, 24]           2,048\n","  DownsampleConv-102         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-103          [-1, 256, 48, 48]         131,072\n","            ReLU-104          [-1, 256, 48, 48]               0\n","    GroupNormAct-105          [-1, 256, 48, 48]             512\n","   StdConv2dSame-106          [-1, 256, 24, 24]         589,824\n","            ReLU-107          [-1, 256, 24, 24]               0\n","    GroupNormAct-108          [-1, 256, 24, 24]             512\n","   StdConv2dSame-109         [-1, 1024, 24, 24]         262,144\n","        Identity-110         [-1, 1024, 24, 24]               0\n","    GroupNormAct-111         [-1, 1024, 24, 24]           2,048\n","        Identity-112         [-1, 1024, 24, 24]               0\n","            ReLU-113         [-1, 1024, 24, 24]               0\n","      Bottleneck-114         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-115          [-1, 256, 24, 24]         262,144\n","            ReLU-116          [-1, 256, 24, 24]               0\n","    GroupNormAct-117          [-1, 256, 24, 24]             512\n","   StdConv2dSame-118          [-1, 256, 24, 24]         589,824\n","            ReLU-119          [-1, 256, 24, 24]               0\n","    GroupNormAct-120          [-1, 256, 24, 24]             512\n","   StdConv2dSame-121         [-1, 1024, 24, 24]         262,144\n","        Identity-122         [-1, 1024, 24, 24]               0\n","    GroupNormAct-123         [-1, 1024, 24, 24]           2,048\n","        Identity-124         [-1, 1024, 24, 24]               0\n","            ReLU-125         [-1, 1024, 24, 24]               0\n","      Bottleneck-126         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-127          [-1, 256, 24, 24]         262,144\n","            ReLU-128          [-1, 256, 24, 24]               0\n","    GroupNormAct-129          [-1, 256, 24, 24]             512\n","   StdConv2dSame-130          [-1, 256, 24, 24]         589,824\n","            ReLU-131          [-1, 256, 24, 24]               0\n","    GroupNormAct-132          [-1, 256, 24, 24]             512\n","   StdConv2dSame-133         [-1, 1024, 24, 24]         262,144\n","        Identity-134         [-1, 1024, 24, 24]               0\n","    GroupNormAct-135         [-1, 1024, 24, 24]           2,048\n","        Identity-136         [-1, 1024, 24, 24]               0\n","            ReLU-137         [-1, 1024, 24, 24]               0\n","      Bottleneck-138         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-139          [-1, 256, 24, 24]         262,144\n","            ReLU-140          [-1, 256, 24, 24]               0\n","    GroupNormAct-141          [-1, 256, 24, 24]             512\n","   StdConv2dSame-142          [-1, 256, 24, 24]         589,824\n","            ReLU-143          [-1, 256, 24, 24]               0\n","    GroupNormAct-144          [-1, 256, 24, 24]             512\n","   StdConv2dSame-145         [-1, 1024, 24, 24]         262,144\n","        Identity-146         [-1, 1024, 24, 24]               0\n","    GroupNormAct-147         [-1, 1024, 24, 24]           2,048\n","        Identity-148         [-1, 1024, 24, 24]               0\n","            ReLU-149         [-1, 1024, 24, 24]               0\n","      Bottleneck-150         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-151          [-1, 256, 24, 24]         262,144\n","            ReLU-152          [-1, 256, 24, 24]               0\n","    GroupNormAct-153          [-1, 256, 24, 24]             512\n","   StdConv2dSame-154          [-1, 256, 24, 24]         589,824\n","            ReLU-155          [-1, 256, 24, 24]               0\n","    GroupNormAct-156          [-1, 256, 24, 24]             512\n","   StdConv2dSame-157         [-1, 1024, 24, 24]         262,144\n","        Identity-158         [-1, 1024, 24, 24]               0\n","    GroupNormAct-159         [-1, 1024, 24, 24]           2,048\n","        Identity-160         [-1, 1024, 24, 24]               0\n","            ReLU-161         [-1, 1024, 24, 24]               0\n","      Bottleneck-162         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-163          [-1, 256, 24, 24]         262,144\n","            ReLU-164          [-1, 256, 24, 24]               0\n","    GroupNormAct-165          [-1, 256, 24, 24]             512\n","   StdConv2dSame-166          [-1, 256, 24, 24]         589,824\n","            ReLU-167          [-1, 256, 24, 24]               0\n","    GroupNormAct-168          [-1, 256, 24, 24]             512\n","   StdConv2dSame-169         [-1, 1024, 24, 24]         262,144\n","        Identity-170         [-1, 1024, 24, 24]               0\n","    GroupNormAct-171         [-1, 1024, 24, 24]           2,048\n","        Identity-172         [-1, 1024, 24, 24]               0\n","            ReLU-173         [-1, 1024, 24, 24]               0\n","      Bottleneck-174         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-175          [-1, 256, 24, 24]         262,144\n","            ReLU-176          [-1, 256, 24, 24]               0\n","    GroupNormAct-177          [-1, 256, 24, 24]             512\n","   StdConv2dSame-178          [-1, 256, 24, 24]         589,824\n","            ReLU-179          [-1, 256, 24, 24]               0\n","    GroupNormAct-180          [-1, 256, 24, 24]             512\n","   StdConv2dSame-181         [-1, 1024, 24, 24]         262,144\n","        Identity-182         [-1, 1024, 24, 24]               0\n","    GroupNormAct-183         [-1, 1024, 24, 24]           2,048\n","        Identity-184         [-1, 1024, 24, 24]               0\n","            ReLU-185         [-1, 1024, 24, 24]               0\n","      Bottleneck-186         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-187          [-1, 256, 24, 24]         262,144\n","            ReLU-188          [-1, 256, 24, 24]               0\n","    GroupNormAct-189          [-1, 256, 24, 24]             512\n","   StdConv2dSame-190          [-1, 256, 24, 24]         589,824\n","            ReLU-191          [-1, 256, 24, 24]               0\n","    GroupNormAct-192          [-1, 256, 24, 24]             512\n","   StdConv2dSame-193         [-1, 1024, 24, 24]         262,144\n","        Identity-194         [-1, 1024, 24, 24]               0\n","    GroupNormAct-195         [-1, 1024, 24, 24]           2,048\n","        Identity-196         [-1, 1024, 24, 24]               0\n","            ReLU-197         [-1, 1024, 24, 24]               0\n","      Bottleneck-198         [-1, 1024, 24, 24]               0\n","   StdConv2dSame-199          [-1, 256, 24, 24]         262,144\n","            ReLU-200          [-1, 256, 24, 24]               0\n","    GroupNormAct-201          [-1, 256, 24, 24]             512\n","   StdConv2dSame-202          [-1, 256, 24, 24]         589,824\n","            ReLU-203          [-1, 256, 24, 24]               0\n","    GroupNormAct-204          [-1, 256, 24, 24]             512\n","   StdConv2dSame-205         [-1, 1024, 24, 24]         262,144\n","        Identity-206         [-1, 1024, 24, 24]               0\n","    GroupNormAct-207         [-1, 1024, 24, 24]           2,048\n","        Identity-208         [-1, 1024, 24, 24]               0\n","            ReLU-209         [-1, 1024, 24, 24]               0\n","      Bottleneck-210         [-1, 1024, 24, 24]               0\n","     ResNetStage-211         [-1, 1024, 24, 24]               0\n","        Identity-212         [-1, 1024, 24, 24]               0\n","        Identity-213         [-1, 1024, 24, 24]               0\n","        Identity-214         [-1, 1024, 24, 24]               0\n","SelectAdaptivePool2d-215         [-1, 1024, 24, 24]               0\n","        Identity-216         [-1, 1024, 24, 24]               0\n","        Identity-217         [-1, 1024, 24, 24]               0\n","  ClassifierHead-218         [-1, 1024, 24, 24]               0\n","        ResNetV2-219         [-1, 1024, 24, 24]               0\n","          Conv2d-220          [-1, 768, 24, 24]         787,200\n","     HybridEmbed-221             [-1, 576, 768]               0\n","         Dropout-222             [-1, 577, 768]               0\n","       LayerNorm-223             [-1, 577, 768]           1,536\n","          Linear-224            [-1, 577, 2304]       1,771,776\n","         Dropout-225         [-1, 12, 577, 577]               0\n","          Linear-226             [-1, 577, 768]         590,592\n","         Dropout-227             [-1, 577, 768]               0\n","       Attention-228             [-1, 577, 768]               0\n","        Identity-229             [-1, 577, 768]               0\n","       LayerNorm-230             [-1, 577, 768]           1,536\n","          Linear-231            [-1, 577, 3072]       2,362,368\n","            GELU-232            [-1, 577, 3072]               0\n","         Dropout-233            [-1, 577, 3072]               0\n","          Linear-234             [-1, 577, 768]       2,360,064\n","         Dropout-235             [-1, 577, 768]               0\n","             Mlp-236             [-1, 577, 768]               0\n","        Identity-237             [-1, 577, 768]               0\n","           Block-238             [-1, 577, 768]               0\n","       LayerNorm-239             [-1, 577, 768]           1,536\n","          Linear-240            [-1, 577, 2304]       1,771,776\n","         Dropout-241         [-1, 12, 577, 577]               0\n","          Linear-242             [-1, 577, 768]         590,592\n","         Dropout-243             [-1, 577, 768]               0\n","       Attention-244             [-1, 577, 768]               0\n","        Identity-245             [-1, 577, 768]               0\n","       LayerNorm-246             [-1, 577, 768]           1,536\n","          Linear-247            [-1, 577, 3072]       2,362,368\n","            GELU-248            [-1, 577, 3072]               0\n","         Dropout-249            [-1, 577, 3072]               0\n","          Linear-250             [-1, 577, 768]       2,360,064\n","         Dropout-251             [-1, 577, 768]               0\n","             Mlp-252             [-1, 577, 768]               0\n","        Identity-253             [-1, 577, 768]               0\n","           Block-254             [-1, 577, 768]               0\n","       LayerNorm-255             [-1, 577, 768]           1,536\n","          Linear-256            [-1, 577, 2304]       1,771,776\n","         Dropout-257         [-1, 12, 577, 577]               0\n","          Linear-258             [-1, 577, 768]         590,592\n","         Dropout-259             [-1, 577, 768]               0\n","       Attention-260             [-1, 577, 768]               0\n","        Identity-261             [-1, 577, 768]               0\n","       LayerNorm-262             [-1, 577, 768]           1,536\n","          Linear-263            [-1, 577, 3072]       2,362,368\n","            GELU-264            [-1, 577, 3072]               0\n","         Dropout-265            [-1, 577, 3072]               0\n","          Linear-266             [-1, 577, 768]       2,360,064\n","         Dropout-267             [-1, 577, 768]               0\n","             Mlp-268             [-1, 577, 768]               0\n","        Identity-269             [-1, 577, 768]               0\n","           Block-270             [-1, 577, 768]               0\n","       LayerNorm-271             [-1, 577, 768]           1,536\n","          Linear-272            [-1, 577, 2304]       1,771,776\n","         Dropout-273         [-1, 12, 577, 577]               0\n","          Linear-274             [-1, 577, 768]         590,592\n","         Dropout-275             [-1, 577, 768]               0\n","       Attention-276             [-1, 577, 768]               0\n","        Identity-277             [-1, 577, 768]               0\n","       LayerNorm-278             [-1, 577, 768]           1,536\n","          Linear-279            [-1, 577, 3072]       2,362,368\n","            GELU-280            [-1, 577, 3072]               0\n","         Dropout-281            [-1, 577, 3072]               0\n","          Linear-282             [-1, 577, 768]       2,360,064\n","         Dropout-283             [-1, 577, 768]               0\n","             Mlp-284             [-1, 577, 768]               0\n","        Identity-285             [-1, 577, 768]               0\n","           Block-286             [-1, 577, 768]               0\n","       LayerNorm-287             [-1, 577, 768]           1,536\n","          Linear-288            [-1, 577, 2304]       1,771,776\n","         Dropout-289         [-1, 12, 577, 577]               0\n","          Linear-290             [-1, 577, 768]         590,592\n","         Dropout-291             [-1, 577, 768]               0\n","       Attention-292             [-1, 577, 768]               0\n","        Identity-293             [-1, 577, 768]               0\n","       LayerNorm-294             [-1, 577, 768]           1,536\n","          Linear-295            [-1, 577, 3072]       2,362,368\n","            GELU-296            [-1, 577, 3072]               0\n","         Dropout-297            [-1, 577, 3072]               0\n","          Linear-298             [-1, 577, 768]       2,360,064\n","         Dropout-299             [-1, 577, 768]               0\n","             Mlp-300             [-1, 577, 768]               0\n","        Identity-301             [-1, 577, 768]               0\n","           Block-302             [-1, 577, 768]               0\n","       LayerNorm-303             [-1, 577, 768]           1,536\n","          Linear-304            [-1, 577, 2304]       1,771,776\n","         Dropout-305         [-1, 12, 577, 577]               0\n","          Linear-306             [-1, 577, 768]         590,592\n","         Dropout-307             [-1, 577, 768]               0\n","       Attention-308             [-1, 577, 768]               0\n","        Identity-309             [-1, 577, 768]               0\n","       LayerNorm-310             [-1, 577, 768]           1,536\n","          Linear-311            [-1, 577, 3072]       2,362,368\n","            GELU-312            [-1, 577, 3072]               0\n","         Dropout-313            [-1, 577, 3072]               0\n","          Linear-314             [-1, 577, 768]       2,360,064\n","         Dropout-315             [-1, 577, 768]               0\n","             Mlp-316             [-1, 577, 768]               0\n","        Identity-317             [-1, 577, 768]               0\n","           Block-318             [-1, 577, 768]               0\n","       LayerNorm-319             [-1, 577, 768]           1,536\n","          Linear-320            [-1, 577, 2304]       1,771,776\n","         Dropout-321         [-1, 12, 577, 577]               0\n","          Linear-322             [-1, 577, 768]         590,592\n","         Dropout-323             [-1, 577, 768]               0\n","       Attention-324             [-1, 577, 768]               0\n","        Identity-325             [-1, 577, 768]               0\n","       LayerNorm-326             [-1, 577, 768]           1,536\n","          Linear-327            [-1, 577, 3072]       2,362,368\n","            GELU-328            [-1, 577, 3072]               0\n","         Dropout-329            [-1, 577, 3072]               0\n","          Linear-330             [-1, 577, 768]       2,360,064\n","         Dropout-331             [-1, 577, 768]               0\n","             Mlp-332             [-1, 577, 768]               0\n","        Identity-333             [-1, 577, 768]               0\n","           Block-334             [-1, 577, 768]               0\n","       LayerNorm-335             [-1, 577, 768]           1,536\n","          Linear-336            [-1, 577, 2304]       1,771,776\n","         Dropout-337         [-1, 12, 577, 577]               0\n","          Linear-338             [-1, 577, 768]         590,592\n","         Dropout-339             [-1, 577, 768]               0\n","       Attention-340             [-1, 577, 768]               0\n","        Identity-341             [-1, 577, 768]               0\n","       LayerNorm-342             [-1, 577, 768]           1,536\n","          Linear-343            [-1, 577, 3072]       2,362,368\n","            GELU-344            [-1, 577, 3072]               0\n","         Dropout-345            [-1, 577, 3072]               0\n","          Linear-346             [-1, 577, 768]       2,360,064\n","         Dropout-347             [-1, 577, 768]               0\n","             Mlp-348             [-1, 577, 768]               0\n","        Identity-349             [-1, 577, 768]               0\n","           Block-350             [-1, 577, 768]               0\n","       LayerNorm-351             [-1, 577, 768]           1,536\n","          Linear-352            [-1, 577, 2304]       1,771,776\n","         Dropout-353         [-1, 12, 577, 577]               0\n","          Linear-354             [-1, 577, 768]         590,592\n","         Dropout-355             [-1, 577, 768]               0\n","       Attention-356             [-1, 577, 768]               0\n","        Identity-357             [-1, 577, 768]               0\n","       LayerNorm-358             [-1, 577, 768]           1,536\n","          Linear-359            [-1, 577, 3072]       2,362,368\n","            GELU-360            [-1, 577, 3072]               0\n","         Dropout-361            [-1, 577, 3072]               0\n","          Linear-362             [-1, 577, 768]       2,360,064\n","         Dropout-363             [-1, 577, 768]               0\n","             Mlp-364             [-1, 577, 768]               0\n","        Identity-365             [-1, 577, 768]               0\n","           Block-366             [-1, 577, 768]               0\n","       LayerNorm-367             [-1, 577, 768]           1,536\n","          Linear-368            [-1, 577, 2304]       1,771,776\n","         Dropout-369         [-1, 12, 577, 577]               0\n","          Linear-370             [-1, 577, 768]         590,592\n","         Dropout-371             [-1, 577, 768]               0\n","       Attention-372             [-1, 577, 768]               0\n","        Identity-373             [-1, 577, 768]               0\n","       LayerNorm-374             [-1, 577, 768]           1,536\n","          Linear-375            [-1, 577, 3072]       2,362,368\n","            GELU-376            [-1, 577, 3072]               0\n","         Dropout-377            [-1, 577, 3072]               0\n","          Linear-378             [-1, 577, 768]       2,360,064\n","         Dropout-379             [-1, 577, 768]               0\n","             Mlp-380             [-1, 577, 768]               0\n","        Identity-381             [-1, 577, 768]               0\n","           Block-382             [-1, 577, 768]               0\n","       LayerNorm-383             [-1, 577, 768]           1,536\n","          Linear-384            [-1, 577, 2304]       1,771,776\n","         Dropout-385         [-1, 12, 577, 577]               0\n","          Linear-386             [-1, 577, 768]         590,592\n","         Dropout-387             [-1, 577, 768]               0\n","       Attention-388             [-1, 577, 768]               0\n","        Identity-389             [-1, 577, 768]               0\n","       LayerNorm-390             [-1, 577, 768]           1,536\n","          Linear-391            [-1, 577, 3072]       2,362,368\n","            GELU-392            [-1, 577, 3072]               0\n","         Dropout-393            [-1, 577, 3072]               0\n","          Linear-394             [-1, 577, 768]       2,360,064\n","         Dropout-395             [-1, 577, 768]               0\n","             Mlp-396             [-1, 577, 768]               0\n","        Identity-397             [-1, 577, 768]               0\n","           Block-398             [-1, 577, 768]               0\n","       LayerNorm-399             [-1, 577, 768]           1,536\n","          Linear-400            [-1, 577, 2304]       1,771,776\n","         Dropout-401         [-1, 12, 577, 577]               0\n","          Linear-402             [-1, 577, 768]         590,592\n","         Dropout-403             [-1, 577, 768]               0\n","       Attention-404             [-1, 577, 768]               0\n","        Identity-405             [-1, 577, 768]               0\n","       LayerNorm-406             [-1, 577, 768]           1,536\n","          Linear-407            [-1, 577, 3072]       2,362,368\n","            GELU-408            [-1, 577, 3072]               0\n","         Dropout-409            [-1, 577, 3072]               0\n","          Linear-410             [-1, 577, 768]       2,360,064\n","         Dropout-411             [-1, 577, 768]               0\n","             Mlp-412             [-1, 577, 768]               0\n","        Identity-413             [-1, 577, 768]               0\n","           Block-414             [-1, 577, 768]               0\n","       LayerNorm-415             [-1, 577, 768]           1,536\n","        Identity-416                  [-1, 768]               0\n","          Linear-417                    [-1, 2]           1,538\n","================================================================\n","Total params: 97,739,586\n","Trainable params: 97,739,586\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 2695.24\n","Params size (MB): 372.85\n","Estimated Total Size (MB): 3069.77\n","----------------------------------------------------------------\n","model : ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 2.228104829788208\n","minibatch AVG loss: 0.5796203553676605\n","Epoch: 1     train index of 5 minibatch: 2      time used: 1.9859619140625\n","minibatch AVG loss: 0.19583376869559288\n","Epoch: 1     train index of 5 minibatch: 3      time used: 1.9872066974639893\n","minibatch AVG loss: 0.451293633133173\n","\n","Epoch: 1  train \n","Loss: 0.4231  Acc: 79.7101\n","benign precision: 78.5714  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 82.5000\n","benign TP: 22.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 7.0\n","malignant precision: 82.5000  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 78.5714\n","malignant TP: 33.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.1493  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 2.1928133964538574\n","minibatch AVG loss: 0.1567375250160694\n","Epoch: 2     train index of 5 minibatch: 2      time used: 1.9870038032531738\n","minibatch AVG loss: 0.11543864086270332\n","Epoch: 2     train index of 5 minibatch: 3      time used: 1.9852614402770996\n","minibatch AVG loss: 0.06410343870520592\n","\n","Epoch: 2  train \n","Loss: 0.1023  Acc: 95.6522\n","benign precision: 93.7500  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 93.7500\n","malignant TP: 36.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.1283  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 2.2004756927490234\n","minibatch AVG loss: 0.04651115499436855\n","Epoch: 3     train index of 5 minibatch: 2      time used: 1.9861888885498047\n","minibatch AVG loss: 0.036499235033988955\n","Epoch: 3     train index of 5 minibatch: 3      time used: 1.9857947826385498\n","minibatch AVG loss: 0.06647884808480739\n","\n","Epoch: 3  train \n","Loss: 0.0456  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.0670  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 2.200462818145752\n","minibatch AVG loss: 0.030029995180666447\n","Epoch: 4     train index of 5 minibatch: 2      time used: 1.9874451160430908\n","minibatch AVG loss: 0.011186603270471097\n","Epoch: 4     train index of 5 minibatch: 3      time used: 1.9868271350860596\n","minibatch AVG loss: 0.040563192777335645\n","\n","Epoch: 4  train \n","Loss: 0.0254  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0529  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 2.201087713241577\n","minibatch AVG loss: 0.011328772874549031\n","Epoch: 5     train index of 5 minibatch: 2      time used: 1.9897022247314453\n","minibatch AVG loss: 0.009486634377390146\n","Epoch: 5     train index of 5 minibatch: 3      time used: 1.9863545894622803\n","minibatch AVG loss: 0.005726247467100621\n","\n","Epoch: 5  train \n","Loss: 0.0107  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0384  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 2.2235734462738037\n","minibatch AVG loss: 0.008242658246308565\n","Epoch: 6     train index of 5 minibatch: 2      time used: 1.9873802661895752\n","minibatch AVG loss: 0.011260231165215373\n","Epoch: 6     train index of 5 minibatch: 3      time used: 1.9865050315856934\n","minibatch AVG loss: 0.007151461741887033\n","\n","Epoch: 6  train \n","Loss: 0.0083  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.0349  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 2.197880268096924\n","minibatch AVG loss: 0.010108140064403415\n","Epoch: 7     train index of 5 minibatch: 2      time used: 1.9856598377227783\n","minibatch AVG loss: 0.006284956494346261\n","Epoch: 7     train index of 5 minibatch: 3      time used: 1.9861130714416504\n","minibatch AVG loss: 0.0027533262269571423\n","\n","Epoch: 7  train \n","Loss: 0.0066  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0704  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 2.220109701156616\n","minibatch AVG loss: 0.0026926223654299974\n","Epoch: 8     train index of 5 minibatch: 2      time used: 1.9886019229888916\n","minibatch AVG loss: 0.010251647117547691\n","Epoch: 8     train index of 5 minibatch: 3      time used: 1.9894757270812988\n","minibatch AVG loss: 0.0036214353516697884\n","\n","Epoch: 8  train \n","Loss: 0.0050  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.0425  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 2.2082252502441406\n","minibatch AVG loss: 0.004399627074599266\n","Epoch: 9     train index of 5 minibatch: 2      time used: 1.9893081188201904\n","minibatch AVG loss: 0.007011981727555394\n","Epoch: 9     train index of 5 minibatch: 3      time used: 1.9898624420166016\n","minibatch AVG loss: 0.0024384949589148163\n","\n","Epoch: 9  train \n","Loss: 0.0044  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.0412  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 2.22052264213562\n","minibatch AVG loss: 0.0026208235416561366\n","Epoch: 10     train index of 5 minibatch: 2      time used: 1.9886126518249512\n","minibatch AVG loss: 0.006121852737851441\n","Epoch: 10     train index of 5 minibatch: 3      time used: 1.9883503913879395\n","minibatch AVG loss: 0.002507905731908977\n","\n","Epoch: 10  train \n","Loss: 0.0038  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0493  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 2.2058112621307373\n","minibatch AVG loss: 0.007222629780881107\n","Epoch: 11     train index of 5 minibatch: 2      time used: 1.9886586666107178\n","minibatch AVG loss: 0.003978018765337765\n","Epoch: 11     train index of 5 minibatch: 3      time used: 1.9896137714385986\n","minibatch AVG loss: 0.003301557432860136\n","\n","Epoch: 11  train \n","Loss: 0.0047  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.0379  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 2.2099759578704834\n","minibatch AVG loss: 0.0021857558051124213\n","Epoch: 12     train index of 5 minibatch: 2      time used: 1.9907572269439697\n","minibatch AVG loss: 0.004401330137625337\n","Epoch: 12     train index of 5 minibatch: 3      time used: 1.9906110763549805\n","minibatch AVG loss: 0.01915416168048978\n","\n","Epoch: 12  train \n","Loss: 0.0079  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0864  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 2.2230167388916016\n","minibatch AVG loss: 0.01713714599609375\n","Epoch: 13     train index of 5 minibatch: 2      time used: 1.9889123439788818\n","minibatch AVG loss: 0.0025725580635480583\n","Epoch: 13     train index of 5 minibatch: 3      time used: 1.9869935512542725\n","minibatch AVG loss: 0.0030344729195348917\n","\n","Epoch: 13  train \n","Loss: 0.0069  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.1328  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 2.2091763019561768\n","minibatch AVG loss: 0.0017042092280462385\n","Epoch: 14     train index of 5 minibatch: 2      time used: 1.9903340339660645\n","minibatch AVG loss: 0.023827098100446163\n","Epoch: 14     train index of 5 minibatch: 3      time used: 1.9885876178741455\n","minibatch AVG loss: 0.006294207694008946\n","\n","Epoch: 14  train \n","Loss: 0.0124  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0842  Acc: 93.7500\n","benign precision: 87.5000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 87.5000\n","malignant TP: 8.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 2.2344322204589844\n","minibatch AVG loss: 0.008500026911497116\n","Epoch: 15     train index of 5 minibatch: 2      time used: 1.9909734725952148\n","minibatch AVG loss: 0.0014918883331120013\n","Epoch: 15     train index of 5 minibatch: 3      time used: 1.991224765777588\n","minibatch AVG loss: 0.007565159955993295\n","\n","Epoch: 15  train \n","Loss: 0.0051  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.1144  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 2.2250959873199463\n","minibatch AVG loss: 0.003819198999553919\n","Epoch: 16     train index of 5 minibatch: 2      time used: 1.9907221794128418\n","minibatch AVG loss: 0.002892989496467635\n","Epoch: 16     train index of 5 minibatch: 3      time used: 1.9906773567199707\n","minibatch AVG loss: 0.011878153879661114\n","\n","Epoch: 16  train \n","Loss: 0.0056  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.0264  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 2.229797601699829\n","minibatch AVG loss: 0.008999679540283977\n","Epoch: 17     train index of 5 minibatch: 2      time used: 1.9898042678833008\n","minibatch AVG loss: 0.0008261075941845775\n","Epoch: 17     train index of 5 minibatch: 3      time used: 1.9906833171844482\n","minibatch AVG loss: 0.0014797691372223198\n","\n","Epoch: 17  train \n","Loss: 0.0036  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.0394  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 2.2156453132629395\n","minibatch AVG loss: 0.0017442089039832354\n","Epoch: 18     train index of 5 minibatch: 2      time used: 1.9905378818511963\n","minibatch AVG loss: 0.0012328377924859524\n","Epoch: 18     train index of 5 minibatch: 3      time used: 1.9898645877838135\n","minibatch AVG loss: 0.001294032414443791\n","\n","Epoch: 18  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.0408  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 2.2206931114196777\n","minibatch AVG loss: 0.001574379368685186\n","Epoch: 19     train index of 5 minibatch: 2      time used: 1.9883801937103271\n","minibatch AVG loss: 0.0012823322438634932\n","Epoch: 19     train index of 5 minibatch: 3      time used: 1.9914624691009521\n","minibatch AVG loss: 0.001626053947256878\n","\n","Epoch: 19  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0499  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 2.2176198959350586\n","minibatch AVG loss: 0.001362928724847734\n","Epoch: 20     train index of 5 minibatch: 2      time used: 1.991001844406128\n","minibatch AVG loss: 0.002429450716590509\n","Epoch: 20     train index of 5 minibatch: 3      time used: 1.9889414310455322\n","minibatch AVG loss: 0.0016125274065416305\n","\n","Epoch: 20  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0391  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 2.2232985496520996\n","minibatch AVG loss: 0.002101977670099586\n","Epoch: 21     train index of 5 minibatch: 2      time used: 1.9889919757843018\n","minibatch AVG loss: 0.0015732092782855035\n","Epoch: 21     train index of 5 minibatch: 3      time used: 1.991339921951294\n","minibatch AVG loss: 0.001385380025021732\n","\n","Epoch: 21  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0543  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 2.2082626819610596\n","minibatch AVG loss: 0.0015750791179016232\n","Epoch: 22     train index of 5 minibatch: 2      time used: 1.9902257919311523\n","minibatch AVG loss: 0.002178229670971632\n","Epoch: 22     train index of 5 minibatch: 3      time used: 1.9880950450897217\n","minibatch AVG loss: 0.0010607462609186769\n","\n","Epoch: 22  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0536  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 2.2113101482391357\n","minibatch AVG loss: 0.0016811937326565384\n","Epoch: 23     train index of 5 minibatch: 2      time used: 1.9871866703033447\n","minibatch AVG loss: 0.0016032788553275168\n","Epoch: 23     train index of 5 minibatch: 3      time used: 1.9894933700561523\n","minibatch AVG loss: 0.0016048486111685633\n","\n","Epoch: 23  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0490  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 2.220484495162964\n","minibatch AVG loss: 0.0017518086358904838\n","Epoch: 24     train index of 5 minibatch: 2      time used: 1.989495038986206\n","minibatch AVG loss: 0.0022269767709076405\n","Epoch: 24     train index of 5 minibatch: 3      time used: 1.9891791343688965\n","minibatch AVG loss: 0.0013925441075116397\n","\n","Epoch: 24  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0406  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 2.220031976699829\n","minibatch AVG loss: 0.0017433889210224152\n","Epoch: 25     train index of 5 minibatch: 2      time used: 1.9899508953094482\n","minibatch AVG loss: 0.0013504663598723709\n","Epoch: 25     train index of 5 minibatch: 3      time used: 1.9890975952148438\n","minibatch AVG loss: 0.0010764265549369155\n","\n","Epoch: 25  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0423  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 2.217714548110962\n","minibatch AVG loss: 0.0010529831401072443\n","Epoch: 26     train index of 5 minibatch: 2      time used: 1.990469217300415\n","minibatch AVG loss: 0.0017027626628987492\n","Epoch: 26     train index of 5 minibatch: 3      time used: 1.9884915351867676\n","minibatch AVG loss: 0.0014550665335264056\n","\n","Epoch: 26  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0455  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 2.2080800533294678\n","minibatch AVG loss: 0.0030262765707448126\n","Epoch: 27     train index of 5 minibatch: 2      time used: 1.9877655506134033\n","minibatch AVG loss: 0.0011510164476931095\n","Epoch: 27     train index of 5 minibatch: 3      time used: 1.9870657920837402\n","minibatch AVG loss: 0.0014538218965753913\n","\n","Epoch: 27  train \n","Loss: 0.0018  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0439  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 2.222252368927002\n","minibatch AVG loss: 0.001817775797098875\n","Epoch: 28     train index of 5 minibatch: 2      time used: 1.9910829067230225\n","minibatch AVG loss: 0.0018509309971705078\n","Epoch: 28     train index of 5 minibatch: 3      time used: 1.988936185836792\n","minibatch AVG loss: 0.0009926705737598241\n","\n","Epoch: 28  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0447  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 2.2130625247955322\n","minibatch AVG loss: 0.0011915515642613173\n","Epoch: 29     train index of 5 minibatch: 2      time used: 1.989877462387085\n","minibatch AVG loss: 0.0020652868435718117\n","Epoch: 29     train index of 5 minibatch: 3      time used: 1.9890398979187012\n","minibatch AVG loss: 0.0012246976140886546\n","\n","Epoch: 29  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0444  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 2.211951732635498\n","minibatch AVG loss: 0.0014348574448376894\n","Epoch: 30     train index of 5 minibatch: 2      time used: 1.989391803741455\n","minibatch AVG loss: 0.0010434332536533475\n","Epoch: 30     train index of 5 minibatch: 3      time used: 1.9892330169677734\n","minibatch AVG loss: 0.001255417219363153\n","\n","Epoch: 30  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0454  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 2.2124686241149902\n","minibatch AVG loss: 0.0014499269076623023\n","Epoch: 31     train index of 5 minibatch: 2      time used: 1.9887819290161133\n","minibatch AVG loss: 0.0011731677572242916\n","Epoch: 31     train index of 5 minibatch: 3      time used: 1.9879462718963623\n","minibatch AVG loss: 0.0013168492121621966\n","\n","Epoch: 31  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0464  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 2.2088897228240967\n","minibatch AVG loss: 0.0017116005532443524\n","Epoch: 32     train index of 5 minibatch: 2      time used: 1.9886658191680908\n","minibatch AVG loss: 0.0038648347021080554\n","Epoch: 32     train index of 5 minibatch: 3      time used: 1.9891688823699951\n","minibatch AVG loss: 0.0013338918564841152\n","\n","Epoch: 32  train \n","Loss: 0.0023  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0604  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 2.2059898376464844\n","minibatch AVG loss: 0.002437847899273038\n","Epoch: 33     train index of 5 minibatch: 2      time used: 1.9901750087738037\n","minibatch AVG loss: 0.0012843111995607614\n","Epoch: 33     train index of 5 minibatch: 3      time used: 1.9886157512664795\n","minibatch AVG loss: 0.0017778403474949301\n","\n","Epoch: 33  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0500  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 2.210594892501831\n","minibatch AVG loss: 0.0008522851741872728\n","Epoch: 34     train index of 5 minibatch: 2      time used: 1.9901511669158936\n","minibatch AVG loss: 0.0014552945853210987\n","Epoch: 34     train index of 5 minibatch: 3      time used: 1.990734338760376\n","minibatch AVG loss: 0.0012255515903234482\n","\n","Epoch: 34  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0483  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 2.209925889968872\n","minibatch AVG loss: 0.0011087614577263594\n","Epoch: 35     train index of 5 minibatch: 2      time used: 1.9886820316314697\n","minibatch AVG loss: 0.001328364177607\n","Epoch: 35     train index of 5 minibatch: 3      time used: 1.9880456924438477\n","minibatch AVG loss: 0.0011029073037207126\n","\n","Epoch: 35  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0484  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 2.208871841430664\n","minibatch AVG loss: 0.002070514590013772\n","Epoch: 36     train index of 5 minibatch: 2      time used: 1.989046335220337\n","minibatch AVG loss: 0.0011121981428004802\n","Epoch: 36     train index of 5 minibatch: 3      time used: 1.9900081157684326\n","minibatch AVG loss: 0.0009103513672016561\n","\n","Epoch: 36  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0489  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 2.2226955890655518\n","minibatch AVG loss: 0.0015700267627835273\n","Epoch: 37     train index of 5 minibatch: 2      time used: 1.9869489669799805\n","minibatch AVG loss: 0.0011965112993493677\n","Epoch: 37     train index of 5 minibatch: 3      time used: 1.9881267547607422\n","minibatch AVG loss: 0.0008381811552681029\n","\n","Epoch: 37  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0494  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 2.206559658050537\n","minibatch AVG loss: 0.0014807950705289842\n","Epoch: 38     train index of 5 minibatch: 2      time used: 1.9881749153137207\n","minibatch AVG loss: 0.0012723110849037766\n","Epoch: 38     train index of 5 minibatch: 3      time used: 1.9871525764465332\n","minibatch AVG loss: 0.0018361846799962223\n","\n","Epoch: 38  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0508  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 2.2039589881896973\n","minibatch AVG loss: 0.0008410937676671893\n","Epoch: 39     train index of 5 minibatch: 2      time used: 1.98724365234375\n","minibatch AVG loss: 0.0024512693285942077\n","Epoch: 39     train index of 5 minibatch: 3      time used: 1.9855141639709473\n","minibatch AVG loss: 0.0020001479308120905\n","\n","Epoch: 39  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0482  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 2.2133517265319824\n","minibatch AVG loss: 0.0011467222240753472\n","Epoch: 40     train index of 5 minibatch: 2      time used: 1.9877972602844238\n","minibatch AVG loss: 0.0026480840519070626\n","Epoch: 40     train index of 5 minibatch: 3      time used: 1.9872889518737793\n","minibatch AVG loss: 0.0009273854899220169\n","\n","Epoch: 40  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0480  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 2.2064077854156494\n","minibatch AVG loss: 0.0017943560611456632\n","Epoch: 41     train index of 5 minibatch: 2      time used: 1.9902458190917969\n","minibatch AVG loss: 0.0025162489037029445\n","Epoch: 41     train index of 5 minibatch: 3      time used: 1.9881021976470947\n","minibatch AVG loss: 0.000719440501416102\n","\n","Epoch: 41  train \n","Loss: 0.0015  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0503  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 2.2140371799468994\n","minibatch AVG loss: 0.0016677976469509303\n","Epoch: 42     train index of 5 minibatch: 2      time used: 1.9880032539367676\n","minibatch AVG loss: 0.0009307166677899658\n","Epoch: 42     train index of 5 minibatch: 3      time used: 1.9870107173919678\n","minibatch AVG loss: 0.0012451910297386348\n","\n","Epoch: 42  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0528  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 2.201364517211914\n","minibatch AVG loss: 0.0007937531278003007\n","Epoch: 43     train index of 5 minibatch: 2      time used: 1.987513542175293\n","minibatch AVG loss: 0.0011524291709065436\n","Epoch: 43     train index of 5 minibatch: 3      time used: 1.9889602661132812\n","minibatch AVG loss: 0.0017668267362751066\n","\n","Epoch: 43  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0529  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 2.206303834915161\n","minibatch AVG loss: 0.0010449010180309415\n","Epoch: 44     train index of 5 minibatch: 2      time used: 1.9879977703094482\n","minibatch AVG loss: 0.0008224135148338973\n","Epoch: 44     train index of 5 minibatch: 3      time used: 1.9899122714996338\n","minibatch AVG loss: 0.0018573152716271578\n","\n","Epoch: 44  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0525  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 2.2070069313049316\n","minibatch AVG loss: 0.0014998965431004762\n","Epoch: 45     train index of 5 minibatch: 2      time used: 1.9883971214294434\n","minibatch AVG loss: 0.0010581139358691871\n","Epoch: 45     train index of 5 minibatch: 3      time used: 1.9885625839233398\n","minibatch AVG loss: 0.0010242780728731305\n","\n","Epoch: 45  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0533  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 2.2061564922332764\n","minibatch AVG loss: 0.0017473696498200297\n","Epoch: 46     train index of 5 minibatch: 2      time used: 1.9893600940704346\n","minibatch AVG loss: 0.0009504710673354566\n","Epoch: 46     train index of 5 minibatch: 3      time used: 1.9887864589691162\n","minibatch AVG loss: 0.0009217993472702801\n","\n","Epoch: 46  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0538  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 2.209669828414917\n","minibatch AVG loss: 0.0012430098257027566\n","Epoch: 47     train index of 5 minibatch: 2      time used: 1.986797571182251\n","minibatch AVG loss: 0.0013213614118285477\n","Epoch: 47     train index of 5 minibatch: 3      time used: 1.9870376586914062\n","minibatch AVG loss: 0.0009221529879141599\n","\n","Epoch: 47  train \n","Loss: 0.0013  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0536  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 2.2010910511016846\n","minibatch AVG loss: 0.0016088305506855249\n","Epoch: 48     train index of 5 minibatch: 2      time used: 1.9869847297668457\n","minibatch AVG loss: 0.001503538218094036\n","Epoch: 48     train index of 5 minibatch: 3      time used: 1.9851710796356201\n","minibatch AVG loss: 0.0010951300209853798\n","\n","Epoch: 48  train \n","Loss: 0.0014  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0519  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 2.1960062980651855\n","minibatch AVG loss: 0.0011743089184165002\n","Epoch: 49     train index of 5 minibatch: 2      time used: 1.9869105815887451\n","minibatch AVG loss: 0.0009677947149612009\n","Epoch: 49     train index of 5 minibatch: 3      time used: 1.98521089553833\n","minibatch AVG loss: 0.0007413470419123768\n","\n","Epoch: 49  train \n","Loss: 0.0010  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0511  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 2.2012321949005127\n","minibatch AVG loss: 0.0014211504749255255\n","Epoch: 50     train index of 5 minibatch: 2      time used: 1.9873607158660889\n","minibatch AVG loss: 0.0013414121756795793\n","Epoch: 50     train index of 5 minibatch: 3      time used: 1.9875624179840088\n","minibatch AVG loss: 0.0009547549532726407\n","\n","Epoch: 50  train \n","Loss: 0.0012  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0516  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 6m 26s\n","Best epoch idx:  50\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aU1iyR9aSK1X"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['efficientnet_b0',\n"," 'efficientnet_b1',\n"," 'efficientnet_b1_pruned',\n"," 'efficientnet_b2',\n"," 'efficientnet_b2_pruned',\n"," 'efficientnet_b2a',\n"," 'efficientnet_b3',\n"," 'efficientnet_b3_pruned',\n"," 'efficientnet_b3a',\n"," 'efficientnet_b4',\n"," 'efficientnet_b5',\n"," 'efficientnet_b6',\n"," 'efficientnet_b7',\n"," 'efficientnet_b8',\n"," 'efficientnet_cc_b0_4e',\n"," 'efficientnet_cc_b0_8e',\n"," 'efficientnet_cc_b1_8e',\n"," 'efficientnet_el',\n"," 'efficientnet_el_pruned',\n"," 'efficientnet_em',\n"," 'efficientnet_es',\n"," 'efficientnet_es_pruned',\n"," 'efficientnet_l2',\n"," 'efficientnet_lite0',\n"," 'efficientnet_lite1',\n"," 'efficientnet_lite2',\n"," 'efficientnet_lite3',\n"," 'efficientnet_lite4',\n"," 'efficientnetv2_l',\n"," 'efficientnetv2_m',\n"," 'efficientnetv2_rw_m',\n"," 'efficientnetv2_rw_s',\n"," 'efficientnetv2_rw_t',\n"," 'efficientnetv2_s',\n"," 'efficientnetv2_xl',\n"," 'gc_efficientnetv2_rw_t',\n"," 'tf_efficientnet_b0',\n"," 'tf_efficientnet_b0_ap',\n"," 'tf_efficientnet_b0_ns',\n"," 'tf_efficientnet_b1',\n"," 'tf_efficientnet_b1_ap',\n"," 'tf_efficientnet_b1_ns',\n"," 'tf_efficientnet_b2',\n"," 'tf_efficientnet_b2_ap',\n"," 'tf_efficientnet_b2_ns',\n"," 'tf_efficientnet_b3',\n"," 'tf_efficientnet_b3_ap',\n"," 'tf_efficientnet_b3_ns',\n"," 'tf_efficientnet_b4',\n"," 'tf_efficientnet_b4_ap',\n"," 'tf_efficientnet_b4_ns',\n"," 'tf_efficientnet_b5',\n"," 'tf_efficientnet_b5_ap',\n"," 'tf_efficientnet_b5_ns',\n"," 'tf_efficientnet_b6',\n"," 'tf_efficientnet_b6_ap',\n"," 'tf_efficientnet_b6_ns',\n"," 'tf_efficientnet_b7',\n"," 'tf_efficientnet_b7_ap',\n"," 'tf_efficientnet_b7_ns',\n"," 'tf_efficientnet_b8',\n"," 'tf_efficientnet_b8_ap',\n"," 'tf_efficientnet_cc_b0_4e',\n"," 'tf_efficientnet_cc_b0_8e',\n"," 'tf_efficientnet_cc_b1_8e',\n"," 'tf_efficientnet_el',\n"," 'tf_efficientnet_em',\n"," 'tf_efficientnet_es',\n"," 'tf_efficientnet_l2_ns',\n"," 'tf_efficientnet_l2_ns_475',\n"," 'tf_efficientnet_lite0',\n"," 'tf_efficientnet_lite1',\n"," 'tf_efficientnet_lite2',\n"," 'tf_efficientnet_lite3',\n"," 'tf_efficientnet_lite4',\n"," 'tf_efficientnetv2_b0',\n"," 'tf_efficientnetv2_b1',\n"," 'tf_efficientnetv2_b2',\n"," 'tf_efficientnetv2_b3',\n"," 'tf_efficientnetv2_l',\n"," 'tf_efficientnetv2_l_in21ft1k',\n"," 'tf_efficientnetv2_l_in21k',\n"," 'tf_efficientnetv2_m',\n"," 'tf_efficientnetv2_m_in21ft1k',\n"," 'tf_efficientnetv2_m_in21k',\n"," 'tf_efficientnetv2_s',\n"," 'tf_efficientnetv2_s_in21ft1k',\n"," 'tf_efficientnetv2_s_in21k',\n"," 'tf_efficientnetv2_xl_in21ft1k',\n"," 'tf_efficientnetv2_xl_in21k']\n","Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra2-cf984f9c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_ra2-cf984f9c.pth\n","test model output： tensor([[-0.9696, -0.4129]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 40, 192, 192]           1,080\n","       BatchNorm2d-2         [-1, 40, 192, 192]              80\n","              SiLU-3         [-1, 40, 192, 192]               0\n","            Conv2d-4         [-1, 40, 192, 192]             360\n","       BatchNorm2d-5         [-1, 40, 192, 192]              80\n","              SiLU-6         [-1, 40, 192, 192]               0\n","            Conv2d-7             [-1, 10, 1, 1]             410\n","              SiLU-8             [-1, 10, 1, 1]               0\n","            Conv2d-9             [-1, 40, 1, 1]             440\n","          Sigmoid-10             [-1, 40, 1, 1]               0\n","    SqueezeExcite-11         [-1, 40, 192, 192]               0\n","           Conv2d-12         [-1, 24, 192, 192]             960\n","      BatchNorm2d-13         [-1, 24, 192, 192]              48\n","         Identity-14         [-1, 24, 192, 192]               0\n","DepthwiseSeparableConv-15         [-1, 24, 192, 192]               0\n","           Conv2d-16         [-1, 24, 192, 192]             216\n","      BatchNorm2d-17         [-1, 24, 192, 192]              48\n","             SiLU-18         [-1, 24, 192, 192]               0\n","           Conv2d-19              [-1, 6, 1, 1]             150\n","             SiLU-20              [-1, 6, 1, 1]               0\n","           Conv2d-21             [-1, 24, 1, 1]             168\n","          Sigmoid-22             [-1, 24, 1, 1]               0\n","    SqueezeExcite-23         [-1, 24, 192, 192]               0\n","           Conv2d-24         [-1, 24, 192, 192]             576\n","      BatchNorm2d-25         [-1, 24, 192, 192]              48\n","         Identity-26         [-1, 24, 192, 192]               0\n","DepthwiseSeparableConv-27         [-1, 24, 192, 192]               0\n","           Conv2d-28        [-1, 144, 192, 192]           3,456\n","      BatchNorm2d-29        [-1, 144, 192, 192]             288\n","             SiLU-30        [-1, 144, 192, 192]               0\n","           Conv2d-31          [-1, 144, 96, 96]           1,296\n","      BatchNorm2d-32          [-1, 144, 96, 96]             288\n","             SiLU-33          [-1, 144, 96, 96]               0\n","           Conv2d-34              [-1, 6, 1, 1]             870\n","             SiLU-35              [-1, 6, 1, 1]               0\n","           Conv2d-36            [-1, 144, 1, 1]           1,008\n","          Sigmoid-37            [-1, 144, 1, 1]               0\n","    SqueezeExcite-38          [-1, 144, 96, 96]               0\n","           Conv2d-39           [-1, 32, 96, 96]           4,608\n","      BatchNorm2d-40           [-1, 32, 96, 96]              64\n"," InvertedResidual-41           [-1, 32, 96, 96]               0\n","           Conv2d-42          [-1, 192, 96, 96]           6,144\n","      BatchNorm2d-43          [-1, 192, 96, 96]             384\n","             SiLU-44          [-1, 192, 96, 96]               0\n","           Conv2d-45          [-1, 192, 96, 96]           1,728\n","      BatchNorm2d-46          [-1, 192, 96, 96]             384\n","             SiLU-47          [-1, 192, 96, 96]               0\n","           Conv2d-48              [-1, 8, 1, 1]           1,544\n","             SiLU-49              [-1, 8, 1, 1]               0\n","           Conv2d-50            [-1, 192, 1, 1]           1,728\n","          Sigmoid-51            [-1, 192, 1, 1]               0\n","    SqueezeExcite-52          [-1, 192, 96, 96]               0\n","           Conv2d-53           [-1, 32, 96, 96]           6,144\n","      BatchNorm2d-54           [-1, 32, 96, 96]              64\n"," InvertedResidual-55           [-1, 32, 96, 96]               0\n","           Conv2d-56          [-1, 192, 96, 96]           6,144\n","      BatchNorm2d-57          [-1, 192, 96, 96]             384\n","             SiLU-58          [-1, 192, 96, 96]               0\n","           Conv2d-59          [-1, 192, 96, 96]           1,728\n","      BatchNorm2d-60          [-1, 192, 96, 96]             384\n","             SiLU-61          [-1, 192, 96, 96]               0\n","           Conv2d-62              [-1, 8, 1, 1]           1,544\n","             SiLU-63              [-1, 8, 1, 1]               0\n","           Conv2d-64            [-1, 192, 1, 1]           1,728\n","          Sigmoid-65            [-1, 192, 1, 1]               0\n","    SqueezeExcite-66          [-1, 192, 96, 96]               0\n","           Conv2d-67           [-1, 32, 96, 96]           6,144\n","      BatchNorm2d-68           [-1, 32, 96, 96]              64\n"," InvertedResidual-69           [-1, 32, 96, 96]               0\n","           Conv2d-70          [-1, 192, 96, 96]           6,144\n","      BatchNorm2d-71          [-1, 192, 96, 96]             384\n","             SiLU-72          [-1, 192, 96, 96]               0\n","           Conv2d-73          [-1, 192, 48, 48]           4,800\n","      BatchNorm2d-74          [-1, 192, 48, 48]             384\n","             SiLU-75          [-1, 192, 48, 48]               0\n","           Conv2d-76              [-1, 8, 1, 1]           1,544\n","             SiLU-77              [-1, 8, 1, 1]               0\n","           Conv2d-78            [-1, 192, 1, 1]           1,728\n","          Sigmoid-79            [-1, 192, 1, 1]               0\n","    SqueezeExcite-80          [-1, 192, 48, 48]               0\n","           Conv2d-81           [-1, 48, 48, 48]           9,216\n","      BatchNorm2d-82           [-1, 48, 48, 48]              96\n"," InvertedResidual-83           [-1, 48, 48, 48]               0\n","           Conv2d-84          [-1, 288, 48, 48]          13,824\n","      BatchNorm2d-85          [-1, 288, 48, 48]             576\n","             SiLU-86          [-1, 288, 48, 48]               0\n","           Conv2d-87          [-1, 288, 48, 48]           7,200\n","      BatchNorm2d-88          [-1, 288, 48, 48]             576\n","             SiLU-89          [-1, 288, 48, 48]               0\n","           Conv2d-90             [-1, 12, 1, 1]           3,468\n","             SiLU-91             [-1, 12, 1, 1]               0\n","           Conv2d-92            [-1, 288, 1, 1]           3,744\n","          Sigmoid-93            [-1, 288, 1, 1]               0\n","    SqueezeExcite-94          [-1, 288, 48, 48]               0\n","           Conv2d-95           [-1, 48, 48, 48]          13,824\n","      BatchNorm2d-96           [-1, 48, 48, 48]              96\n"," InvertedResidual-97           [-1, 48, 48, 48]               0\n","           Conv2d-98          [-1, 288, 48, 48]          13,824\n","      BatchNorm2d-99          [-1, 288, 48, 48]             576\n","            SiLU-100          [-1, 288, 48, 48]               0\n","          Conv2d-101          [-1, 288, 48, 48]           7,200\n","     BatchNorm2d-102          [-1, 288, 48, 48]             576\n","            SiLU-103          [-1, 288, 48, 48]               0\n","          Conv2d-104             [-1, 12, 1, 1]           3,468\n","            SiLU-105             [-1, 12, 1, 1]               0\n","          Conv2d-106            [-1, 288, 1, 1]           3,744\n","         Sigmoid-107            [-1, 288, 1, 1]               0\n","   SqueezeExcite-108          [-1, 288, 48, 48]               0\n","          Conv2d-109           [-1, 48, 48, 48]          13,824\n","     BatchNorm2d-110           [-1, 48, 48, 48]              96\n","InvertedResidual-111           [-1, 48, 48, 48]               0\n","          Conv2d-112          [-1, 288, 48, 48]          13,824\n","     BatchNorm2d-113          [-1, 288, 48, 48]             576\n","            SiLU-114          [-1, 288, 48, 48]               0\n","          Conv2d-115          [-1, 288, 24, 24]           2,592\n","     BatchNorm2d-116          [-1, 288, 24, 24]             576\n","            SiLU-117          [-1, 288, 24, 24]               0\n","          Conv2d-118             [-1, 12, 1, 1]           3,468\n","            SiLU-119             [-1, 12, 1, 1]               0\n","          Conv2d-120            [-1, 288, 1, 1]           3,744\n","         Sigmoid-121            [-1, 288, 1, 1]               0\n","   SqueezeExcite-122          [-1, 288, 24, 24]               0\n","          Conv2d-123           [-1, 96, 24, 24]          27,648\n","     BatchNorm2d-124           [-1, 96, 24, 24]             192\n","InvertedResidual-125           [-1, 96, 24, 24]               0\n","          Conv2d-126          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-127          [-1, 576, 24, 24]           1,152\n","            SiLU-128          [-1, 576, 24, 24]               0\n","          Conv2d-129          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-130          [-1, 576, 24, 24]           1,152\n","            SiLU-131          [-1, 576, 24, 24]               0\n","          Conv2d-132             [-1, 24, 1, 1]          13,848\n","            SiLU-133             [-1, 24, 1, 1]               0\n","          Conv2d-134            [-1, 576, 1, 1]          14,400\n","         Sigmoid-135            [-1, 576, 1, 1]               0\n","   SqueezeExcite-136          [-1, 576, 24, 24]               0\n","          Conv2d-137           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-138           [-1, 96, 24, 24]             192\n","InvertedResidual-139           [-1, 96, 24, 24]               0\n","          Conv2d-140          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-141          [-1, 576, 24, 24]           1,152\n","            SiLU-142          [-1, 576, 24, 24]               0\n","          Conv2d-143          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-144          [-1, 576, 24, 24]           1,152\n","            SiLU-145          [-1, 576, 24, 24]               0\n","          Conv2d-146             [-1, 24, 1, 1]          13,848\n","            SiLU-147             [-1, 24, 1, 1]               0\n","          Conv2d-148            [-1, 576, 1, 1]          14,400\n","         Sigmoid-149            [-1, 576, 1, 1]               0\n","   SqueezeExcite-150          [-1, 576, 24, 24]               0\n","          Conv2d-151           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-152           [-1, 96, 24, 24]             192\n","InvertedResidual-153           [-1, 96, 24, 24]               0\n","          Conv2d-154          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-155          [-1, 576, 24, 24]           1,152\n","            SiLU-156          [-1, 576, 24, 24]               0\n","          Conv2d-157          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-158          [-1, 576, 24, 24]           1,152\n","            SiLU-159          [-1, 576, 24, 24]               0\n","          Conv2d-160             [-1, 24, 1, 1]          13,848\n","            SiLU-161             [-1, 24, 1, 1]               0\n","          Conv2d-162            [-1, 576, 1, 1]          14,400\n","         Sigmoid-163            [-1, 576, 1, 1]               0\n","   SqueezeExcite-164          [-1, 576, 24, 24]               0\n","          Conv2d-165           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-166           [-1, 96, 24, 24]             192\n","InvertedResidual-167           [-1, 96, 24, 24]               0\n","          Conv2d-168          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-169          [-1, 576, 24, 24]           1,152\n","            SiLU-170          [-1, 576, 24, 24]               0\n","          Conv2d-171          [-1, 576, 24, 24]           5,184\n","     BatchNorm2d-172          [-1, 576, 24, 24]           1,152\n","            SiLU-173          [-1, 576, 24, 24]               0\n","          Conv2d-174             [-1, 24, 1, 1]          13,848\n","            SiLU-175             [-1, 24, 1, 1]               0\n","          Conv2d-176            [-1, 576, 1, 1]          14,400\n","         Sigmoid-177            [-1, 576, 1, 1]               0\n","   SqueezeExcite-178          [-1, 576, 24, 24]               0\n","          Conv2d-179           [-1, 96, 24, 24]          55,296\n","     BatchNorm2d-180           [-1, 96, 24, 24]             192\n","InvertedResidual-181           [-1, 96, 24, 24]               0\n","          Conv2d-182          [-1, 576, 24, 24]          55,296\n","     BatchNorm2d-183          [-1, 576, 24, 24]           1,152\n","            SiLU-184          [-1, 576, 24, 24]               0\n","          Conv2d-185          [-1, 576, 24, 24]          14,400\n","     BatchNorm2d-186          [-1, 576, 24, 24]           1,152\n","            SiLU-187          [-1, 576, 24, 24]               0\n","          Conv2d-188             [-1, 24, 1, 1]          13,848\n","            SiLU-189             [-1, 24, 1, 1]               0\n","          Conv2d-190            [-1, 576, 1, 1]          14,400\n","         Sigmoid-191            [-1, 576, 1, 1]               0\n","   SqueezeExcite-192          [-1, 576, 24, 24]               0\n","          Conv2d-193          [-1, 136, 24, 24]          78,336\n","     BatchNorm2d-194          [-1, 136, 24, 24]             272\n","InvertedResidual-195          [-1, 136, 24, 24]               0\n","          Conv2d-196          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-197          [-1, 816, 24, 24]           1,632\n","            SiLU-198          [-1, 816, 24, 24]               0\n","          Conv2d-199          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-200          [-1, 816, 24, 24]           1,632\n","            SiLU-201          [-1, 816, 24, 24]               0\n","          Conv2d-202             [-1, 34, 1, 1]          27,778\n","            SiLU-203             [-1, 34, 1, 1]               0\n","          Conv2d-204            [-1, 816, 1, 1]          28,560\n","         Sigmoid-205            [-1, 816, 1, 1]               0\n","   SqueezeExcite-206          [-1, 816, 24, 24]               0\n","          Conv2d-207          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-208          [-1, 136, 24, 24]             272\n","InvertedResidual-209          [-1, 136, 24, 24]               0\n","          Conv2d-210          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-211          [-1, 816, 24, 24]           1,632\n","            SiLU-212          [-1, 816, 24, 24]               0\n","          Conv2d-213          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-214          [-1, 816, 24, 24]           1,632\n","            SiLU-215          [-1, 816, 24, 24]               0\n","          Conv2d-216             [-1, 34, 1, 1]          27,778\n","            SiLU-217             [-1, 34, 1, 1]               0\n","          Conv2d-218            [-1, 816, 1, 1]          28,560\n","         Sigmoid-219            [-1, 816, 1, 1]               0\n","   SqueezeExcite-220          [-1, 816, 24, 24]               0\n","          Conv2d-221          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-222          [-1, 136, 24, 24]             272\n","InvertedResidual-223          [-1, 136, 24, 24]               0\n","          Conv2d-224          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-225          [-1, 816, 24, 24]           1,632\n","            SiLU-226          [-1, 816, 24, 24]               0\n","          Conv2d-227          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-228          [-1, 816, 24, 24]           1,632\n","            SiLU-229          [-1, 816, 24, 24]               0\n","          Conv2d-230             [-1, 34, 1, 1]          27,778\n","            SiLU-231             [-1, 34, 1, 1]               0\n","          Conv2d-232            [-1, 816, 1, 1]          28,560\n","         Sigmoid-233            [-1, 816, 1, 1]               0\n","   SqueezeExcite-234          [-1, 816, 24, 24]               0\n","          Conv2d-235          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-236          [-1, 136, 24, 24]             272\n","InvertedResidual-237          [-1, 136, 24, 24]               0\n","          Conv2d-238          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-239          [-1, 816, 24, 24]           1,632\n","            SiLU-240          [-1, 816, 24, 24]               0\n","          Conv2d-241          [-1, 816, 24, 24]          20,400\n","     BatchNorm2d-242          [-1, 816, 24, 24]           1,632\n","            SiLU-243          [-1, 816, 24, 24]               0\n","          Conv2d-244             [-1, 34, 1, 1]          27,778\n","            SiLU-245             [-1, 34, 1, 1]               0\n","          Conv2d-246            [-1, 816, 1, 1]          28,560\n","         Sigmoid-247            [-1, 816, 1, 1]               0\n","   SqueezeExcite-248          [-1, 816, 24, 24]               0\n","          Conv2d-249          [-1, 136, 24, 24]         110,976\n","     BatchNorm2d-250          [-1, 136, 24, 24]             272\n","InvertedResidual-251          [-1, 136, 24, 24]               0\n","          Conv2d-252          [-1, 816, 24, 24]         110,976\n","     BatchNorm2d-253          [-1, 816, 24, 24]           1,632\n","            SiLU-254          [-1, 816, 24, 24]               0\n","          Conv2d-255          [-1, 816, 12, 12]          20,400\n","     BatchNorm2d-256          [-1, 816, 12, 12]           1,632\n","            SiLU-257          [-1, 816, 12, 12]               0\n","          Conv2d-258             [-1, 34, 1, 1]          27,778\n","            SiLU-259             [-1, 34, 1, 1]               0\n","          Conv2d-260            [-1, 816, 1, 1]          28,560\n","         Sigmoid-261            [-1, 816, 1, 1]               0\n","   SqueezeExcite-262          [-1, 816, 12, 12]               0\n","          Conv2d-263          [-1, 232, 12, 12]         189,312\n","     BatchNorm2d-264          [-1, 232, 12, 12]             464\n","InvertedResidual-265          [-1, 232, 12, 12]               0\n","          Conv2d-266         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-267         [-1, 1392, 12, 12]           2,784\n","            SiLU-268         [-1, 1392, 12, 12]               0\n","          Conv2d-269         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-270         [-1, 1392, 12, 12]           2,784\n","            SiLU-271         [-1, 1392, 12, 12]               0\n","          Conv2d-272             [-1, 58, 1, 1]          80,794\n","            SiLU-273             [-1, 58, 1, 1]               0\n","          Conv2d-274           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-275           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-276         [-1, 1392, 12, 12]               0\n","          Conv2d-277          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-278          [-1, 232, 12, 12]             464\n","InvertedResidual-279          [-1, 232, 12, 12]               0\n","          Conv2d-280         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-281         [-1, 1392, 12, 12]           2,784\n","            SiLU-282         [-1, 1392, 12, 12]               0\n","          Conv2d-283         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-284         [-1, 1392, 12, 12]           2,784\n","            SiLU-285         [-1, 1392, 12, 12]               0\n","          Conv2d-286             [-1, 58, 1, 1]          80,794\n","            SiLU-287             [-1, 58, 1, 1]               0\n","          Conv2d-288           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-289           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-290         [-1, 1392, 12, 12]               0\n","          Conv2d-291          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-292          [-1, 232, 12, 12]             464\n","InvertedResidual-293          [-1, 232, 12, 12]               0\n","          Conv2d-294         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-295         [-1, 1392, 12, 12]           2,784\n","            SiLU-296         [-1, 1392, 12, 12]               0\n","          Conv2d-297         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-298         [-1, 1392, 12, 12]           2,784\n","            SiLU-299         [-1, 1392, 12, 12]               0\n","          Conv2d-300             [-1, 58, 1, 1]          80,794\n","            SiLU-301             [-1, 58, 1, 1]               0\n","          Conv2d-302           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-303           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-304         [-1, 1392, 12, 12]               0\n","          Conv2d-305          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-306          [-1, 232, 12, 12]             464\n","InvertedResidual-307          [-1, 232, 12, 12]               0\n","          Conv2d-308         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-309         [-1, 1392, 12, 12]           2,784\n","            SiLU-310         [-1, 1392, 12, 12]               0\n","          Conv2d-311         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-312         [-1, 1392, 12, 12]           2,784\n","            SiLU-313         [-1, 1392, 12, 12]               0\n","          Conv2d-314             [-1, 58, 1, 1]          80,794\n","            SiLU-315             [-1, 58, 1, 1]               0\n","          Conv2d-316           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-317           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-318         [-1, 1392, 12, 12]               0\n","          Conv2d-319          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-320          [-1, 232, 12, 12]             464\n","InvertedResidual-321          [-1, 232, 12, 12]               0\n","          Conv2d-322         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-323         [-1, 1392, 12, 12]           2,784\n","            SiLU-324         [-1, 1392, 12, 12]               0\n","          Conv2d-325         [-1, 1392, 12, 12]          34,800\n","     BatchNorm2d-326         [-1, 1392, 12, 12]           2,784\n","            SiLU-327         [-1, 1392, 12, 12]               0\n","          Conv2d-328             [-1, 58, 1, 1]          80,794\n","            SiLU-329             [-1, 58, 1, 1]               0\n","          Conv2d-330           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-331           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-332         [-1, 1392, 12, 12]               0\n","          Conv2d-333          [-1, 232, 12, 12]         322,944\n","     BatchNorm2d-334          [-1, 232, 12, 12]             464\n","InvertedResidual-335          [-1, 232, 12, 12]               0\n","          Conv2d-336         [-1, 1392, 12, 12]         322,944\n","     BatchNorm2d-337         [-1, 1392, 12, 12]           2,784\n","            SiLU-338         [-1, 1392, 12, 12]               0\n","          Conv2d-339         [-1, 1392, 12, 12]          12,528\n","     BatchNorm2d-340         [-1, 1392, 12, 12]           2,784\n","            SiLU-341         [-1, 1392, 12, 12]               0\n","          Conv2d-342             [-1, 58, 1, 1]          80,794\n","            SiLU-343             [-1, 58, 1, 1]               0\n","          Conv2d-344           [-1, 1392, 1, 1]          82,128\n","         Sigmoid-345           [-1, 1392, 1, 1]               0\n","   SqueezeExcite-346         [-1, 1392, 12, 12]               0\n","          Conv2d-347          [-1, 384, 12, 12]         534,528\n","     BatchNorm2d-348          [-1, 384, 12, 12]             768\n","InvertedResidual-349          [-1, 384, 12, 12]               0\n","          Conv2d-350         [-1, 2304, 12, 12]         884,736\n","     BatchNorm2d-351         [-1, 2304, 12, 12]           4,608\n","            SiLU-352         [-1, 2304, 12, 12]               0\n","          Conv2d-353         [-1, 2304, 12, 12]          20,736\n","     BatchNorm2d-354         [-1, 2304, 12, 12]           4,608\n","            SiLU-355         [-1, 2304, 12, 12]               0\n","          Conv2d-356             [-1, 96, 1, 1]         221,280\n","            SiLU-357             [-1, 96, 1, 1]               0\n","          Conv2d-358           [-1, 2304, 1, 1]         223,488\n","         Sigmoid-359           [-1, 2304, 1, 1]               0\n","   SqueezeExcite-360         [-1, 2304, 12, 12]               0\n","          Conv2d-361          [-1, 384, 12, 12]         884,736\n","     BatchNorm2d-362          [-1, 384, 12, 12]             768\n","InvertedResidual-363          [-1, 384, 12, 12]               0\n","          Conv2d-364         [-1, 1536, 12, 12]         589,824\n","     BatchNorm2d-365         [-1, 1536, 12, 12]           3,072\n","            SiLU-366         [-1, 1536, 12, 12]               0\n","AdaptiveAvgPool2d-367           [-1, 1536, 1, 1]               0\n","         Flatten-368                 [-1, 1536]               0\n","SelectAdaptivePool2d-369                 [-1, 1536]               0\n","          Linear-370                    [-1, 2]           3,074\n","================================================================\n","Total params: 10,699,306\n","Trainable params: 10,699,306\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 996.83\n","Params size (MB): 40.81\n","Estimated Total Size (MB): 1039.33\n","----------------------------------------------------------------\n","model : efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 0.8684439659118652\n","minibatch AVG loss: 1.9570230007171632\n","Epoch: 1     train index of 5 minibatch: 2      time used: 0.6183016300201416\n","minibatch AVG loss: 1.877180576324463\n","Epoch: 1     train index of 5 minibatch: 3      time used: 0.6168503761291504\n","minibatch AVG loss: 1.7647091627120972\n","\n","Epoch: 1  train \n","Loss: 1.7111  Acc: 46.3768\n","benign precision: 41.1765  recall: 46.6667\n","benign sensitivity: 46.6667  specificity: 47.3684\n","benign FPR: 52.6316  NPV: 52.9412\n","benign TP: 14.0\n","benign TN: 18.0\n","benign FP: 20.0\n","benign FN: 16.0\n","malignant precision: 52.9412  recall: 47.3684\n","malignant sensitivity: 47.3684  specificity: 46.6667\n","malignant FPR: 53.3333  NPV: 41.1765\n","malignant TP: 18.0\n","malignant TN: 14.0\n","malignant FP: 16.0\n","malignant FN: 20.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 1.7805  Acc: 43.7500\n","benign precision: 40.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 33.3333\n","benign FPR: 66.6667  NPV: 50.0000\n","benign TP: 4.0\n","benign TN: 3.0\n","benign FP: 6.0\n","benign FN: 3.0\n","malignant precision: 50.0000  recall: 33.3333\n","malignant sensitivity: 33.3333  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 40.0000\n","malignant TP: 3.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 0.836634635925293\n","minibatch AVG loss: 1.336791330575943\n","Epoch: 2     train index of 5 minibatch: 2      time used: 0.6235744953155518\n","minibatch AVG loss: 2.1897605657577515\n","Epoch: 2     train index of 5 minibatch: 3      time used: 0.6167378425598145\n","minibatch AVG loss: 1.6775642022490502\n","\n","Epoch: 2  train \n","Loss: 1.6553  Acc: 50.7246\n","benign precision: 45.4545  recall: 50.0000\n","benign sensitivity: 50.0000  specificity: 52.6316\n","benign FPR: 47.3684  NPV: 57.1429\n","benign TP: 15.0\n","benign TN: 20.0\n","benign FP: 18.0\n","benign FN: 15.0\n","malignant precision: 57.1429  recall: 52.6316\n","malignant sensitivity: 52.6316  specificity: 50.0000\n","malignant FPR: 50.0000  NPV: 45.4545\n","malignant TP: 20.0\n","malignant TN: 15.0\n","malignant FP: 15.0\n","malignant FN: 18.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 191.1035  Acc: 31.2500\n","benign precision: 16.6667  recall: 14.2857\n","benign sensitivity: 14.2857  specificity: 44.4444\n","benign FPR: 55.5556  NPV: 40.0000\n","benign TP: 1.0\n","benign TN: 4.0\n","benign FP: 5.0\n","benign FN: 6.0\n","malignant precision: 40.0000  recall: 44.4444\n","malignant sensitivity: 44.4444  specificity: 14.2857\n","malignant FPR: 85.7143  NPV: 16.6667\n","malignant TP: 4.0\n","malignant TN: 1.0\n","malignant FP: 6.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 0.8433048725128174\n","minibatch AVG loss: 0.8780019402503967\n","Epoch: 3     train index of 5 minibatch: 2      time used: 0.6185932159423828\n","minibatch AVG loss: 1.305192121863365\n","Epoch: 3     train index of 5 minibatch: 3      time used: 0.6168038845062256\n","minibatch AVG loss: 1.0262336552143096\n","\n","Epoch: 3  train \n","Loss: 1.0131  Acc: 68.1159\n","benign precision: 62.1622  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 63.1579\n","benign FPR: 36.8421  NPV: 77.4194\n","benign TP: 23.0\n","benign TN: 24.0\n","benign FP: 14.0\n","benign FN: 7.0\n","malignant precision: 77.4194  recall: 63.1579\n","malignant sensitivity: 63.1579  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 62.1622\n","malignant TP: 24.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 14.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 1282.2036  Acc: 62.5000\n","benign precision: 57.1429  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 66.6667\n","benign TP: 4.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 66.6667  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 57.1429\n","malignant TP: 6.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 0.8361146450042725\n","minibatch AVG loss: 0.3773675411939621\n","Epoch: 4     train index of 5 minibatch: 2      time used: 0.6164891719818115\n","minibatch AVG loss: 0.8568596050143242\n","Epoch: 4     train index of 5 minibatch: 3      time used: 0.6159987449645996\n","minibatch AVG loss: 1.2265659214928746\n","\n","Epoch: 4  train \n","Loss: 0.7673  Acc: 68.1159\n","benign precision: 63.3333  recall: 65.5172\n","benign sensitivity: 65.5172  specificity: 71.7949\n","benign FPR: 28.2051  NPV: 73.6842\n","benign TP: 19.0\n","benign TN: 28.0\n","benign FP: 11.0\n","benign FN: 10.0\n","malignant precision: 73.6842  recall: 71.7949\n","malignant sensitivity: 71.7949  specificity: 65.5172\n","malignant FPR: 34.4828  NPV: 63.3333\n","malignant TP: 28.0\n","malignant TN: 19.0\n","malignant FP: 10.0\n","malignant FN: 11.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.7911  Acc: 68.7500\n","benign precision: 62.5000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 75.0000\n","benign TP: 5.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 75.0000  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 62.5000\n","malignant TP: 6.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 0.8511972427368164\n","minibatch AVG loss: 0.31909665167331697\n","Epoch: 5     train index of 5 minibatch: 2      time used: 0.6175901889801025\n","minibatch AVG loss: 0.9570833340287208\n","Epoch: 5     train index of 5 minibatch: 3      time used: 0.6144320964813232\n","minibatch AVG loss: 1.0079717099666596\n","\n","Epoch: 5  train \n","Loss: 0.6811  Acc: 73.9130\n","benign precision: 71.4286  recall: 68.9655\n","benign sensitivity: 68.9655  specificity: 79.4872\n","benign FPR: 20.5128  NPV: 77.5000\n","benign TP: 20.0\n","benign TN: 31.0\n","benign FP: 8.0\n","benign FN: 9.0\n","malignant precision: 77.5000  recall: 79.4872\n","malignant sensitivity: 79.4872  specificity: 68.9655\n","malignant FPR: 31.0345  NPV: 71.4286\n","malignant TP: 31.0\n","malignant TN: 20.0\n","malignant FP: 9.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 1658.9104  Acc: 56.2500\n","benign precision: 50.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 55.5556\n","benign FPR: 44.4444  NPV: 62.5000\n","benign TP: 4.0\n","benign TN: 5.0\n","benign FP: 4.0\n","benign FN: 3.0\n","malignant precision: 62.5000  recall: 55.5556\n","malignant sensitivity: 55.5556  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 50.0000\n","malignant TP: 5.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 0.8314180374145508\n","minibatch AVG loss: 1.1383488953113556\n","Epoch: 6     train index of 5 minibatch: 2      time used: 0.6153795719146729\n","minibatch AVG loss: 0.7062031090259552\n","Epoch: 6     train index of 5 minibatch: 3      time used: 0.615851640701294\n","minibatch AVG loss: 0.8373304069042206\n","\n","Epoch: 6  train \n","Loss: 0.8352  Acc: 68.1159\n","benign precision: 62.5000  recall: 68.9655\n","benign sensitivity: 68.9655  specificity: 69.2308\n","benign FPR: 30.7692  NPV: 75.0000\n","benign TP: 20.0\n","benign TN: 27.0\n","benign FP: 12.0\n","benign FN: 9.0\n","malignant precision: 75.0000  recall: 69.2308\n","malignant sensitivity: 69.2308  specificity: 68.9655\n","malignant FPR: 31.0345  NPV: 62.5000\n","malignant TP: 27.0\n","malignant TN: 20.0\n","malignant FP: 9.0\n","malignant FN: 12.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.5464  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 0.8395590782165527\n","minibatch AVG loss: 0.5561711102724075\n","Epoch: 7     train index of 5 minibatch: 2      time used: 0.6192588806152344\n","minibatch AVG loss: 0.5366761386394501\n","Epoch: 7     train index of 5 minibatch: 3      time used: 0.6126728057861328\n","minibatch AVG loss: 0.5640555202960968\n","\n","Epoch: 7  train \n","Loss: 0.5278  Acc: 76.8116\n","benign precision: 75.0000  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 80.0000\n","benign TP: 21.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 8.0\n","malignant precision: 80.0000  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 75.0000\n","malignant TP: 32.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 85.2468  Acc: 62.5000\n","benign precision: 60.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 63.6364\n","benign TP: 3.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 63.6364  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 60.0000\n","malignant TP: 7.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 0.8273084163665771\n","minibatch AVG loss: 0.39451600760221484\n","Epoch: 8     train index of 5 minibatch: 2      time used: 0.6118013858795166\n","minibatch AVG loss: 0.8643528997898102\n","Epoch: 8     train index of 5 minibatch: 3      time used: 0.6147842407226562\n","minibatch AVG loss: 0.8273769438266754\n","\n","Epoch: 8  train \n","Loss: 0.6674  Acc: 71.0145\n","benign precision: 68.9655  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 74.3590\n","benign TP: 20.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 10.0\n","malignant precision: 74.3590  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 68.9655\n","malignant TP: 29.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 1088.0334  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 0.8333189487457275\n","minibatch AVG loss: 1.0639166690409183\n","Epoch: 9     train index of 5 minibatch: 2      time used: 0.6140134334564209\n","minibatch AVG loss: 0.33448309190571307\n","Epoch: 9     train index of 5 minibatch: 3      time used: 0.6126909255981445\n","minibatch AVG loss: 0.6179348707199097\n","\n","Epoch: 9  train \n","Loss: 0.6028  Acc: 72.4638\n","benign precision: 70.0000  recall: 70.0000\n","benign sensitivity: 70.0000  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 76.3158\n","benign TP: 21.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 9.0\n","malignant precision: 76.3158  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 70.0000\n","malignant FPR: 30.0000  NPV: 70.0000\n","malignant TP: 29.0\n","malignant TN: 21.0\n","malignant FP: 9.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 289.7846  Acc: 62.5000\n","benign precision: 57.1429  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 66.6667\n","benign FPR: 33.3333  NPV: 66.6667\n","benign TP: 4.0\n","benign TN: 6.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 66.6667  recall: 66.6667\n","malignant sensitivity: 66.6667  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 57.1429\n","malignant TP: 6.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 0.8368852138519287\n","minibatch AVG loss: 0.5530329130589962\n","Epoch: 10     train index of 5 minibatch: 2      time used: 0.6146423816680908\n","minibatch AVG loss: 0.7334300637245178\n","Epoch: 10     train index of 5 minibatch: 3      time used: 0.614544153213501\n","minibatch AVG loss: 0.7320360124111176\n","\n","Epoch: 10  train \n","Loss: 0.6790  Acc: 71.0145\n","benign precision: 70.3704  recall: 63.3333\n","benign sensitivity: 63.3333  specificity: 78.9474\n","benign FPR: 21.0526  NPV: 73.1707\n","benign TP: 19.0\n","benign TN: 30.0\n","benign FP: 8.0\n","benign FN: 11.0\n","malignant precision: 73.1707  recall: 78.9474\n","malignant sensitivity: 78.9474  specificity: 63.3333\n","malignant FPR: 36.6667  NPV: 70.3704\n","malignant TP: 30.0\n","malignant TN: 19.0\n","malignant FP: 11.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.6843  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 0.8339447975158691\n","minibatch AVG loss: 0.8879032611846924\n","Epoch: 11     train index of 5 minibatch: 2      time used: 0.6165363788604736\n","minibatch AVG loss: 0.642544561624527\n","Epoch: 11     train index of 5 minibatch: 3      time used: 0.613023042678833\n","minibatch AVG loss: 1.2217288807034492\n","\n","Epoch: 11  train \n","Loss: 0.8088  Acc: 66.6667\n","benign precision: 64.2857  recall: 60.0000\n","benign sensitivity: 60.0000  specificity: 73.6842\n","benign FPR: 26.3158  NPV: 70.0000\n","benign TP: 18.0\n","benign TN: 28.0\n","benign FP: 10.0\n","benign FN: 12.0\n","malignant precision: 70.0000  recall: 73.6842\n","malignant sensitivity: 73.6842  specificity: 60.0000\n","malignant FPR: 40.0000  NPV: 64.2857\n","malignant TP: 28.0\n","malignant TN: 18.0\n","malignant FP: 12.0\n","malignant FN: 10.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 30.9123  Acc: 62.5000\n","benign precision: 60.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 63.6364\n","benign TP: 3.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 4.0\n","malignant precision: 63.6364  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 60.0000\n","malignant TP: 7.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 0.8485422134399414\n","minibatch AVG loss: 0.5982629209756851\n","Epoch: 12     train index of 5 minibatch: 2      time used: 0.6176795959472656\n","minibatch AVG loss: 0.8558461397886277\n","Epoch: 12     train index of 5 minibatch: 3      time used: 0.6142382621765137\n","minibatch AVG loss: 0.7622829355299473\n","\n","Epoch: 12  train \n","Loss: 0.6628  Acc: 75.3623\n","benign precision: 70.9677  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 76.9231\n","benign FPR: 23.0769  NPV: 81.0811\n","benign TP: 22.0\n","benign TN: 30.0\n","benign FP: 9.0\n","benign FN: 7.0\n","malignant precision: 81.0811  recall: 76.9231\n","malignant sensitivity: 76.9231  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 70.9677\n","malignant TP: 30.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 64.6786  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 0.8341410160064697\n","minibatch AVG loss: 0.4289121448993683\n","Epoch: 13     train index of 5 minibatch: 2      time used: 0.612480878829956\n","minibatch AVG loss: 0.8662071794271469\n","Epoch: 13     train index of 5 minibatch: 3      time used: 0.6172130107879639\n","minibatch AVG loss: 0.35658566039055584\n","\n","Epoch: 13  train \n","Loss: 0.5396  Acc: 78.2609\n","benign precision: 70.2703  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 71.7949\n","benign FPR: 28.2051  NPV: 90.3226\n","benign TP: 26.0\n","benign TN: 28.0\n","benign FP: 11.0\n","benign FN: 3.0\n","malignant precision: 90.3226  recall: 71.7949\n","malignant sensitivity: 71.7949  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 70.2703\n","malignant TP: 28.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 11.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 74.8635  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 0.8388566970825195\n","minibatch AVG loss: 0.7951020527631044\n","Epoch: 14     train index of 5 minibatch: 2      time used: 0.6151738166809082\n","minibatch AVG loss: 0.6948146939277648\n","Epoch: 14     train index of 5 minibatch: 3      time used: 0.6179983615875244\n","minibatch AVG loss: 0.19632033626548945\n","\n","Epoch: 14  train \n","Loss: 0.5021  Acc: 79.7101\n","benign precision: 81.4815  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 80.4878\n","benign TP: 22.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 8.0\n","malignant precision: 80.4878  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 81.4815\n","malignant TP: 33.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 9.0682  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 0.8399596214294434\n","minibatch AVG loss: 0.7379183024168015\n","Epoch: 15     train index of 5 minibatch: 2      time used: 0.6176836490631104\n","minibatch AVG loss: 0.5113214671611785\n","Epoch: 15     train index of 5 minibatch: 3      time used: 0.6141934394836426\n","minibatch AVG loss: 0.532202860713005\n","\n","Epoch: 15  train \n","Loss: 0.5355  Acc: 79.7101\n","benign precision: 80.7692  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 80.9524\n","benign TP: 21.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 8.0\n","malignant precision: 80.9524  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 80.7692\n","malignant TP: 34.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 12.0162  Acc: 75.0000\n","benign precision: 71.4286  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 77.7778\n","benign TP: 5.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 77.7778  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 71.4286\n","malignant TP: 7.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 0.8355629444122314\n","minibatch AVG loss: 0.7728868395090103\n","Epoch: 16     train index of 5 minibatch: 2      time used: 0.6162204742431641\n","minibatch AVG loss: 0.7310224056243897\n","Epoch: 16     train index of 5 minibatch: 3      time used: 0.6141834259033203\n","minibatch AVG loss: 0.5550510615110398\n","\n","Epoch: 16  train \n","Loss: 0.6106  Acc: 75.3623\n","benign precision: 71.8750  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 76.3158\n","benign FPR: 23.6842  NPV: 80.5556\n","benign TP: 23.0\n","benign TN: 29.0\n","benign FP: 9.0\n","benign FN: 7.0\n","malignant precision: 80.5556  recall: 76.3158\n","malignant sensitivity: 76.3158  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 71.8750\n","malignant TP: 29.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.8725  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 0.8327529430389404\n","minibatch AVG loss: 0.7181567788124085\n","Epoch: 17     train index of 5 minibatch: 2      time used: 0.614764928817749\n","minibatch AVG loss: 0.8190402060747146\n","Epoch: 17     train index of 5 minibatch: 3      time used: 0.6139950752258301\n","minibatch AVG loss: 0.42692397236824037\n","\n","Epoch: 17  train \n","Loss: 0.6588  Acc: 72.4638\n","benign precision: 67.7419  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 74.3590\n","benign FPR: 25.6410  NPV: 78.3784\n","benign TP: 21.0\n","benign TN: 29.0\n","benign FP: 10.0\n","benign FN: 8.0\n","malignant precision: 78.3784  recall: 74.3590\n","malignant sensitivity: 74.3590  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 67.7419\n","malignant TP: 29.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 10.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 2586.2637  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 0.8407540321350098\n","minibatch AVG loss: 0.310434190556407\n","Epoch: 18     train index of 5 minibatch: 2      time used: 0.6207983493804932\n","minibatch AVG loss: 0.2748772704973817\n","Epoch: 18     train index of 5 minibatch: 3      time used: 0.62005615234375\n","minibatch AVG loss: 0.6581941105425357\n","\n","Epoch: 18  train \n","Loss: 0.4167  Acc: 81.1594\n","benign precision: 80.0000  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 84.2105\n","benign TP: 24.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 6.0\n","malignant precision: 84.2105  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 80.0000\n","malignant TP: 32.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 3.0152  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 0.8376643657684326\n","minibatch AVG loss: 0.21235055960714816\n","Epoch: 19     train index of 5 minibatch: 2      time used: 0.6177780628204346\n","minibatch AVG loss: 0.7148317477665842\n","Epoch: 19     train index of 5 minibatch: 3      time used: 0.6230132579803467\n","minibatch AVG loss: 0.49348380863666536\n","\n","Epoch: 19  train \n","Loss: 0.4346  Acc: 81.1594\n","benign precision: 77.4194  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 86.4865\n","benign TP: 24.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 5.0\n","malignant precision: 86.4865  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 77.4194\n","malignant TP: 32.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 31.1188  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 0.8409183025360107\n","minibatch AVG loss: 0.7574904527515173\n","Epoch: 20     train index of 5 minibatch: 2      time used: 0.6226801872253418\n","minibatch AVG loss: 0.45135736614465716\n","Epoch: 20     train index of 5 minibatch: 3      time used: 0.6144464015960693\n","minibatch AVG loss: 0.7361269772052765\n","\n","Epoch: 20  train \n","Loss: 0.6103  Acc: 73.9130\n","benign precision: 74.0741  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 81.5789\n","benign FPR: 18.4211  NPV: 75.6098\n","benign TP: 20.0\n","benign TN: 31.0\n","benign FP: 7.0\n","benign FN: 10.0\n","malignant precision: 75.6098  recall: 81.5789\n","malignant sensitivity: 81.5789  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 74.0741\n","malignant TP: 31.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 964.9232  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 0.8404998779296875\n","minibatch AVG loss: 0.9052974801510573\n","Epoch: 21     train index of 5 minibatch: 2      time used: 0.6172428131103516\n","minibatch AVG loss: 0.365259051695466\n","Epoch: 21     train index of 5 minibatch: 3      time used: 0.6210176944732666\n","minibatch AVG loss: 0.4886875247582793\n","\n","Epoch: 21  train \n","Loss: 0.5579  Acc: 81.1594\n","benign precision: 77.4194  recall: 82.7586\n","benign sensitivity: 82.7586  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 86.4865\n","benign TP: 24.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 5.0\n","malignant precision: 86.4865  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 82.7586\n","malignant FPR: 17.2414  NPV: 77.4194\n","malignant TP: 32.0\n","malignant TN: 24.0\n","malignant FP: 5.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 563.0458  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 0.8409686088562012\n","minibatch AVG loss: 0.2774100827984512\n","Epoch: 22     train index of 5 minibatch: 2      time used: 0.6186940670013428\n","minibatch AVG loss: 0.7953063607215881\n","Epoch: 22     train index of 5 minibatch: 3      time used: 0.6146335601806641\n","minibatch AVG loss: 0.18279016409069299\n","\n","Epoch: 22  train \n","Loss: 0.3962  Acc: 84.0580\n","benign precision: 85.7143  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 85.0000\n","benign TP: 24.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 6.0\n","malignant precision: 85.0000  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 85.7143\n","malignant TP: 34.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.5893  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 0.8326468467712402\n","minibatch AVG loss: 0.3943325564265251\n","Epoch: 23     train index of 5 minibatch: 2      time used: 0.6162376403808594\n","minibatch AVG loss: 0.22843828983604908\n","Epoch: 23     train index of 5 minibatch: 3      time used: 0.6141142845153809\n","minibatch AVG loss: 0.5471898898482322\n","\n","Epoch: 23  train \n","Loss: 0.3603  Acc: 82.6087\n","benign precision: 85.1852  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 82.9268\n","benign TP: 23.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 7.0\n","malignant precision: 82.9268  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 85.1852\n","malignant TP: 34.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 103.0606  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 0.8348774909973145\n","minibatch AVG loss: 0.7545232839882374\n","Epoch: 24     train index of 5 minibatch: 2      time used: 0.6152524948120117\n","minibatch AVG loss: 0.294289737381041\n","Epoch: 24     train index of 5 minibatch: 3      time used: 0.6134350299835205\n","minibatch AVG loss: 0.9459060966968537\n","\n","Epoch: 24  train \n","Loss: 0.5965  Acc: 76.8116\n","benign precision: 77.7778  recall: 70.0000\n","benign sensitivity: 70.0000  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 78.0488\n","benign TP: 21.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 9.0\n","malignant precision: 78.0488  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 70.0000\n","malignant FPR: 30.0000  NPV: 77.7778\n","malignant TP: 32.0\n","malignant TN: 21.0\n","malignant FP: 9.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.6304  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 0.8324224948883057\n","minibatch AVG loss: 0.24119721055030824\n","Epoch: 25     train index of 5 minibatch: 2      time used: 0.6152880191802979\n","minibatch AVG loss: 0.27974669970571997\n","Epoch: 25     train index of 5 minibatch: 3      time used: 0.6144640445709229\n","minibatch AVG loss: 0.37889354676008224\n","\n","Epoch: 25  train \n","Loss: 0.2981  Acc: 84.0580\n","benign precision: 83.3333  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 86.8421\n","benign TP: 25.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 5.0\n","malignant precision: 86.8421  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 83.3333\n","malignant TP: 33.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 89.5126  Acc: 68.7500\n","benign precision: 66.6667  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 70.0000\n","benign TP: 4.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 3.0\n","malignant precision: 70.0000  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 66.6667\n","malignant TP: 7.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 0.8296685218811035\n","minibatch AVG loss: 0.296914990991354\n","Epoch: 26     train index of 5 minibatch: 2      time used: 0.612706184387207\n","minibatch AVG loss: 0.3965958744287491\n","Epoch: 26     train index of 5 minibatch: 3      time used: 0.6149082183837891\n","minibatch AVG loss: 0.3555706351995468\n","\n","Epoch: 26  train \n","Loss: 0.3232  Acc: 88.4058\n","benign precision: 92.5926  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 87.8049\n","benign TP: 25.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 5.0\n","malignant precision: 87.8049  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 92.5926\n","malignant TP: 36.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 167.7719  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 0.8289055824279785\n","minibatch AVG loss: 1.1195923924446105\n","Epoch: 27     train index of 5 minibatch: 2      time used: 0.6143951416015625\n","minibatch AVG loss: 0.5742907140403986\n","Epoch: 27     train index of 5 minibatch: 3      time used: 0.6119563579559326\n","minibatch AVG loss: 0.33779953606426716\n","\n","Epoch: 27  train \n","Loss: 0.5902  Acc: 72.4638\n","benign precision: 70.3704  recall: 65.5172\n","benign sensitivity: 65.5172  specificity: 79.4872\n","benign FPR: 20.5128  NPV: 75.6098\n","benign TP: 19.0\n","benign TN: 31.0\n","benign FP: 8.0\n","benign FN: 10.0\n","malignant precision: 75.6098  recall: 79.4872\n","malignant sensitivity: 79.4872  specificity: 65.5172\n","malignant FPR: 34.4828  NPV: 70.3704\n","malignant TP: 31.0\n","malignant TN: 19.0\n","malignant FP: 10.0\n","malignant FN: 8.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 6.2768  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 0.8272860050201416\n","minibatch AVG loss: 0.9233411312103271\n","Epoch: 28     train index of 5 minibatch: 2      time used: 0.6135096549987793\n","minibatch AVG loss: 0.507437040284276\n","Epoch: 28     train index of 5 minibatch: 3      time used: 0.6130244731903076\n","minibatch AVG loss: 0.8363240957260132\n","\n","Epoch: 28  train \n","Loss: 0.7197  Acc: 73.9130\n","benign precision: 74.0741  recall: 66.6667\n","benign sensitivity: 66.6667  specificity: 81.5789\n","benign FPR: 18.4211  NPV: 75.6098\n","benign TP: 20.0\n","benign TN: 31.0\n","benign FP: 7.0\n","benign FN: 10.0\n","malignant precision: 75.6098  recall: 81.5789\n","malignant sensitivity: 81.5789  specificity: 66.6667\n","malignant FPR: 33.3333  NPV: 74.0741\n","malignant TP: 31.0\n","malignant TN: 20.0\n","malignant FP: 10.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 675.6941  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 0.8301255702972412\n","minibatch AVG loss: 0.29909760020673276\n","Epoch: 29     train index of 5 minibatch: 2      time used: 0.6143057346343994\n","minibatch AVG loss: 0.5074305040761828\n","Epoch: 29     train index of 5 minibatch: 3      time used: 0.613534688949585\n","minibatch AVG loss: 0.4905051112174988\n","\n","Epoch: 29  train \n","Loss: 0.4408  Acc: 81.1594\n","benign precision: 74.2857  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 76.9231\n","benign FPR: 23.0769  NPV: 90.9091\n","benign TP: 26.0\n","benign TN: 30.0\n","benign FP: 9.0\n","benign FN: 3.0\n","malignant precision: 90.9091  recall: 76.9231\n","malignant sensitivity: 76.9231  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 74.2857\n","malignant TP: 30.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 9.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.5463  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 0.8389585018157959\n","minibatch AVG loss: 0.4130612950772047\n","Epoch: 30     train index of 5 minibatch: 2      time used: 0.6160163879394531\n","minibatch AVG loss: 0.41299573073047213\n","Epoch: 30     train index of 5 minibatch: 3      time used: 0.611281156539917\n","minibatch AVG loss: 0.6955630071461201\n","\n","Epoch: 30  train \n","Loss: 0.4737  Acc: 81.1594\n","benign precision: 82.1429  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 82.5000\n","benign TP: 23.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 7.0\n","malignant precision: 82.5000  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 82.1429\n","malignant TP: 33.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.2490  Acc: 87.5000\n","benign precision: 85.7143  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 88.8889\n","benign TP: 6.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 88.8889  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 85.7143\n","malignant TP: 8.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 0.8351964950561523\n","minibatch AVG loss: 1.0610498115420341\n","Epoch: 31     train index of 5 minibatch: 2      time used: 0.6153759956359863\n","minibatch AVG loss: 0.11315603405237198\n","Epoch: 31     train index of 5 minibatch: 3      time used: 0.6147744655609131\n","minibatch AVG loss: 0.49547806680202483\n","\n","Epoch: 31  train \n","Loss: 0.5389  Acc: 78.2609\n","benign precision: 78.5714  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 80.0000\n","benign TP: 22.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 8.0\n","malignant precision: 80.0000  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 78.5714\n","malignant TP: 32.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 1582.8062  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 0.8339414596557617\n","minibatch AVG loss: 0.4620880216360092\n","Epoch: 32     train index of 5 minibatch: 2      time used: 0.6143548488616943\n","minibatch AVG loss: 0.17873241603374482\n","Epoch: 32     train index of 5 minibatch: 3      time used: 0.6118640899658203\n","minibatch AVG loss: 0.6243561565876007\n","\n","Epoch: 32  train \n","Loss: 0.4085  Acc: 78.2609\n","benign precision: 76.6667  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 81.5789\n","benign FPR: 18.4211  NPV: 81.5789\n","benign TP: 23.0\n","benign TN: 31.0\n","benign FP: 7.0\n","benign FN: 7.0\n","malignant precision: 81.5789  recall: 81.5789\n","malignant sensitivity: 81.5789  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 76.6667\n","malignant TP: 31.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 837.1430  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 0.8299551010131836\n","minibatch AVG loss: 0.6884774550795555\n","Epoch: 33     train index of 5 minibatch: 2      time used: 0.6132979393005371\n","minibatch AVG loss: 0.12143784388899803\n","Epoch: 33     train index of 5 minibatch: 3      time used: 0.6141502857208252\n","minibatch AVG loss: 0.661178269609809\n","\n","Epoch: 33  train \n","Loss: 0.5122  Acc: 78.2609\n","benign precision: 77.7778  recall: 72.4138\n","benign sensitivity: 72.4138  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 80.4878\n","benign TP: 21.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 8.0\n","malignant precision: 80.4878  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 72.4138\n","malignant FPR: 27.5862  NPV: 77.7778\n","malignant TP: 33.0\n","malignant TN: 21.0\n","malignant FP: 8.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 1273.1202  Acc: 68.7500\n","benign precision: 75.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 66.6667\n","benign TP: 3.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 4.0\n","malignant precision: 66.6667  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 75.0000\n","malignant TP: 8.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 0.8284127712249756\n","minibatch AVG loss: 0.09096804708242416\n","Epoch: 34     train index of 5 minibatch: 2      time used: 0.612328052520752\n","minibatch AVG loss: 0.7426744999364019\n","Epoch: 34     train index of 5 minibatch: 3      time used: 0.614518404006958\n","minibatch AVG loss: 0.4650731852278113\n","\n","Epoch: 34  train \n","Loss: 0.3932  Acc: 91.3043\n","benign precision: 90.3226  recall: 93.3333\n","benign sensitivity: 93.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 94.5946\n","benign TP: 28.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 2.0\n","malignant precision: 94.5946  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 93.3333\n","malignant FPR: 6.6667  NPV: 90.3226\n","malignant TP: 35.0\n","malignant TN: 28.0\n","malignant FP: 2.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 6.7804  Acc: 75.0000\n","benign precision: 71.4286  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 77.7778\n","benign FPR: 22.2222  NPV: 77.7778\n","benign TP: 5.0\n","benign TN: 7.0\n","benign FP: 2.0\n","benign FN: 2.0\n","malignant precision: 77.7778  recall: 77.7778\n","malignant sensitivity: 77.7778  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 71.4286\n","malignant TP: 7.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 0.8272762298583984\n","minibatch AVG loss: 0.5027392895426601\n","Epoch: 35     train index of 5 minibatch: 2      time used: 0.6138522624969482\n","minibatch AVG loss: 0.5666448891162872\n","Epoch: 35     train index of 5 minibatch: 3      time used: 0.6142408847808838\n","minibatch AVG loss: 0.19388769138604403\n","\n","Epoch: 35  train \n","Loss: 0.3765  Acc: 84.0580\n","benign precision: 78.7879  recall: 89.6552\n","benign sensitivity: 89.6552  specificity: 82.0513\n","benign FPR: 17.9487  NPV: 91.4286\n","benign TP: 26.0\n","benign TN: 32.0\n","benign FP: 7.0\n","benign FN: 3.0\n","malignant precision: 91.4286  recall: 82.0513\n","malignant sensitivity: 82.0513  specificity: 89.6552\n","malignant FPR: 10.3448  NPV: 78.7879\n","malignant TP: 32.0\n","malignant TN: 26.0\n","malignant FP: 3.0\n","malignant FN: 7.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.4683  Acc: 87.5000\n","benign precision: 85.7143  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 88.8889\n","benign TP: 6.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 88.8889  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 85.7143\n","malignant TP: 8.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 0.8307263851165771\n","minibatch AVG loss: 0.8649134477600455\n","Epoch: 36     train index of 5 minibatch: 2      time used: 0.6146156787872314\n","minibatch AVG loss: 0.31774636879563334\n","Epoch: 36     train index of 5 minibatch: 3      time used: 0.612912654876709\n","minibatch AVG loss: 0.9519572556018829\n","\n","Epoch: 36  train \n","Loss: 0.6523  Acc: 73.9130\n","benign precision: 68.5714  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 71.0526\n","benign FPR: 28.9474  NPV: 81.8182\n","benign TP: 24.0\n","benign TN: 27.0\n","benign FP: 11.0\n","benign FN: 6.0\n","malignant precision: 81.8182  recall: 71.0526\n","malignant sensitivity: 71.0526  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 68.5714\n","malignant TP: 27.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 11.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.4428  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 0.8293187618255615\n","minibatch AVG loss: 0.49584939852356913\n","Epoch: 37     train index of 5 minibatch: 2      time used: 0.6127469539642334\n","minibatch AVG loss: 0.1942448154091835\n","Epoch: 37     train index of 5 minibatch: 3      time used: 0.6110556125640869\n","minibatch AVG loss: 0.1508466675877571\n","\n","Epoch: 37  train \n","Loss: 0.2802  Acc: 89.8551\n","benign precision: 90.0000  recall: 90.0000\n","benign sensitivity: 90.0000  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 92.1053\n","benign TP: 27.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 3.0\n","malignant precision: 92.1053  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 90.0000\n","malignant FPR: 10.0000  NPV: 90.0000\n","malignant TP: 35.0\n","malignant TN: 27.0\n","malignant FP: 3.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.3679  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 0.8273694515228271\n","minibatch AVG loss: 0.6400252597406506\n","Epoch: 38     train index of 5 minibatch: 2      time used: 0.6140363216400146\n","minibatch AVG loss: 0.31968437805771827\n","Epoch: 38     train index of 5 minibatch: 3      time used: 0.6149864196777344\n","minibatch AVG loss: 0.10949399620294571\n","\n","Epoch: 38  train \n","Loss: 0.3879  Acc: 85.5072\n","benign precision: 83.3333  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 89.4737\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 83.3333\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.6550  Acc: 75.0000\n","benign precision: 100.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 69.2308\n","benign TP: 3.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 69.2308  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 0.8325502872467041\n","minibatch AVG loss: 0.36760849207639695\n","Epoch: 39     train index of 5 minibatch: 2      time used: 0.6154763698577881\n","minibatch AVG loss: 0.4010841013863683\n","Epoch: 39     train index of 5 minibatch: 3      time used: 0.6162073612213135\n","minibatch AVG loss: 0.34924308583140373\n","\n","Epoch: 39  train \n","Loss: 0.4019  Acc: 85.5072\n","benign precision: 83.8710  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 86.8421\n","benign FPR: 13.1579  NPV: 89.1892\n","benign TP: 26.0\n","benign TN: 33.0\n","benign FP: 5.0\n","benign FN: 4.0\n","malignant precision: 89.1892  recall: 86.8421\n","malignant sensitivity: 86.8421  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 83.8710\n","malignant TP: 33.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.3022  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 0.8278565406799316\n","minibatch AVG loss: 0.40117332600057126\n","Epoch: 40     train index of 5 minibatch: 2      time used: 0.614738941192627\n","minibatch AVG loss: 0.17695634253323078\n","Epoch: 40     train index of 5 minibatch: 3      time used: 0.6139616966247559\n","minibatch AVG loss: 0.4183753401041031\n","\n","Epoch: 40  train \n","Loss: 0.3432  Acc: 82.6087\n","benign precision: 84.6154  recall: 75.8621\n","benign sensitivity: 75.8621  specificity: 89.7436\n","benign FPR: 10.2564  NPV: 83.3333\n","benign TP: 22.0\n","benign TN: 35.0\n","benign FP: 4.0\n","benign FN: 7.0\n","malignant precision: 83.3333  recall: 89.7436\n","malignant sensitivity: 89.7436  specificity: 75.8621\n","malignant FPR: 24.1379  NPV: 84.6154\n","malignant TP: 35.0\n","malignant TN: 22.0\n","malignant FP: 7.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.4715  Acc: 81.2500\n","benign precision: 100.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 75.0000\n","benign TP: 4.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 3.0\n","malignant precision: 75.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 0.82916259765625\n","minibatch AVG loss: 0.2822773244231939\n","Epoch: 41     train index of 5 minibatch: 2      time used: 0.6144611835479736\n","minibatch AVG loss: 0.6872972034849226\n","Epoch: 41     train index of 5 minibatch: 3      time used: 0.6140365600585938\n","minibatch AVG loss: 0.10484684824477881\n","\n","Epoch: 41  train \n","Loss: 0.3150  Acc: 86.9565\n","benign precision: 89.2857  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 87.5000\n","benign TP: 25.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.5000  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 89.2857\n","malignant TP: 35.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 75.2147  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 0.8460140228271484\n","minibatch AVG loss: 0.5246941767632961\n","Epoch: 42     train index of 5 minibatch: 2      time used: 0.6139993667602539\n","minibatch AVG loss: 0.2744161393493414\n","Epoch: 42     train index of 5 minibatch: 3      time used: 0.6142804622650146\n","minibatch AVG loss: 0.6188558645546436\n","\n","Epoch: 42  train \n","Loss: 0.4391  Acc: 82.6087\n","benign precision: 88.0000  recall: 73.3333\n","benign sensitivity: 73.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 81.3953\n","benign TP: 22.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 8.0\n","malignant precision: 81.3953  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 73.3333\n","malignant FPR: 26.6667  NPV: 88.0000\n","malignant TP: 35.0\n","malignant TN: 22.0\n","malignant FP: 8.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 637.0967  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 0.8318660259246826\n","minibatch AVG loss: 0.5593993373215198\n","Epoch: 43     train index of 5 minibatch: 2      time used: 0.6146671772003174\n","minibatch AVG loss: 0.5368828888051211\n","Epoch: 43     train index of 5 minibatch: 3      time used: 0.613600492477417\n","minibatch AVG loss: 0.8678288780152797\n","\n","Epoch: 43  train \n","Loss: 0.7300  Acc: 79.7101\n","benign precision: 79.3103  recall: 76.6667\n","benign sensitivity: 76.6667  specificity: 84.2105\n","benign FPR: 15.7895  NPV: 82.0513\n","benign TP: 23.0\n","benign TN: 32.0\n","benign FP: 6.0\n","benign FN: 7.0\n","malignant precision: 82.0513  recall: 84.2105\n","malignant sensitivity: 84.2105  specificity: 76.6667\n","malignant FPR: 23.3333  NPV: 79.3103\n","malignant TP: 32.0\n","malignant TN: 23.0\n","malignant FP: 7.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.3919  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 0.8343920707702637\n","minibatch AVG loss: 0.10508509967476129\n","Epoch: 44     train index of 5 minibatch: 2      time used: 0.6133725643157959\n","minibatch AVG loss: 0.5021856635808944\n","Epoch: 44     train index of 5 minibatch: 3      time used: 0.6132490634918213\n","minibatch AVG loss: 0.2171864679083228\n","\n","Epoch: 44  train \n","Loss: 0.2404  Acc: 86.9565\n","benign precision: 89.2857  recall: 83.3333\n","benign sensitivity: 83.3333  specificity: 92.1053\n","benign FPR: 7.8947  NPV: 87.5000\n","benign TP: 25.0\n","benign TN: 35.0\n","benign FP: 3.0\n","benign FN: 5.0\n","malignant precision: 87.5000  recall: 92.1053\n","malignant sensitivity: 92.1053  specificity: 83.3333\n","malignant FPR: 16.6667  NPV: 89.2857\n","malignant TP: 35.0\n","malignant TN: 25.0\n","malignant FP: 5.0\n","malignant FN: 3.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 35.0497  Acc: 75.0000\n","benign precision: 80.0000  recall: 57.1429\n","benign sensitivity: 57.1429  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 72.7273\n","benign TP: 4.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 3.0\n","malignant precision: 72.7273  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 57.1429\n","malignant FPR: 42.8571  NPV: 80.0000\n","malignant TP: 8.0\n","malignant TN: 4.0\n","malignant FP: 3.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 0.8290650844573975\n","minibatch AVG loss: 0.04059755392372608\n","Epoch: 45     train index of 5 minibatch: 2      time used: 0.6140186786651611\n","minibatch AVG loss: 0.4932128444314003\n","Epoch: 45     train index of 5 minibatch: 3      time used: 0.6171860694885254\n","minibatch AVG loss: 0.5149665903300047\n","\n","Epoch: 45  train \n","Loss: 0.3365  Acc: 86.9565\n","benign precision: 92.3077  recall: 80.0000\n","benign sensitivity: 80.0000  specificity: 94.7368\n","benign FPR: 5.2632  NPV: 85.7143\n","benign TP: 24.0\n","benign TN: 36.0\n","benign FP: 2.0\n","benign FN: 6.0\n","malignant precision: 85.7143  recall: 94.7368\n","malignant sensitivity: 94.7368  specificity: 80.0000\n","malignant FPR: 20.0000  NPV: 92.3077\n","malignant TP: 36.0\n","malignant TN: 24.0\n","malignant FP: 6.0\n","malignant FN: 2.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.4295  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 0.8262894153594971\n","minibatch AVG loss: 0.24851269517093896\n","Epoch: 46     train index of 5 minibatch: 2      time used: 0.6159632205963135\n","minibatch AVG loss: 0.2512459225952625\n","Epoch: 46     train index of 5 minibatch: 3      time used: 0.6141629219055176\n","minibatch AVG loss: 0.341335628926754\n","\n","Epoch: 46  train \n","Loss: 0.2982  Acc: 86.9565\n","benign precision: 86.6667  recall: 86.6667\n","benign sensitivity: 86.6667  specificity: 89.4737\n","benign FPR: 10.5263  NPV: 89.4737\n","benign TP: 26.0\n","benign TN: 34.0\n","benign FP: 4.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 89.4737\n","malignant sensitivity: 89.4737  specificity: 86.6667\n","malignant FPR: 13.3333  NPV: 86.6667\n","malignant TP: 34.0\n","malignant TN: 26.0\n","malignant FP: 4.0\n","malignant FN: 4.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.4208  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 0.8439474105834961\n","minibatch AVG loss: 0.43517268896102906\n","Epoch: 47     train index of 5 minibatch: 2      time used: 0.6150820255279541\n","minibatch AVG loss: 0.08442978765815497\n","Epoch: 47     train index of 5 minibatch: 3      time used: 0.6122210025787354\n","minibatch AVG loss: 0.44352425523102285\n","\n","Epoch: 47  train \n","Loss: 0.3433  Acc: 81.1594\n","benign precision: 79.3103  recall: 79.3103\n","benign sensitivity: 79.3103  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 84.6154\n","benign TP: 23.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 6.0\n","malignant precision: 84.6154  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 79.3103\n","malignant FPR: 20.6897  NPV: 79.3103\n","malignant TP: 33.0\n","malignant TN: 23.0\n","malignant FP: 6.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.5268  Acc: 75.0000\n","benign precision: 100.0000  recall: 42.8571\n","benign sensitivity: 42.8571  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 69.2308\n","benign TP: 3.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 4.0\n","malignant precision: 69.2308  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 42.8571\n","malignant FPR: 57.1429  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 3.0\n","malignant FP: 4.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 0.8323268890380859\n","minibatch AVG loss: 0.5128003172576427\n","Epoch: 48     train index of 5 minibatch: 2      time used: 0.6147868633270264\n","minibatch AVG loss: 0.2592822477221489\n","Epoch: 48     train index of 5 minibatch: 3      time used: 0.6149835586547852\n","minibatch AVG loss: 0.2606227047275752\n","\n","Epoch: 48  train \n","Loss: 0.3142  Acc: 82.6087\n","benign precision: 82.1429  recall: 79.3103\n","benign sensitivity: 79.3103  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 85.0000\n","benign TP: 23.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 6.0\n","malignant precision: 85.0000  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 79.3103\n","malignant FPR: 20.6897  NPV: 82.1429\n","malignant TP: 34.0\n","malignant TN: 23.0\n","malignant FP: 6.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.2533  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 0.8343214988708496\n","minibatch AVG loss: 0.14984919577836991\n","Epoch: 49     train index of 5 minibatch: 2      time used: 0.615715503692627\n","minibatch AVG loss: 0.05611614491790533\n","Epoch: 49     train index of 5 minibatch: 3      time used: 0.6123046875\n","minibatch AVG loss: 0.5205739974975586\n","\n","Epoch: 49  train \n","Loss: 0.2118  Acc: 88.4058\n","benign precision: 82.3529  recall: 96.5517\n","benign sensitivity: 96.5517  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 97.0588\n","benign TP: 28.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 1.0\n","malignant precision: 97.0588  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 96.5517\n","malignant FPR: 3.4483  NPV: 82.3529\n","malignant TP: 33.0\n","malignant TN: 28.0\n","malignant FP: 1.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.2944  Acc: 87.5000\n","benign precision: 100.0000  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 81.8182\n","benign TP: 5.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 81.8182  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 0.8317093849182129\n","minibatch AVG loss: 0.37508045732975004\n","Epoch: 50     train index of 5 minibatch: 2      time used: 0.6167140007019043\n","minibatch AVG loss: 0.14638783410191536\n","Epoch: 50     train index of 5 minibatch: 3      time used: 0.6125755310058594\n","minibatch AVG loss: 0.9324848413467407\n","\n","Epoch: 50  train \n","Loss: 0.4276  Acc: 85.5072\n","benign precision: 83.3333  recall: 86.2069\n","benign sensitivity: 86.2069  specificity: 87.1795\n","benign FPR: 12.8205  NPV: 89.4737\n","benign TP: 25.0\n","benign TN: 34.0\n","benign FP: 5.0\n","benign FN: 4.0\n","malignant precision: 89.4737  recall: 87.1795\n","malignant sensitivity: 87.1795  specificity: 86.2069\n","malignant FPR: 13.7931  NPV: 83.3333\n","malignant TP: 34.0\n","malignant TN: 25.0\n","malignant FP: 4.0\n","malignant FN: 5.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 225.8420  Acc: 81.2500\n","benign precision: 83.3333  recall: 71.4286\n","benign sensitivity: 71.4286  specificity: 88.8889\n","benign FPR: 11.1111  NPV: 80.0000\n","benign TP: 5.0\n","benign TN: 8.0\n","benign FP: 1.0\n","benign FN: 2.0\n","malignant precision: 80.0000  recall: 88.8889\n","malignant sensitivity: 88.8889  specificity: 71.4286\n","malignant FPR: 28.5714  NPV: 83.3333\n","malignant TP: 8.0\n","malignant TN: 5.0\n","malignant FP: 2.0\n","malignant FN: 1.0\n","\n","\n","\n","Training complete in 2m 12s\n","Best epoch idx:  37\n","Best epoch train Acc: 89.855072\n","Best epoch val Acc: 93.750000\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z2jo4ttKTTNL"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","*********************************setting*************************************\n","Namespace(Pre_Trained_model_path=None, att_module='SimAM', attn_drop_rate=0.0, augmentation_name=None, backbone_PT_off=False, batch_size=4, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=False, enable_notify=False, enable_sam=False, enable_tensorboard=True, enable_visualize_check=False, gpu_idx=-1, intake_epochs=0, linearprobing=False, lr=1e-05, lrf=0.05, model_idx='swin_b_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, num_epochs=50, num_workers=2, opt_name='Adam', paint=True, pos_embedding_off=False)\n","we dont have more GPU idx here, try to use gpu_idx=0\n","['swin_base_patch4_window7_224',\n"," 'swin_base_patch4_window7_224_in22k',\n"," 'swin_base_patch4_window12_384',\n"," 'swin_base_patch4_window12_384_in22k',\n"," 'swin_large_patch4_window7_224',\n"," 'swin_large_patch4_window7_224_in22k',\n"," 'swin_large_patch4_window12_384',\n"," 'swin_large_patch4_window12_384_in22k',\n"," 'swin_small_patch4_window7_224',\n"," 'swin_tiny_patch4_window7_224']\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_base_patch4_window12_384_22kto1k.pth\" to /root/.cache/torch/hub/checkpoints/swin_base_patch4_window12_384_22kto1k.pth\n","test model output： tensor([[-0.0427, -0.0044]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","GPU: 0\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1          [-1, 128, 96, 96]           6,272\n","         LayerNorm-2            [-1, 9216, 128]             256\n","        PatchEmbed-3            [-1, 9216, 128]               0\n","           Dropout-4            [-1, 9216, 128]               0\n","         LayerNorm-5            [-1, 9216, 128]             256\n","            Linear-6             [-1, 144, 384]          49,536\n","           Softmax-7          [-1, 4, 144, 144]               0\n","           Dropout-8          [-1, 4, 144, 144]               0\n","            Linear-9             [-1, 144, 128]          16,512\n","          Dropout-10             [-1, 144, 128]               0\n","  WindowAttention-11             [-1, 144, 128]               0\n","         Identity-12            [-1, 9216, 128]               0\n","        LayerNorm-13            [-1, 9216, 128]             256\n","           Linear-14            [-1, 9216, 512]          66,048\n","             GELU-15            [-1, 9216, 512]               0\n","          Dropout-16            [-1, 9216, 512]               0\n","           Linear-17            [-1, 9216, 128]          65,664\n","          Dropout-18            [-1, 9216, 128]               0\n","              Mlp-19            [-1, 9216, 128]               0\n","         Identity-20            [-1, 9216, 128]               0\n","SwinTransformerBlock-21            [-1, 9216, 128]               0\n","        LayerNorm-22            [-1, 9216, 128]             256\n","           Linear-23             [-1, 144, 384]          49,536\n","          Softmax-24          [-1, 4, 144, 144]               0\n","          Dropout-25          [-1, 4, 144, 144]               0\n","           Linear-26             [-1, 144, 128]          16,512\n","          Dropout-27             [-1, 144, 128]               0\n","  WindowAttention-28             [-1, 144, 128]               0\n","         DropPath-29            [-1, 9216, 128]               0\n","        LayerNorm-30            [-1, 9216, 128]             256\n","           Linear-31            [-1, 9216, 512]          66,048\n","             GELU-32            [-1, 9216, 512]               0\n","          Dropout-33            [-1, 9216, 512]               0\n","           Linear-34            [-1, 9216, 128]          65,664\n","          Dropout-35            [-1, 9216, 128]               0\n","              Mlp-36            [-1, 9216, 128]               0\n","         DropPath-37            [-1, 9216, 128]               0\n","SwinTransformerBlock-38            [-1, 9216, 128]               0\n","        LayerNorm-39            [-1, 2304, 512]           1,024\n","           Linear-40            [-1, 2304, 256]         131,072\n","     PatchMerging-41            [-1, 2304, 256]               0\n","       BasicLayer-42            [-1, 2304, 256]               0\n","        LayerNorm-43            [-1, 2304, 256]             512\n","           Linear-44             [-1, 144, 768]         197,376\n","          Softmax-45          [-1, 8, 144, 144]               0\n","          Dropout-46          [-1, 8, 144, 144]               0\n","           Linear-47             [-1, 144, 256]          65,792\n","          Dropout-48             [-1, 144, 256]               0\n","  WindowAttention-49             [-1, 144, 256]               0\n","         DropPath-50            [-1, 2304, 256]               0\n","        LayerNorm-51            [-1, 2304, 256]             512\n","           Linear-52           [-1, 2304, 1024]         263,168\n","             GELU-53           [-1, 2304, 1024]               0\n","          Dropout-54           [-1, 2304, 1024]               0\n","           Linear-55            [-1, 2304, 256]         262,400\n","          Dropout-56            [-1, 2304, 256]               0\n","              Mlp-57            [-1, 2304, 256]               0\n","         DropPath-58            [-1, 2304, 256]               0\n","SwinTransformerBlock-59            [-1, 2304, 256]               0\n","        LayerNorm-60            [-1, 2304, 256]             512\n","           Linear-61             [-1, 144, 768]         197,376\n","          Softmax-62          [-1, 8, 144, 144]               0\n","          Dropout-63          [-1, 8, 144, 144]               0\n","           Linear-64             [-1, 144, 256]          65,792\n","          Dropout-65             [-1, 144, 256]               0\n","  WindowAttention-66             [-1, 144, 256]               0\n","         DropPath-67            [-1, 2304, 256]               0\n","        LayerNorm-68            [-1, 2304, 256]             512\n","           Linear-69           [-1, 2304, 1024]         263,168\n","             GELU-70           [-1, 2304, 1024]               0\n","          Dropout-71           [-1, 2304, 1024]               0\n","           Linear-72            [-1, 2304, 256]         262,400\n","          Dropout-73            [-1, 2304, 256]               0\n","              Mlp-74            [-1, 2304, 256]               0\n","         DropPath-75            [-1, 2304, 256]               0\n","SwinTransformerBlock-76            [-1, 2304, 256]               0\n","        LayerNorm-77            [-1, 576, 1024]           2,048\n","           Linear-78             [-1, 576, 512]         524,288\n","     PatchMerging-79             [-1, 576, 512]               0\n","       BasicLayer-80             [-1, 576, 512]               0\n","        LayerNorm-81             [-1, 576, 512]           1,024\n","           Linear-82            [-1, 144, 1536]         787,968\n","          Softmax-83         [-1, 16, 144, 144]               0\n","          Dropout-84         [-1, 16, 144, 144]               0\n","           Linear-85             [-1, 144, 512]         262,656\n","          Dropout-86             [-1, 144, 512]               0\n","  WindowAttention-87             [-1, 144, 512]               0\n","         DropPath-88             [-1, 576, 512]               0\n","        LayerNorm-89             [-1, 576, 512]           1,024\n","           Linear-90            [-1, 576, 2048]       1,050,624\n","             GELU-91            [-1, 576, 2048]               0\n","          Dropout-92            [-1, 576, 2048]               0\n","           Linear-93             [-1, 576, 512]       1,049,088\n","          Dropout-94             [-1, 576, 512]               0\n","              Mlp-95             [-1, 576, 512]               0\n","         DropPath-96             [-1, 576, 512]               0\n","SwinTransformerBlock-97             [-1, 576, 512]               0\n","        LayerNorm-98             [-1, 576, 512]           1,024\n","           Linear-99            [-1, 144, 1536]         787,968\n","         Softmax-100         [-1, 16, 144, 144]               0\n","         Dropout-101         [-1, 16, 144, 144]               0\n","          Linear-102             [-1, 144, 512]         262,656\n","         Dropout-103             [-1, 144, 512]               0\n"," WindowAttention-104             [-1, 144, 512]               0\n","        DropPath-105             [-1, 576, 512]               0\n","       LayerNorm-106             [-1, 576, 512]           1,024\n","          Linear-107            [-1, 576, 2048]       1,050,624\n","            GELU-108            [-1, 576, 2048]               0\n","         Dropout-109            [-1, 576, 2048]               0\n","          Linear-110             [-1, 576, 512]       1,049,088\n","         Dropout-111             [-1, 576, 512]               0\n","             Mlp-112             [-1, 576, 512]               0\n","        DropPath-113             [-1, 576, 512]               0\n","SwinTransformerBlock-114             [-1, 576, 512]               0\n","       LayerNorm-115             [-1, 576, 512]           1,024\n","          Linear-116            [-1, 144, 1536]         787,968\n","         Softmax-117         [-1, 16, 144, 144]               0\n","         Dropout-118         [-1, 16, 144, 144]               0\n","          Linear-119             [-1, 144, 512]         262,656\n","         Dropout-120             [-1, 144, 512]               0\n"," WindowAttention-121             [-1, 144, 512]               0\n","        DropPath-122             [-1, 576, 512]               0\n","       LayerNorm-123             [-1, 576, 512]           1,024\n","          Linear-124            [-1, 576, 2048]       1,050,624\n","            GELU-125            [-1, 576, 2048]               0\n","         Dropout-126            [-1, 576, 2048]               0\n","          Linear-127             [-1, 576, 512]       1,049,088\n","         Dropout-128             [-1, 576, 512]               0\n","             Mlp-129             [-1, 576, 512]               0\n","        DropPath-130             [-1, 576, 512]               0\n","SwinTransformerBlock-131             [-1, 576, 512]               0\n","       LayerNorm-132             [-1, 576, 512]           1,024\n","          Linear-133            [-1, 144, 1536]         787,968\n","         Softmax-134         [-1, 16, 144, 144]               0\n","         Dropout-135         [-1, 16, 144, 144]               0\n","          Linear-136             [-1, 144, 512]         262,656\n","         Dropout-137             [-1, 144, 512]               0\n"," WindowAttention-138             [-1, 144, 512]               0\n","        DropPath-139             [-1, 576, 512]               0\n","       LayerNorm-140             [-1, 576, 512]           1,024\n","          Linear-141            [-1, 576, 2048]       1,050,624\n","            GELU-142            [-1, 576, 2048]               0\n","         Dropout-143            [-1, 576, 2048]               0\n","          Linear-144             [-1, 576, 512]       1,049,088\n","         Dropout-145             [-1, 576, 512]               0\n","             Mlp-146             [-1, 576, 512]               0\n","        DropPath-147             [-1, 576, 512]               0\n","SwinTransformerBlock-148             [-1, 576, 512]               0\n","       LayerNorm-149             [-1, 576, 512]           1,024\n","          Linear-150            [-1, 144, 1536]         787,968\n","         Softmax-151         [-1, 16, 144, 144]               0\n","         Dropout-152         [-1, 16, 144, 144]               0\n","          Linear-153             [-1, 144, 512]         262,656\n","         Dropout-154             [-1, 144, 512]               0\n"," WindowAttention-155             [-1, 144, 512]               0\n","        DropPath-156             [-1, 576, 512]               0\n","       LayerNorm-157             [-1, 576, 512]           1,024\n","          Linear-158            [-1, 576, 2048]       1,050,624\n","            GELU-159            [-1, 576, 2048]               0\n","         Dropout-160            [-1, 576, 2048]               0\n","          Linear-161             [-1, 576, 512]       1,049,088\n","         Dropout-162             [-1, 576, 512]               0\n","             Mlp-163             [-1, 576, 512]               0\n","        DropPath-164             [-1, 576, 512]               0\n","SwinTransformerBlock-165             [-1, 576, 512]               0\n","       LayerNorm-166             [-1, 576, 512]           1,024\n","          Linear-167            [-1, 144, 1536]         787,968\n","         Softmax-168         [-1, 16, 144, 144]               0\n","         Dropout-169         [-1, 16, 144, 144]               0\n","          Linear-170             [-1, 144, 512]         262,656\n","         Dropout-171             [-1, 144, 512]               0\n"," WindowAttention-172             [-1, 144, 512]               0\n","        DropPath-173             [-1, 576, 512]               0\n","       LayerNorm-174             [-1, 576, 512]           1,024\n","          Linear-175            [-1, 576, 2048]       1,050,624\n","            GELU-176            [-1, 576, 2048]               0\n","         Dropout-177            [-1, 576, 2048]               0\n","          Linear-178             [-1, 576, 512]       1,049,088\n","         Dropout-179             [-1, 576, 512]               0\n","             Mlp-180             [-1, 576, 512]               0\n","        DropPath-181             [-1, 576, 512]               0\n","SwinTransformerBlock-182             [-1, 576, 512]               0\n","       LayerNorm-183             [-1, 576, 512]           1,024\n","          Linear-184            [-1, 144, 1536]         787,968\n","         Softmax-185         [-1, 16, 144, 144]               0\n","         Dropout-186         [-1, 16, 144, 144]               0\n","          Linear-187             [-1, 144, 512]         262,656\n","         Dropout-188             [-1, 144, 512]               0\n"," WindowAttention-189             [-1, 144, 512]               0\n","        DropPath-190             [-1, 576, 512]               0\n","       LayerNorm-191             [-1, 576, 512]           1,024\n","          Linear-192            [-1, 576, 2048]       1,050,624\n","            GELU-193            [-1, 576, 2048]               0\n","         Dropout-194            [-1, 576, 2048]               0\n","          Linear-195             [-1, 576, 512]       1,049,088\n","         Dropout-196             [-1, 576, 512]               0\n","             Mlp-197             [-1, 576, 512]               0\n","        DropPath-198             [-1, 576, 512]               0\n","SwinTransformerBlock-199             [-1, 576, 512]               0\n","       LayerNorm-200             [-1, 576, 512]           1,024\n","          Linear-201            [-1, 144, 1536]         787,968\n","         Softmax-202         [-1, 16, 144, 144]               0\n","         Dropout-203         [-1, 16, 144, 144]               0\n","          Linear-204             [-1, 144, 512]         262,656\n","         Dropout-205             [-1, 144, 512]               0\n"," WindowAttention-206             [-1, 144, 512]               0\n","        DropPath-207             [-1, 576, 512]               0\n","       LayerNorm-208             [-1, 576, 512]           1,024\n","          Linear-209            [-1, 576, 2048]       1,050,624\n","            GELU-210            [-1, 576, 2048]               0\n","         Dropout-211            [-1, 576, 2048]               0\n","          Linear-212             [-1, 576, 512]       1,049,088\n","         Dropout-213             [-1, 576, 512]               0\n","             Mlp-214             [-1, 576, 512]               0\n","        DropPath-215             [-1, 576, 512]               0\n","SwinTransformerBlock-216             [-1, 576, 512]               0\n","       LayerNorm-217             [-1, 576, 512]           1,024\n","          Linear-218            [-1, 144, 1536]         787,968\n","         Softmax-219         [-1, 16, 144, 144]               0\n","         Dropout-220         [-1, 16, 144, 144]               0\n","          Linear-221             [-1, 144, 512]         262,656\n","         Dropout-222             [-1, 144, 512]               0\n"," WindowAttention-223             [-1, 144, 512]               0\n","        DropPath-224             [-1, 576, 512]               0\n","       LayerNorm-225             [-1, 576, 512]           1,024\n","          Linear-226            [-1, 576, 2048]       1,050,624\n","            GELU-227            [-1, 576, 2048]               0\n","         Dropout-228            [-1, 576, 2048]               0\n","          Linear-229             [-1, 576, 512]       1,049,088\n","         Dropout-230             [-1, 576, 512]               0\n","             Mlp-231             [-1, 576, 512]               0\n","        DropPath-232             [-1, 576, 512]               0\n","SwinTransformerBlock-233             [-1, 576, 512]               0\n","       LayerNorm-234             [-1, 576, 512]           1,024\n","          Linear-235            [-1, 144, 1536]         787,968\n","         Softmax-236         [-1, 16, 144, 144]               0\n","         Dropout-237         [-1, 16, 144, 144]               0\n","          Linear-238             [-1, 144, 512]         262,656\n","         Dropout-239             [-1, 144, 512]               0\n"," WindowAttention-240             [-1, 144, 512]               0\n","        DropPath-241             [-1, 576, 512]               0\n","       LayerNorm-242             [-1, 576, 512]           1,024\n","          Linear-243            [-1, 576, 2048]       1,050,624\n","            GELU-244            [-1, 576, 2048]               0\n","         Dropout-245            [-1, 576, 2048]               0\n","          Linear-246             [-1, 576, 512]       1,049,088\n","         Dropout-247             [-1, 576, 512]               0\n","             Mlp-248             [-1, 576, 512]               0\n","        DropPath-249             [-1, 576, 512]               0\n","SwinTransformerBlock-250             [-1, 576, 512]               0\n","       LayerNorm-251             [-1, 576, 512]           1,024\n","          Linear-252            [-1, 144, 1536]         787,968\n","         Softmax-253         [-1, 16, 144, 144]               0\n","         Dropout-254         [-1, 16, 144, 144]               0\n","          Linear-255             [-1, 144, 512]         262,656\n","         Dropout-256             [-1, 144, 512]               0\n"," WindowAttention-257             [-1, 144, 512]               0\n","        DropPath-258             [-1, 576, 512]               0\n","       LayerNorm-259             [-1, 576, 512]           1,024\n","          Linear-260            [-1, 576, 2048]       1,050,624\n","            GELU-261            [-1, 576, 2048]               0\n","         Dropout-262            [-1, 576, 2048]               0\n","          Linear-263             [-1, 576, 512]       1,049,088\n","         Dropout-264             [-1, 576, 512]               0\n","             Mlp-265             [-1, 576, 512]               0\n","        DropPath-266             [-1, 576, 512]               0\n","SwinTransformerBlock-267             [-1, 576, 512]               0\n","       LayerNorm-268             [-1, 576, 512]           1,024\n","          Linear-269            [-1, 144, 1536]         787,968\n","         Softmax-270         [-1, 16, 144, 144]               0\n","         Dropout-271         [-1, 16, 144, 144]               0\n","          Linear-272             [-1, 144, 512]         262,656\n","         Dropout-273             [-1, 144, 512]               0\n"," WindowAttention-274             [-1, 144, 512]               0\n","        DropPath-275             [-1, 576, 512]               0\n","       LayerNorm-276             [-1, 576, 512]           1,024\n","          Linear-277            [-1, 576, 2048]       1,050,624\n","            GELU-278            [-1, 576, 2048]               0\n","         Dropout-279            [-1, 576, 2048]               0\n","          Linear-280             [-1, 576, 512]       1,049,088\n","         Dropout-281             [-1, 576, 512]               0\n","             Mlp-282             [-1, 576, 512]               0\n","        DropPath-283             [-1, 576, 512]               0\n","SwinTransformerBlock-284             [-1, 576, 512]               0\n","       LayerNorm-285             [-1, 576, 512]           1,024\n","          Linear-286            [-1, 144, 1536]         787,968\n","         Softmax-287         [-1, 16, 144, 144]               0\n","         Dropout-288         [-1, 16, 144, 144]               0\n","          Linear-289             [-1, 144, 512]         262,656\n","         Dropout-290             [-1, 144, 512]               0\n"," WindowAttention-291             [-1, 144, 512]               0\n","        DropPath-292             [-1, 576, 512]               0\n","       LayerNorm-293             [-1, 576, 512]           1,024\n","          Linear-294            [-1, 576, 2048]       1,050,624\n","            GELU-295            [-1, 576, 2048]               0\n","         Dropout-296            [-1, 576, 2048]               0\n","          Linear-297             [-1, 576, 512]       1,049,088\n","         Dropout-298             [-1, 576, 512]               0\n","             Mlp-299             [-1, 576, 512]               0\n","        DropPath-300             [-1, 576, 512]               0\n","SwinTransformerBlock-301             [-1, 576, 512]               0\n","       LayerNorm-302             [-1, 576, 512]           1,024\n","          Linear-303            [-1, 144, 1536]         787,968\n","         Softmax-304         [-1, 16, 144, 144]               0\n","         Dropout-305         [-1, 16, 144, 144]               0\n","          Linear-306             [-1, 144, 512]         262,656\n","         Dropout-307             [-1, 144, 512]               0\n"," WindowAttention-308             [-1, 144, 512]               0\n","        DropPath-309             [-1, 576, 512]               0\n","       LayerNorm-310             [-1, 576, 512]           1,024\n","          Linear-311            [-1, 576, 2048]       1,050,624\n","            GELU-312            [-1, 576, 2048]               0\n","         Dropout-313            [-1, 576, 2048]               0\n","          Linear-314             [-1, 576, 512]       1,049,088\n","         Dropout-315             [-1, 576, 512]               0\n","             Mlp-316             [-1, 576, 512]               0\n","        DropPath-317             [-1, 576, 512]               0\n","SwinTransformerBlock-318             [-1, 576, 512]               0\n","       LayerNorm-319             [-1, 576, 512]           1,024\n","          Linear-320            [-1, 144, 1536]         787,968\n","         Softmax-321         [-1, 16, 144, 144]               0\n","         Dropout-322         [-1, 16, 144, 144]               0\n","          Linear-323             [-1, 144, 512]         262,656\n","         Dropout-324             [-1, 144, 512]               0\n"," WindowAttention-325             [-1, 144, 512]               0\n","        DropPath-326             [-1, 576, 512]               0\n","       LayerNorm-327             [-1, 576, 512]           1,024\n","          Linear-328            [-1, 576, 2048]       1,050,624\n","            GELU-329            [-1, 576, 2048]               0\n","         Dropout-330            [-1, 576, 2048]               0\n","          Linear-331             [-1, 576, 512]       1,049,088\n","         Dropout-332             [-1, 576, 512]               0\n","             Mlp-333             [-1, 576, 512]               0\n","        DropPath-334             [-1, 576, 512]               0\n","SwinTransformerBlock-335             [-1, 576, 512]               0\n","       LayerNorm-336             [-1, 576, 512]           1,024\n","          Linear-337            [-1, 144, 1536]         787,968\n","         Softmax-338         [-1, 16, 144, 144]               0\n","         Dropout-339         [-1, 16, 144, 144]               0\n","          Linear-340             [-1, 144, 512]         262,656\n","         Dropout-341             [-1, 144, 512]               0\n"," WindowAttention-342             [-1, 144, 512]               0\n","        DropPath-343             [-1, 576, 512]               0\n","       LayerNorm-344             [-1, 576, 512]           1,024\n","          Linear-345            [-1, 576, 2048]       1,050,624\n","            GELU-346            [-1, 576, 2048]               0\n","         Dropout-347            [-1, 576, 2048]               0\n","          Linear-348             [-1, 576, 512]       1,049,088\n","         Dropout-349             [-1, 576, 512]               0\n","             Mlp-350             [-1, 576, 512]               0\n","        DropPath-351             [-1, 576, 512]               0\n","SwinTransformerBlock-352             [-1, 576, 512]               0\n","       LayerNorm-353             [-1, 576, 512]           1,024\n","          Linear-354            [-1, 144, 1536]         787,968\n","         Softmax-355         [-1, 16, 144, 144]               0\n","         Dropout-356         [-1, 16, 144, 144]               0\n","          Linear-357             [-1, 144, 512]         262,656\n","         Dropout-358             [-1, 144, 512]               0\n"," WindowAttention-359             [-1, 144, 512]               0\n","        DropPath-360             [-1, 576, 512]               0\n","       LayerNorm-361             [-1, 576, 512]           1,024\n","          Linear-362            [-1, 576, 2048]       1,050,624\n","            GELU-363            [-1, 576, 2048]               0\n","         Dropout-364            [-1, 576, 2048]               0\n","          Linear-365             [-1, 576, 512]       1,049,088\n","         Dropout-366             [-1, 576, 512]               0\n","             Mlp-367             [-1, 576, 512]               0\n","        DropPath-368             [-1, 576, 512]               0\n","SwinTransformerBlock-369             [-1, 576, 512]               0\n","       LayerNorm-370             [-1, 576, 512]           1,024\n","          Linear-371            [-1, 144, 1536]         787,968\n","         Softmax-372         [-1, 16, 144, 144]               0\n","         Dropout-373         [-1, 16, 144, 144]               0\n","          Linear-374             [-1, 144, 512]         262,656\n","         Dropout-375             [-1, 144, 512]               0\n"," WindowAttention-376             [-1, 144, 512]               0\n","        DropPath-377             [-1, 576, 512]               0\n","       LayerNorm-378             [-1, 576, 512]           1,024\n","          Linear-379            [-1, 576, 2048]       1,050,624\n","            GELU-380            [-1, 576, 2048]               0\n","         Dropout-381            [-1, 576, 2048]               0\n","          Linear-382             [-1, 576, 512]       1,049,088\n","         Dropout-383             [-1, 576, 512]               0\n","             Mlp-384             [-1, 576, 512]               0\n","        DropPath-385             [-1, 576, 512]               0\n","SwinTransformerBlock-386             [-1, 576, 512]               0\n","       LayerNorm-387            [-1, 144, 2048]           4,096\n","          Linear-388            [-1, 144, 1024]       2,097,152\n","    PatchMerging-389            [-1, 144, 1024]               0\n","      BasicLayer-390            [-1, 144, 1024]               0\n","       LayerNorm-391            [-1, 144, 1024]           2,048\n","          Linear-392            [-1, 144, 3072]       3,148,800\n","         Softmax-393         [-1, 32, 144, 144]               0\n","         Dropout-394         [-1, 32, 144, 144]               0\n","          Linear-395            [-1, 144, 1024]       1,049,600\n","         Dropout-396            [-1, 144, 1024]               0\n"," WindowAttention-397            [-1, 144, 1024]               0\n","        DropPath-398            [-1, 144, 1024]               0\n","       LayerNorm-399            [-1, 144, 1024]           2,048\n","          Linear-400            [-1, 144, 4096]       4,198,400\n","            GELU-401            [-1, 144, 4096]               0\n","         Dropout-402            [-1, 144, 4096]               0\n","          Linear-403            [-1, 144, 1024]       4,195,328\n","         Dropout-404            [-1, 144, 1024]               0\n","             Mlp-405            [-1, 144, 1024]               0\n","        DropPath-406            [-1, 144, 1024]               0\n","SwinTransformerBlock-407            [-1, 144, 1024]               0\n","       LayerNorm-408            [-1, 144, 1024]           2,048\n","          Linear-409            [-1, 144, 3072]       3,148,800\n","         Softmax-410         [-1, 32, 144, 144]               0\n","         Dropout-411         [-1, 32, 144, 144]               0\n","          Linear-412            [-1, 144, 1024]       1,049,600\n","         Dropout-413            [-1, 144, 1024]               0\n"," WindowAttention-414            [-1, 144, 1024]               0\n","        DropPath-415            [-1, 144, 1024]               0\n","       LayerNorm-416            [-1, 144, 1024]           2,048\n","          Linear-417            [-1, 144, 4096]       4,198,400\n","            GELU-418            [-1, 144, 4096]               0\n","         Dropout-419            [-1, 144, 4096]               0\n","          Linear-420            [-1, 144, 1024]       4,195,328\n","         Dropout-421            [-1, 144, 1024]               0\n","             Mlp-422            [-1, 144, 1024]               0\n","        DropPath-423            [-1, 144, 1024]               0\n","SwinTransformerBlock-424            [-1, 144, 1024]               0\n","      BasicLayer-425            [-1, 144, 1024]               0\n","       LayerNorm-426            [-1, 144, 1024]           2,048\n","AdaptiveAvgPool1d-427              [-1, 1024, 1]               0\n","          Linear-428                    [-1, 2]           2,050\n","================================================================\n","Total params: 86,681,730\n","Trainable params: 86,681,730\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.69\n","Forward/backward pass size (MB): 1670.91\n","Params size (MB): 330.66\n","Estimated Total Size (MB): 2003.27\n","----------------------------------------------------------------\n","model : swin_b_384_401_PT_lf05_b4_warwick_CLS\n","no valid counterparts augmentation selected\n","Epoch 1/50\n","----------\n","Epoch: 1     train index of 5 minibatch: 1      time used: 1.9038598537445068\n","minibatch AVG loss: 0.7353549480438233\n","Epoch: 1     train index of 5 minibatch: 2      time used: 1.642911672592163\n","minibatch AVG loss: 0.7408145785331726\n","Epoch: 1     train index of 5 minibatch: 3      time used: 1.6441073417663574\n","minibatch AVG loss: 0.5590930759906769\n","\n","Epoch: 1  train \n","Loss: 0.6606  Acc: 56.5217\n","benign precision: 50.0000  recall: 20.6897\n","benign sensitivity: 20.6897  specificity: 84.6154\n","benign FPR: 15.3846  NPV: 58.9286\n","benign TP: 6.0\n","benign TN: 33.0\n","benign FP: 6.0\n","benign FN: 23.0\n","malignant precision: 58.9286  recall: 84.6154\n","malignant sensitivity: 84.6154  specificity: 20.6897\n","malignant FPR: 79.3103  NPV: 50.0000\n","malignant TP: 33.0\n","malignant TN: 6.0\n","malignant FP: 23.0\n","malignant FN: 6.0\n","\n","\n","\n","Epoch: 1  val \n","Loss: 0.5688  Acc: 68.7500\n","benign precision: 100.0000  recall: 28.5714\n","benign sensitivity: 28.5714  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 64.2857\n","benign TP: 2.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 5.0\n","malignant precision: 64.2857  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 28.5714\n","malignant FPR: 71.4286  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 2.0\n","malignant FP: 5.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 2/50\n","----------\n","Epoch: 2     train index of 5 minibatch: 1      time used: 1.8532402515411377\n","minibatch AVG loss: 0.5335982799530029\n","Epoch: 2     train index of 5 minibatch: 2      time used: 1.6444711685180664\n","minibatch AVG loss: 0.382857483625412\n","Epoch: 2     train index of 5 minibatch: 3      time used: 1.6440024375915527\n","minibatch AVG loss: 0.46208685636520386\n","\n","Epoch: 2  train \n","Loss: 0.4417  Acc: 82.6087\n","benign precision: 100.0000  recall: 63.3333\n","benign sensitivity: 63.3333  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 77.5510\n","benign TP: 19.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 11.0\n","malignant precision: 77.5510  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 63.3333\n","malignant FPR: 36.6667  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 19.0\n","malignant FP: 11.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 2  val \n","Loss: 0.4156  Acc: 93.7500\n","benign precision: 100.0000  recall: 85.7143\n","benign sensitivity: 85.7143  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 90.0000\n","benign TP: 6.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 90.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 85.7143\n","malignant FPR: 14.2857  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 6.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 3/50\n","----------\n","Epoch: 3     train index of 5 minibatch: 1      time used: 1.8560731410980225\n","minibatch AVG loss: 0.29889670610427854\n","Epoch: 3     train index of 5 minibatch: 2      time used: 1.6437854766845703\n","minibatch AVG loss: 0.3303549766540527\n","Epoch: 3     train index of 5 minibatch: 3      time used: 1.6443853378295898\n","minibatch AVG loss: 0.16896767914295197\n","\n","Epoch: 3  train \n","Loss: 0.2507  Acc: 95.6522\n","benign precision: 100.0000  recall: 93.1034\n","benign sensitivity: 93.1034  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 95.1220\n","benign TP: 27.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 2.0\n","malignant precision: 95.1220  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 93.1034\n","malignant FPR: 6.8966  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 27.0\n","malignant FP: 2.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 3  val \n","Loss: 0.1997  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 4/50\n","----------\n","Epoch: 4     train index of 5 minibatch: 1      time used: 1.8548202514648438\n","minibatch AVG loss: 0.17355067282915115\n","Epoch: 4     train index of 5 minibatch: 2      time used: 1.6438939571380615\n","minibatch AVG loss: 0.08528325557708741\n","Epoch: 4     train index of 5 minibatch: 3      time used: 1.6444051265716553\n","minibatch AVG loss: 0.09563354998826981\n","\n","Epoch: 4  train \n","Loss: 0.1228  Acc: 97.1014\n","benign precision: 100.0000  recall: 96.6667\n","benign sensitivity: 96.6667  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.4359\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.4359  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 96.6667\n","malignant FPR: 3.3333  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 4  val \n","Loss: 0.0940  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 5/50\n","----------\n","Epoch: 5     train index of 5 minibatch: 1      time used: 1.8522703647613525\n","minibatch AVG loss: 0.06049239672720432\n","Epoch: 5     train index of 5 minibatch: 2      time used: 1.6430346965789795\n","minibatch AVG loss: 0.0424632865935564\n","Epoch: 5     train index of 5 minibatch: 3      time used: 1.6428241729736328\n","minibatch AVG loss: 0.0634436534717679\n","\n","Epoch: 5  train \n","Loss: 0.0512  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 5  val \n","Loss: 0.0649  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 6/50\n","----------\n","Epoch: 6     train index of 5 minibatch: 1      time used: 1.8552989959716797\n","minibatch AVG loss: 0.05076751559972763\n","Epoch: 6     train index of 5 minibatch: 2      time used: 1.6429064273834229\n","minibatch AVG loss: 0.02428567074239254\n","Epoch: 6     train index of 5 minibatch: 3      time used: 1.6431171894073486\n","minibatch AVG loss: 0.029717652685940267\n","\n","Epoch: 6  train \n","Loss: 0.0337  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 6  val \n","Loss: 0.0457  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 7/50\n","----------\n","Epoch: 7     train index of 5 minibatch: 1      time used: 1.857666015625\n","minibatch AVG loss: 0.015558041445910931\n","Epoch: 7     train index of 5 minibatch: 2      time used: 1.6472830772399902\n","minibatch AVG loss: 0.019232392217963933\n","Epoch: 7     train index of 5 minibatch: 3      time used: 1.6442182064056396\n","minibatch AVG loss: 0.022699098475277422\n","\n","Epoch: 7  train \n","Loss: 0.0189  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 7  val \n","Loss: 0.0400  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 8/50\n","----------\n","Epoch: 8     train index of 5 minibatch: 1      time used: 1.8580129146575928\n","minibatch AVG loss: 0.018908510403707623\n","Epoch: 8     train index of 5 minibatch: 2      time used: 1.64375638961792\n","minibatch AVG loss: 0.02700494062155485\n","Epoch: 8     train index of 5 minibatch: 3      time used: 1.6447076797485352\n","minibatch AVG loss: 0.04383824425749481\n","\n","Epoch: 8  train \n","Loss: 0.0272  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 8  val \n","Loss: 0.0380  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 9/50\n","----------\n","Epoch: 9     train index of 5 minibatch: 1      time used: 1.8587727546691895\n","minibatch AVG loss: 0.07302062986418605\n","Epoch: 9     train index of 5 minibatch: 2      time used: 1.643686294555664\n","minibatch AVG loss: 0.00660850303247571\n","Epoch: 9     train index of 5 minibatch: 3      time used: 1.644028902053833\n","minibatch AVG loss: 0.016094424389302732\n","\n","Epoch: 9  train \n","Loss: 0.0284  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 9  val \n","Loss: 0.0369  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 10/50\n","----------\n","Epoch: 10     train index of 5 minibatch: 1      time used: 1.8599214553833008\n","minibatch AVG loss: 0.010192613769322633\n","Epoch: 10     train index of 5 minibatch: 2      time used: 1.6441633701324463\n","minibatch AVG loss: 0.02563717644661665\n","Epoch: 10     train index of 5 minibatch: 3      time used: 1.6428005695343018\n","minibatch AVG loss: 0.010452859848737717\n","\n","Epoch: 10  train \n","Loss: 0.0145  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 10  val \n","Loss: 0.0340  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 11/50\n","----------\n","Epoch: 11     train index of 5 minibatch: 1      time used: 1.8509225845336914\n","minibatch AVG loss: 0.007278477819636464\n","Epoch: 11     train index of 5 minibatch: 2      time used: 1.6426355838775635\n","minibatch AVG loss: 0.005212533194571734\n","Epoch: 11     train index of 5 minibatch: 3      time used: 1.6433641910552979\n","minibatch AVG loss: 0.006650672457180917\n","\n","Epoch: 11  train \n","Loss: 0.0071  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 11  val \n","Loss: 0.0313  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 12/50\n","----------\n","Epoch: 12     train index of 5 minibatch: 1      time used: 1.858672857284546\n","minibatch AVG loss: 0.015971773117780686\n","Epoch: 12     train index of 5 minibatch: 2      time used: 1.6433863639831543\n","minibatch AVG loss: 0.007661101291887462\n","Epoch: 12     train index of 5 minibatch: 3      time used: 1.6434054374694824\n","minibatch AVG loss: 0.009079899359494447\n","\n","Epoch: 12  train \n","Loss: 0.0150  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 12  val \n","Loss: 0.0277  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 13/50\n","----------\n","Epoch: 13     train index of 5 minibatch: 1      time used: 1.8581154346466064\n","minibatch AVG loss: 0.022509327344596385\n","Epoch: 13     train index of 5 minibatch: 2      time used: 1.6433582305908203\n","minibatch AVG loss: 0.008213443588465453\n","Epoch: 13     train index of 5 minibatch: 3      time used: 1.6436803340911865\n","minibatch AVG loss: 0.00905811374541372\n","\n","Epoch: 13  train \n","Loss: 0.0126  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 13  val \n","Loss: 0.0250  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 14/50\n","----------\n","Epoch: 14     train index of 5 minibatch: 1      time used: 1.8661963939666748\n","minibatch AVG loss: 0.007686625933274627\n","Epoch: 14     train index of 5 minibatch: 2      time used: 1.6446523666381836\n","minibatch AVG loss: 0.013884159736335278\n","Epoch: 14     train index of 5 minibatch: 3      time used: 1.6435847282409668\n","minibatch AVG loss: 0.013684043101966381\n","\n","Epoch: 14  train \n","Loss: 0.0133  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 14  val \n","Loss: 0.0229  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 15/50\n","----------\n","Epoch: 15     train index of 5 minibatch: 1      time used: 1.858931064605713\n","minibatch AVG loss: 0.0029830078361555933\n","Epoch: 15     train index of 5 minibatch: 2      time used: 1.6432406902313232\n","minibatch AVG loss: 0.0071982922032475475\n","Epoch: 15     train index of 5 minibatch: 3      time used: 1.6423118114471436\n","minibatch AVG loss: 0.03067235597409308\n","\n","Epoch: 15  train \n","Loss: 0.0128  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 15  val \n","Loss: 0.0229  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 16/50\n","----------\n","Epoch: 16     train index of 5 minibatch: 1      time used: 1.8559720516204834\n","minibatch AVG loss: 0.013706087390892208\n","Epoch: 16     train index of 5 minibatch: 2      time used: 1.6441845893859863\n","minibatch AVG loss: 0.08372750408016146\n","Epoch: 16     train index of 5 minibatch: 3      time used: 1.6446166038513184\n","minibatch AVG loss: 0.022224905784241855\n","\n","Epoch: 16  train \n","Loss: 0.0458  Acc: 97.1014\n","benign precision: 96.6667  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.4359\n","benign FPR: 2.5641  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 38.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.4359\n","malignant sensitivity: 97.4359  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.6667\n","malignant TP: 38.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 16  val \n","Loss: 0.0307  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 17/50\n","----------\n","Epoch: 17     train index of 5 minibatch: 1      time used: 1.8592913150787354\n","minibatch AVG loss: 0.016117722471244635\n","Epoch: 17     train index of 5 minibatch: 2      time used: 1.6442561149597168\n","minibatch AVG loss: 0.05507644908502698\n","Epoch: 17     train index of 5 minibatch: 3      time used: 1.6499958038330078\n","minibatch AVG loss: 0.06731396210379899\n","\n","Epoch: 17  train \n","Loss: 0.0405  Acc: 97.1014\n","benign precision: 96.7742  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.3684\n","benign FPR: 2.6316  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 37.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.3684\n","malignant sensitivity: 97.3684  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 96.7742\n","malignant TP: 37.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","\n","Epoch: 17  val \n","Loss: 0.0205  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 18/50\n","----------\n","Epoch: 18     train index of 5 minibatch: 1      time used: 1.8587570190429688\n","minibatch AVG loss: 0.023083095299080016\n","Epoch: 18     train index of 5 minibatch: 2      time used: 1.6433658599853516\n","minibatch AVG loss: 0.003101084858644754\n","Epoch: 18     train index of 5 minibatch: 3      time used: 1.6437268257141113\n","minibatch AVG loss: 0.013782363873906434\n","\n","Epoch: 18  train \n","Loss: 0.0121  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 18  val \n","Loss: 0.0111  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 19/50\n","----------\n","Epoch: 19     train index of 5 minibatch: 1      time used: 1.8631975650787354\n","minibatch AVG loss: 0.017692144913598894\n","Epoch: 19     train index of 5 minibatch: 2      time used: 1.6454627513885498\n","minibatch AVG loss: 0.003602686966769397\n","Epoch: 19     train index of 5 minibatch: 3      time used: 1.6454739570617676\n","minibatch AVG loss: 0.002539835579227656\n","\n","Epoch: 19  train \n","Loss: 0.0071  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 19  val \n","Loss: 0.0100  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 20/50\n","----------\n","Epoch: 20     train index of 5 minibatch: 1      time used: 1.8572702407836914\n","minibatch AVG loss: 0.002270322456024587\n","Epoch: 20     train index of 5 minibatch: 2      time used: 1.645815372467041\n","minibatch AVG loss: 0.0015559366438537836\n","Epoch: 20     train index of 5 minibatch: 3      time used: 1.6442899703979492\n","minibatch AVG loss: 0.021115351864136756\n","\n","Epoch: 20  train \n","Loss: 0.0084  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 20  val \n","Loss: 0.0251  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 21/50\n","----------\n","Epoch: 21     train index of 5 minibatch: 1      time used: 1.8612644672393799\n","minibatch AVG loss: 0.008943499345332385\n","Epoch: 21     train index of 5 minibatch: 2      time used: 1.6453893184661865\n","minibatch AVG loss: 0.007941583660431207\n","Epoch: 21     train index of 5 minibatch: 3      time used: 1.6454718112945557\n","minibatch AVG loss: 0.0034021793166175486\n","\n","Epoch: 21  train \n","Loss: 0.0061  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 21  val \n","Loss: 0.0102  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 22/50\n","----------\n","Epoch: 22     train index of 5 minibatch: 1      time used: 1.8614528179168701\n","minibatch AVG loss: 0.003155975684057921\n","Epoch: 22     train index of 5 minibatch: 2      time used: 1.644526481628418\n","minibatch AVG loss: 0.004498165706172586\n","Epoch: 22     train index of 5 minibatch: 3      time used: 1.643040418624878\n","minibatch AVG loss: 0.0016220621997490524\n","\n","Epoch: 22  train \n","Loss: 0.0036  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 22  val \n","Loss: 0.0104  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 23/50\n","----------\n","Epoch: 23     train index of 5 minibatch: 1      time used: 1.859367847442627\n","minibatch AVG loss: 0.005222468404099345\n","Epoch: 23     train index of 5 minibatch: 2      time used: 1.6457273960113525\n","minibatch AVG loss: 0.0013926191837526857\n","Epoch: 23     train index of 5 minibatch: 3      time used: 1.6450514793395996\n","minibatch AVG loss: 0.0015695383888669312\n","\n","Epoch: 23  train \n","Loss: 0.0026  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 23  val \n","Loss: 0.0104  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 24/50\n","----------\n","Epoch: 24     train index of 5 minibatch: 1      time used: 1.8612339496612549\n","minibatch AVG loss: 0.0018347359728068113\n","Epoch: 24     train index of 5 minibatch: 2      time used: 1.6446788311004639\n","minibatch AVG loss: 0.01575452125398442\n","Epoch: 24     train index of 5 minibatch: 3      time used: 1.6440746784210205\n","minibatch AVG loss: 0.0029622195288538933\n","\n","Epoch: 24  train \n","Loss: 0.0068  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 24  val \n","Loss: 0.0115  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 25/50\n","----------\n","Epoch: 25     train index of 5 minibatch: 1      time used: 1.8574326038360596\n","minibatch AVG loss: 0.007840053224936128\n","Epoch: 25     train index of 5 minibatch: 2      time used: 1.6439554691314697\n","minibatch AVG loss: 0.003886550571769476\n","Epoch: 25     train index of 5 minibatch: 3      time used: 1.645014762878418\n","minibatch AVG loss: 0.004136776376981288\n","\n","Epoch: 25  train \n","Loss: 0.0049  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 25  val \n","Loss: 0.0125  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 26/50\n","----------\n","Epoch: 26     train index of 5 minibatch: 1      time used: 1.8620100021362305\n","minibatch AVG loss: 0.003028896707110107\n","Epoch: 26     train index of 5 minibatch: 2      time used: 1.6436767578125\n","minibatch AVG loss: 0.0019992991350591184\n","Epoch: 26     train index of 5 minibatch: 3      time used: 1.6441373825073242\n","minibatch AVG loss: 0.0021093905437737704\n","\n","Epoch: 26  train \n","Loss: 0.0023  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 26  val \n","Loss: 0.0120  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 27/50\n","----------\n","Epoch: 27     train index of 5 minibatch: 1      time used: 1.8607439994812012\n","minibatch AVG loss: 0.002410520100966096\n","Epoch: 27     train index of 5 minibatch: 2      time used: 1.6430327892303467\n","minibatch AVG loss: 0.002599350502714515\n","Epoch: 27     train index of 5 minibatch: 3      time used: 1.645782232284546\n","minibatch AVG loss: 0.0019407328451052307\n","\n","Epoch: 27  train \n","Loss: 0.0021  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 27  val \n","Loss: 0.0118  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 28/50\n","----------\n","Epoch: 28     train index of 5 minibatch: 1      time used: 1.8534409999847412\n","minibatch AVG loss: 0.005187149834819138\n","Epoch: 28     train index of 5 minibatch: 2      time used: 1.6458933353424072\n","minibatch AVG loss: 0.0013403346296399833\n","Epoch: 28     train index of 5 minibatch: 3      time used: 1.643550157546997\n","minibatch AVG loss: 0.001772212260402739\n","\n","Epoch: 28  train \n","Loss: 0.0063  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 28  val \n","Loss: 0.0115  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 29/50\n","----------\n","Epoch: 29     train index of 5 minibatch: 1      time used: 1.8573508262634277\n","minibatch AVG loss: 0.003391714475583285\n","Epoch: 29     train index of 5 minibatch: 2      time used: 1.6438093185424805\n","minibatch AVG loss: 0.00831755220424384\n","Epoch: 29     train index of 5 minibatch: 3      time used: 1.6439602375030518\n","minibatch AVG loss: 0.009674365480896086\n","\n","Epoch: 29  train \n","Loss: 0.0065  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 29  val \n","Loss: 0.0111  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 30/50\n","----------\n","Epoch: 30     train index of 5 minibatch: 1      time used: 1.858670711517334\n","minibatch AVG loss: 0.002150799275841564\n","Epoch: 30     train index of 5 minibatch: 2      time used: 1.6439220905303955\n","minibatch AVG loss: 0.0017667853739112616\n","Epoch: 30     train index of 5 minibatch: 3      time used: 1.6432623863220215\n","minibatch AVG loss: 0.0010807873099111021\n","\n","Epoch: 30  train \n","Loss: 0.0016  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 30  val \n","Loss: 0.0109  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 31/50\n","----------\n","Epoch: 31     train index of 5 minibatch: 1      time used: 1.8560490608215332\n","minibatch AVG loss: 0.002140035934280604\n","Epoch: 31     train index of 5 minibatch: 2      time used: 1.644948959350586\n","minibatch AVG loss: 0.010662775323726236\n","Epoch: 31     train index of 5 minibatch: 3      time used: 1.6450097560882568\n","minibatch AVG loss: 0.0027769274543970822\n","\n","Epoch: 31  train \n","Loss: 0.0047  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 31  val \n","Loss: 0.0097  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 32/50\n","----------\n","Epoch: 32     train index of 5 minibatch: 1      time used: 1.86330246925354\n","minibatch AVG loss: 0.010096740233711899\n","Epoch: 32     train index of 5 minibatch: 2      time used: 1.6444740295410156\n","minibatch AVG loss: 0.0019501950941048563\n","Epoch: 32     train index of 5 minibatch: 3      time used: 1.6434118747711182\n","minibatch AVG loss: 0.0038176528643816708\n","\n","Epoch: 32  train \n","Loss: 0.0049  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 32  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 33/50\n","----------\n","Epoch: 33     train index of 5 minibatch: 1      time used: 1.8603136539459229\n","minibatch AVG loss: 0.005331573181319982\n","Epoch: 33     train index of 5 minibatch: 2      time used: 1.6442523002624512\n","minibatch AVG loss: 0.002922020002733916\n","Epoch: 33     train index of 5 minibatch: 3      time used: 1.64406418800354\n","minibatch AVG loss: 0.001568407006561756\n","\n","Epoch: 33  train \n","Loss: 0.0030  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 33  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 34/50\n","----------\n","Epoch: 34     train index of 5 minibatch: 1      time used: 1.863020896911621\n","minibatch AVG loss: 0.006684825057163835\n","Epoch: 34     train index of 5 minibatch: 2      time used: 1.6440839767456055\n","minibatch AVG loss: 0.007808436080813408\n","Epoch: 34     train index of 5 minibatch: 3      time used: 1.64304780960083\n","minibatch AVG loss: 0.001847143069608137\n","\n","Epoch: 34  train \n","Loss: 0.0053  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 34  val \n","Loss: 0.0091  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 35/50\n","----------\n","Epoch: 35     train index of 5 minibatch: 1      time used: 1.8539745807647705\n","minibatch AVG loss: 0.0015778456581756473\n","Epoch: 35     train index of 5 minibatch: 2      time used: 1.6440327167510986\n","minibatch AVG loss: 0.0024886560160666702\n","Epoch: 35     train index of 5 minibatch: 3      time used: 1.643932580947876\n","minibatch AVG loss: 0.0009361222328152508\n","\n","Epoch: 35  train \n","Loss: 0.0017  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 35  val \n","Loss: 0.0091  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 36/50\n","----------\n","Epoch: 36     train index of 5 minibatch: 1      time used: 1.860006332397461\n","minibatch AVG loss: 0.004009176744148135\n","Epoch: 36     train index of 5 minibatch: 2      time used: 1.652716875076294\n","minibatch AVG loss: 0.003081659565214068\n","Epoch: 36     train index of 5 minibatch: 3      time used: 1.6437079906463623\n","minibatch AVG loss: 0.0013680797768756748\n","\n","Epoch: 36  train \n","Loss: 0.0026  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 36  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 37/50\n","----------\n","Epoch: 37     train index of 5 minibatch: 1      time used: 1.856720209121704\n","minibatch AVG loss: 0.005016686674207449\n","Epoch: 37     train index of 5 minibatch: 2      time used: 1.6439545154571533\n","minibatch AVG loss: 0.0030671458691358565\n","Epoch: 37     train index of 5 minibatch: 3      time used: 1.6436471939086914\n","minibatch AVG loss: 0.004675325704738498\n","\n","Epoch: 37  train \n","Loss: 0.0038  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 37  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 38/50\n","----------\n","Epoch: 38     train index of 5 minibatch: 1      time used: 1.8683891296386719\n","minibatch AVG loss: 0.0014572059852071106\n","Epoch: 38     train index of 5 minibatch: 2      time used: 1.6444816589355469\n","minibatch AVG loss: 0.0016886112513020634\n","Epoch: 38     train index of 5 minibatch: 3      time used: 1.6439964771270752\n","minibatch AVG loss: 0.00706432294100523\n","\n","Epoch: 38  train \n","Loss: 0.0031  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 38  val \n","Loss: 0.0093  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 39/50\n","----------\n","Epoch: 39     train index of 5 minibatch: 1      time used: 1.8544480800628662\n","minibatch AVG loss: 0.0024869034299626947\n","Epoch: 39     train index of 5 minibatch: 2      time used: 1.6448819637298584\n","minibatch AVG loss: 0.0022738369647413493\n","Epoch: 39     train index of 5 minibatch: 3      time used: 1.6442673206329346\n","minibatch AVG loss: 0.001724421256221831\n","\n","Epoch: 39  train \n","Loss: 0.0019  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 39  val \n","Loss: 0.0093  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 40/50\n","----------\n","Epoch: 40     train index of 5 minibatch: 1      time used: 1.8547484874725342\n","minibatch AVG loss: 0.00213293528649956\n","Epoch: 40     train index of 5 minibatch: 2      time used: 1.6444056034088135\n","minibatch AVG loss: 0.001396857388317585\n","Epoch: 40     train index of 5 minibatch: 3      time used: 1.6464638710021973\n","minibatch AVG loss: 0.001959518645890057\n","\n","Epoch: 40  train \n","Loss: 0.0023  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 40  val \n","Loss: 0.0094  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 41/50\n","----------\n","Epoch: 41     train index of 5 minibatch: 1      time used: 1.853914499282837\n","minibatch AVG loss: 0.0029850064893253148\n","Epoch: 41     train index of 5 minibatch: 2      time used: 1.6434569358825684\n","minibatch AVG loss: 0.002448201982770115\n","Epoch: 41     train index of 5 minibatch: 3      time used: 1.6444640159606934\n","minibatch AVG loss: 0.0022067692247219385\n","\n","Epoch: 41  train \n","Loss: 0.0027  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 41  val \n","Loss: 0.0093  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 42/50\n","----------\n","Epoch: 42     train index of 5 minibatch: 1      time used: 1.853546142578125\n","minibatch AVG loss: 0.0018479476333595813\n","Epoch: 42     train index of 5 minibatch: 2      time used: 1.6451365947723389\n","minibatch AVG loss: 0.004504936654120684\n","Epoch: 42     train index of 5 minibatch: 3      time used: 1.645237684249878\n","minibatch AVG loss: 0.0025636263424530627\n","\n","Epoch: 42  train \n","Loss: 0.0030  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 42  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 43/50\n","----------\n","Epoch: 43     train index of 5 minibatch: 1      time used: 1.851348876953125\n","minibatch AVG loss: 0.0029308039520401508\n","Epoch: 43     train index of 5 minibatch: 2      time used: 1.6446692943572998\n","minibatch AVG loss: 0.0022234646719880404\n","Epoch: 43     train index of 5 minibatch: 3      time used: 1.6431808471679688\n","minibatch AVG loss: 0.004067040770314634\n","\n","Epoch: 43  train \n","Loss: 0.0030  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 43  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 44/50\n","----------\n","Epoch: 44     train index of 5 minibatch: 1      time used: 1.8588709831237793\n","minibatch AVG loss: 0.00119941511657089\n","Epoch: 44     train index of 5 minibatch: 2      time used: 1.6432127952575684\n","minibatch AVG loss: 0.004607482836581767\n","Epoch: 44     train index of 5 minibatch: 3      time used: 1.6434886455535889\n","minibatch AVG loss: 0.006387976102996617\n","\n","Epoch: 44  train \n","Loss: 0.0037  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 44  val \n","Loss: 0.0091  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 45/50\n","----------\n","Epoch: 45     train index of 5 minibatch: 1      time used: 1.8618335723876953\n","minibatch AVG loss: 0.00225739823654294\n","Epoch: 45     train index of 5 minibatch: 2      time used: 1.6454424858093262\n","minibatch AVG loss: 0.00198483950807713\n","Epoch: 45     train index of 5 minibatch: 3      time used: 1.6436779499053955\n","minibatch AVG loss: 0.005959616578184068\n","\n","Epoch: 45  train \n","Loss: 0.0033  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 45  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 46/50\n","----------\n","Epoch: 46     train index of 5 minibatch: 1      time used: 1.8657677173614502\n","minibatch AVG loss: 0.0019104673527181148\n","Epoch: 46     train index of 5 minibatch: 2      time used: 1.6456832885742188\n","minibatch AVG loss: 0.0024182277033105493\n","Epoch: 46     train index of 5 minibatch: 3      time used: 1.6451525688171387\n","minibatch AVG loss: 0.0023845041869208217\n","\n","Epoch: 46  train \n","Loss: 0.0021  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 46  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 47/50\n","----------\n","Epoch: 47     train index of 5 minibatch: 1      time used: 1.8598051071166992\n","minibatch AVG loss: 0.002006068255286664\n","Epoch: 47     train index of 5 minibatch: 2      time used: 1.6453394889831543\n","minibatch AVG loss: 0.006576228339690715\n","Epoch: 47     train index of 5 minibatch: 3      time used: 1.6438777446746826\n","minibatch AVG loss: 0.004070025053806603\n","\n","Epoch: 47  train \n","Loss: 0.0039  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 29.0\n","benign TN: 39.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 39.0\n","malignant TN: 29.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 47  val \n","Loss: 0.0092  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 48/50\n","----------\n","Epoch: 48     train index of 5 minibatch: 1      time used: 1.8738296031951904\n","minibatch AVG loss: 0.002643773570889607\n","Epoch: 48     train index of 5 minibatch: 2      time used: 1.6439738273620605\n","minibatch AVG loss: 0.0032638081756886095\n","Epoch: 48     train index of 5 minibatch: 3      time used: 1.646686315536499\n","minibatch AVG loss: 0.011094726040028036\n","\n","Epoch: 48  train \n","Loss: 0.0052  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 48  val \n","Loss: 0.0091  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 49/50\n","----------\n","Epoch: 49     train index of 5 minibatch: 1      time used: 1.853461742401123\n","minibatch AVG loss: 0.0020253096707165243\n","Epoch: 49     train index of 5 minibatch: 2      time used: 1.644531011581421\n","minibatch AVG loss: 0.00214318938087672\n","Epoch: 49     train index of 5 minibatch: 3      time used: 1.6433265209197998\n","minibatch AVG loss: 0.0021821254515089093\n","\n","Epoch: 49  train \n","Loss: 0.0021  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 49  val \n","Loss: 0.0091  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch 50/50\n","----------\n","Epoch: 50     train index of 5 minibatch: 1      time used: 1.8543026447296143\n","minibatch AVG loss: 0.0024636772344820202\n","Epoch: 50     train index of 5 minibatch: 2      time used: 1.6429719924926758\n","minibatch AVG loss: 0.003159262239933014\n","Epoch: 50     train index of 5 minibatch: 3      time used: 1.6447739601135254\n","minibatch AVG loss: 0.007337781286332756\n","\n","Epoch: 50  train \n","Loss: 0.0041  Acc: 98.5507\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 30.0\n","benign TN: 38.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 38.0\n","malignant TN: 30.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Epoch: 50  val \n","Loss: 0.0091  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 7.0\n","benign TN: 9.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 9.0\n","malignant TN: 7.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","\n","Training complete in 5m 24s\n","Best epoch idx:  50\n","Best epoch train Acc: 98.550725\n","Best epoch val Acc: 100.000000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","model trained by GPU (idx:0) has been saved at  /home/MIL_Experiment/saved_models/CLS_swin_b_384_401_PT_lf05_b4_warwick_CLS.pth\n"]}],"source":["!python Train.py --model_idx swin_b_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --batch_size 4 --lr 0.00001 --lrf 0.05 --enable_tensorboard --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"markdown","metadata":{"id":"9A2NqlIySHZo"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9a_q06xzYk-0"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[-1.3722, -0.9734]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.22563624382019043\n","minibatch AVG loss: 0.09176824444148224\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.13779091835021973\n","minibatch AVG loss: 0.0009323768361355178\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.13713765144348145\n","minibatch AVG loss: 0.002067203569549747\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.13699841499328613\n","minibatch AVG loss: 0.021840812209120486\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.13704538345336914\n","minibatch AVG loss: 0.0017317810368695063\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.13749098777770996\n","minibatch AVG loss: 0.0001763051704983809\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.13701581954956055\n","minibatch AVG loss: 0.3735541293535789\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.1379101276397705\n","minibatch AVG loss: 0.027708673732558965\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.13742589950561523\n","minibatch AVG loss: 0.007501245988532901\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.1369645595550537\n","minibatch AVG loss: 0.0536749026432517\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.13739585876464844\n","minibatch AVG loss: 0.0072741829324513676\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.1383836269378662\n","minibatch AVG loss: 0.018882102286443114\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.13787221908569336\n","minibatch AVG loss: 0.040750522132293555\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.1376025676727295\n","minibatch AVG loss: 0.16088467565241443\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.1381394863128662\n","minibatch AVG loss: 0.0004063732922077179\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.13686609268188477\n","minibatch AVG loss: 0.0010677237674826755\n","\n","Epoch:  test \n","Loss: 0.0506  Acc: 97.5000\n","benign precision: 97.2973  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 97.6744\n","benign FPR: 2.3256  NPV: 97.6744\n","benign TP: 36.0\n","benign TN: 42.0\n","benign FP: 1.0\n","benign FN: 1.0\n","malignant precision: 97.6744  recall: 97.6744\n","malignant sensitivity: 97.6744  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 97.2973\n","malignant TP: 42.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 1.0\n","\n","\n","Testing complete in 0m 28s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xz7xxOutaaQ0"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[ 0.9346, -0.2648]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.21093034744262695\n","minibatch AVG loss: 0.007510975871628034\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.13776469230651855\n","minibatch AVG loss: 8.856407225721341e-05\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.13725948333740234\n","minibatch AVG loss: 0.0489659838340117\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.13784098625183105\n","minibatch AVG loss: 0.0007625962636666373\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.13767123222351074\n","minibatch AVG loss: 0.37807564634638313\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.13758039474487305\n","minibatch AVG loss: 0.0004132322081886741\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.13812541961669922\n","minibatch AVG loss: 0.009279765693054286\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.1379711627960205\n","minibatch AVG loss: 0.028848671465300412\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.1373891830444336\n","minibatch AVG loss: 8.20820159788127e-05\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.13738775253295898\n","minibatch AVG loss: 0.006404839502624782\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.1379683017730713\n","minibatch AVG loss: 0.015586202660779236\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.13755273818969727\n","minibatch AVG loss: 0.04524223356143011\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.13764166831970215\n","minibatch AVG loss: 0.0014261563004765777\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.13797974586486816\n","minibatch AVG loss: 0.08339508761036854\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.13750028610229492\n","minibatch AVG loss: 4.839847183291113e-06\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.13723993301391602\n","minibatch AVG loss: 0.0028103104305387205\n","\n","Epoch:  test \n","Loss: 0.0393  Acc: 98.7500\n","benign precision: 100.0000  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.7273\n","benign TP: 36.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.7273  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 27s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Cutout_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7Qe7P80UatEv"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[0.1860, 0.7218]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.22057414054870605\n","minibatch AVG loss: 0.058738645538687705\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.13797235488891602\n","minibatch AVG loss: 0.01985793593339622\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.13815522193908691\n","minibatch AVG loss: 0.055238106253091245\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.13825416564941406\n","minibatch AVG loss: 0.07936022472567857\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.138474702835083\n","minibatch AVG loss: 0.08119792800862342\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.13820123672485352\n","minibatch AVG loss: 0.012220895127393305\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.13751816749572754\n","minibatch AVG loss: 0.1833937103394419\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.13820505142211914\n","minibatch AVG loss: 0.024810587428510188\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.1384739875793457\n","minibatch AVG loss: 0.06275262385606765\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.13867568969726562\n","minibatch AVG loss: 0.03870630073361099\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.13822293281555176\n","minibatch AVG loss: 0.03333145789802074\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.13780498504638672\n","minibatch AVG loss: 0.027109902631491423\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.13815712928771973\n","minibatch AVG loss: 0.035895587486447764\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.1374986171722412\n","minibatch AVG loss: 0.07015891140326858\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.13810181617736816\n","minibatch AVG loss: 0.006797495647333562\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.13923358917236328\n","minibatch AVG loss: 0.013446693494915962\n","\n","Epoch:  test \n","Loss: 0.0502  Acc: 98.7500\n","benign precision: 100.0000  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.7273\n","benign TP: 36.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.7273  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 27s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_CutMix_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4Hx39eVgatzK"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['convit_base',\n"," 'convit_small',\n"," 'convit_tiny',\n"," 'crossvit_9_240',\n"," 'crossvit_9_dagger_240',\n"," 'crossvit_15_240',\n"," 'crossvit_15_dagger_240',\n"," 'crossvit_15_dagger_408',\n"," 'crossvit_18_240',\n"," 'crossvit_18_dagger_240',\n"," 'crossvit_18_dagger_408',\n"," 'crossvit_base_240',\n"," 'crossvit_small_240',\n"," 'crossvit_tiny_240',\n"," 'levit_128',\n"," 'levit_128s',\n"," 'levit_192',\n"," 'levit_256',\n"," 'levit_384',\n"," 'vit_base_patch8_224',\n"," 'vit_base_patch8_224_in21k',\n"," 'vit_base_patch16_224',\n"," 'vit_base_patch16_224_in21k',\n"," 'vit_base_patch16_224_miil',\n"," 'vit_base_patch16_224_miil_in21k',\n"," 'vit_base_patch16_384',\n"," 'vit_base_patch16_sam_224',\n"," 'vit_base_patch32_224',\n"," 'vit_base_patch32_224_in21k',\n"," 'vit_base_patch32_384',\n"," 'vit_base_patch32_sam_224',\n"," 'vit_base_r26_s32_224',\n"," 'vit_base_r50_s16_224',\n"," 'vit_base_r50_s16_224_in21k',\n"," 'vit_base_r50_s16_384',\n"," 'vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224',\n"," 'vit_giant_patch14_224',\n"," 'vit_gigantic_patch14_224',\n"," 'vit_huge_patch14_224',\n"," 'vit_huge_patch14_224_in21k',\n"," 'vit_large_patch16_224',\n"," 'vit_large_patch16_224_in21k',\n"," 'vit_large_patch16_384',\n"," 'vit_large_patch32_224',\n"," 'vit_large_patch32_224_in21k',\n"," 'vit_large_patch32_384',\n"," 'vit_large_r50_s32_224',\n"," 'vit_large_r50_s32_224_in21k',\n"," 'vit_large_r50_s32_384',\n"," 'vit_small_patch16_224',\n"," 'vit_small_patch16_224_in21k',\n"," 'vit_small_patch16_384',\n"," 'vit_small_patch32_224',\n"," 'vit_small_patch32_224_in21k',\n"," 'vit_small_patch32_384',\n"," 'vit_small_r26_s32_224',\n"," 'vit_small_r26_s32_224_in21k',\n"," 'vit_small_r26_s32_384',\n"," 'vit_small_resnet26d_224',\n"," 'vit_small_resnet50d_s16_224',\n"," 'vit_tiny_patch16_224',\n"," 'vit_tiny_patch16_224_in21k',\n"," 'vit_tiny_patch16_384',\n"," 'vit_tiny_r_s16_p8_224',\n"," 'vit_tiny_r_s16_p8_224_in21k',\n"," 'vit_tiny_r_s16_p8_384']\n","test model output： tensor([[-0.0531, -0.1556]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.21828150749206543\n","minibatch AVG loss: 0.03782573007047176\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.137528657913208\n","minibatch AVG loss: 0.017928684409707785\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.137237548828125\n","minibatch AVG loss: 0.10728546411264687\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.1375575065612793\n","minibatch AVG loss: 0.07907037734985352\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.13698673248291016\n","minibatch AVG loss: 0.029902813443914055\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.13791108131408691\n","minibatch AVG loss: 0.037616953486576676\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.1371142864227295\n","minibatch AVG loss: 0.13029025066643954\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.13721394538879395\n","minibatch AVG loss: 0.0747137213125825\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.13760066032409668\n","minibatch AVG loss: 0.06948955096304417\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.1372525691986084\n","minibatch AVG loss: 0.07421436056029052\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.13710761070251465\n","minibatch AVG loss: 0.05738993007689715\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.13699674606323242\n","minibatch AVG loss: 0.05012065954506397\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.13794684410095215\n","minibatch AVG loss: 0.028609728999435903\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.13784146308898926\n","minibatch AVG loss: 0.04994609029963613\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.137923002243042\n","minibatch AVG loss: 0.017365395673550665\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.13762164115905762\n","minibatch AVG loss: 0.0034146551508456467\n","\n","Epoch:  test \n","Loss: 0.0541  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 37.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 37.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 27s\n"]}],"source":["!python Test.py --model_idx ViT_384_401_PT_lf05_b4_warwick_Mixup_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RtBaK8ZdL1X7"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","test model output： tensor([[0.5564, 0.5691]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : ResNet50_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ResNet50_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.16187763214111328\n","minibatch AVG loss: 0.22278977558016777\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.053488969802856445\n","minibatch AVG loss: 0.017096895864233373\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.05356121063232422\n","minibatch AVG loss: 0.1253877777431626\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.05483365058898926\n","minibatch AVG loss: 0.10827808601316065\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.05495166778564453\n","minibatch AVG loss: 0.019286938080040273\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.0533907413482666\n","minibatch AVG loss: 0.014101969930925407\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.05309605598449707\n","minibatch AVG loss: 0.07436683409614489\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.05384397506713867\n","minibatch AVG loss: 0.011575226639979518\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.053290367126464844\n","minibatch AVG loss: 0.03404477573931217\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.05257463455200195\n","minibatch AVG loss: 0.09629004690796136\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.05340385437011719\n","minibatch AVG loss: 0.034095611050724985\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.05261087417602539\n","minibatch AVG loss: 0.014588680677115917\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.053762197494506836\n","minibatch AVG loss: 0.01506814882159233\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.05249309539794922\n","minibatch AVG loss: 0.021200632490217686\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.05339479446411133\n","minibatch AVG loss: 0.026342281140387058\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.05074167251586914\n","minibatch AVG loss: 0.012437207624316216\n","\n","Epoch:  test \n","Loss: 0.0529  Acc: 98.7500\n","benign precision: 100.0000  recall: 97.2973\n","benign sensitivity: 97.2973  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 97.7273\n","benign TP: 36.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 1.0\n","malignant precision: 97.7273  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 97.2973\n","malignant FPR: 2.7027  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 36.0\n","malignant FP: 1.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 24s\n"]}],"source":["!python Test.py --model_idx ResNet50_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cpO0kpfrSLNH"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['vit_base_resnet26d_224',\n"," 'vit_base_resnet50_224_in21k',\n"," 'vit_base_resnet50_384',\n"," 'vit_base_resnet50d_224']\n","test model output： tensor([[-0.3735,  0.1534]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.3152658939361572\n","minibatch AVG loss: 0.007429265615064651\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.1901226043701172\n","minibatch AVG loss: 0.0029483516933396458\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.1940619945526123\n","minibatch AVG loss: 0.0018463320389855652\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.19164824485778809\n","minibatch AVG loss: 0.0062236236524768175\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.19336676597595215\n","minibatch AVG loss: 0.007043305377010256\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.1916663646697998\n","minibatch AVG loss: 0.0012861140654422342\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.1920177936553955\n","minibatch AVG loss: 0.010265306627843529\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.19195151329040527\n","minibatch AVG loss: 0.035671225469559434\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.19212961196899414\n","minibatch AVG loss: 0.006228194932918996\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.19271302223205566\n","minibatch AVG loss: 0.05409265346825123\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.19216561317443848\n","minibatch AVG loss: 0.0041123350500129165\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.19345331192016602\n","minibatch AVG loss: 0.003303693630732596\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.19277477264404297\n","minibatch AVG loss: 0.009641842846758664\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.19370794296264648\n","minibatch AVG loss: 0.02962793202023022\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.19175291061401367\n","minibatch AVG loss: 0.00162131012766622\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.1935274600982666\n","minibatch AVG loss: 0.0025440572993829847\n","\n","Epoch:  test \n","Loss: 0.0115  Acc: 100.0000\n","benign precision: 100.0000  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 100.0000\n","benign FPR: 0.0000  NPV: 100.0000\n","benign TP: 37.0\n","benign TN: 43.0\n","benign FP: 0.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 100.0000\n","malignant sensitivity: 100.0000  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 100.0000\n","malignant TP: 43.0\n","malignant TN: 37.0\n","malignant FP: 0.0\n","malignant FN: 0.0\n","\n","\n","Testing complete in 0m 29s\n"]}],"source":["!python Test.py --model_idx ResN50_ViT_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WIrMFuz5SLd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['efficientnet_b0',\n"," 'efficientnet_b1',\n"," 'efficientnet_b1_pruned',\n"," 'efficientnet_b2',\n"," 'efficientnet_b2_pruned',\n"," 'efficientnet_b2a',\n"," 'efficientnet_b3',\n"," 'efficientnet_b3_pruned',\n"," 'efficientnet_b3a',\n"," 'efficientnet_b4',\n"," 'efficientnet_b5',\n"," 'efficientnet_b6',\n"," 'efficientnet_b7',\n"," 'efficientnet_b8',\n"," 'efficientnet_cc_b0_4e',\n"," 'efficientnet_cc_b0_8e',\n"," 'efficientnet_cc_b1_8e',\n"," 'efficientnet_el',\n"," 'efficientnet_el_pruned',\n"," 'efficientnet_em',\n"," 'efficientnet_es',\n"," 'efficientnet_es_pruned',\n"," 'efficientnet_l2',\n"," 'efficientnet_lite0',\n"," 'efficientnet_lite1',\n"," 'efficientnet_lite2',\n"," 'efficientnet_lite3',\n"," 'efficientnet_lite4',\n"," 'efficientnetv2_l',\n"," 'efficientnetv2_m',\n"," 'efficientnetv2_rw_m',\n"," 'efficientnetv2_rw_s',\n"," 'efficientnetv2_rw_t',\n"," 'efficientnetv2_s',\n"," 'efficientnetv2_xl',\n"," 'gc_efficientnetv2_rw_t',\n"," 'tf_efficientnet_b0',\n"," 'tf_efficientnet_b0_ap',\n"," 'tf_efficientnet_b0_ns',\n"," 'tf_efficientnet_b1',\n"," 'tf_efficientnet_b1_ap',\n"," 'tf_efficientnet_b1_ns',\n"," 'tf_efficientnet_b2',\n"," 'tf_efficientnet_b2_ap',\n"," 'tf_efficientnet_b2_ns',\n"," 'tf_efficientnet_b3',\n"," 'tf_efficientnet_b3_ap',\n"," 'tf_efficientnet_b3_ns',\n"," 'tf_efficientnet_b4',\n"," 'tf_efficientnet_b4_ap',\n"," 'tf_efficientnet_b4_ns',\n"," 'tf_efficientnet_b5',\n"," 'tf_efficientnet_b5_ap',\n"," 'tf_efficientnet_b5_ns',\n"," 'tf_efficientnet_b6',\n"," 'tf_efficientnet_b6_ap',\n"," 'tf_efficientnet_b6_ns',\n"," 'tf_efficientnet_b7',\n"," 'tf_efficientnet_b7_ap',\n"," 'tf_efficientnet_b7_ns',\n"," 'tf_efficientnet_b8',\n"," 'tf_efficientnet_b8_ap',\n"," 'tf_efficientnet_cc_b0_4e',\n"," 'tf_efficientnet_cc_b0_8e',\n"," 'tf_efficientnet_cc_b1_8e',\n"," 'tf_efficientnet_el',\n"," 'tf_efficientnet_em',\n"," 'tf_efficientnet_es',\n"," 'tf_efficientnet_l2_ns',\n"," 'tf_efficientnet_l2_ns_475',\n"," 'tf_efficientnet_lite0',\n"," 'tf_efficientnet_lite1',\n"," 'tf_efficientnet_lite2',\n"," 'tf_efficientnet_lite3',\n"," 'tf_efficientnet_lite4',\n"," 'tf_efficientnetv2_b0',\n"," 'tf_efficientnetv2_b1',\n"," 'tf_efficientnetv2_b2',\n"," 'tf_efficientnetv2_b3',\n"," 'tf_efficientnetv2_l',\n"," 'tf_efficientnetv2_l_in21ft1k',\n"," 'tf_efficientnetv2_l_in21k',\n"," 'tf_efficientnetv2_m',\n"," 'tf_efficientnetv2_m_in21ft1k',\n"," 'tf_efficientnetv2_m_in21k',\n"," 'tf_efficientnetv2_s',\n"," 'tf_efficientnetv2_s_in21ft1k',\n"," 'tf_efficientnetv2_s_in21k',\n"," 'tf_efficientnetv2_xl_in21ft1k',\n"," 'tf_efficientnetv2_xl_in21k']\n","test model output： tensor([[ 7.5354, -3.5428]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.2135155200958252\n","minibatch AVG loss: 0.5215031054882274\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.09243083000183105\n","minibatch AVG loss: 0.1298586948258162\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.09270954132080078\n","minibatch AVG loss: 0.06554969771968899\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.09218001365661621\n","minibatch AVG loss: 0.11479129676008597\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.0936276912689209\n","minibatch AVG loss: 1.5222656439058482\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.09638500213623047\n","minibatch AVG loss: 0.06865397993824444\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.09301257133483887\n","minibatch AVG loss: 0.3061022201552987\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.09460687637329102\n","minibatch AVG loss: 0.8368458185344935\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.09165406227111816\n","minibatch AVG loss: 0.003368789699743502\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.09029650688171387\n","minibatch AVG loss: 0.5544158175587655\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.09245729446411133\n","minibatch AVG loss: 0.11754333201497502\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.09145498275756836\n","minibatch AVG loss: 0.0027437257578640127\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.08929300308227539\n","minibatch AVG loss: 0.6439336534589529\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.09163951873779297\n","minibatch AVG loss: 1.2581457749183755\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.08949398994445801\n","minibatch AVG loss: 0.035516668623313305\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.08871221542358398\n","minibatch AVG loss: 0.132804923126605\n","\n","Epoch:  test \n","Loss: 0.3946  Acc: 88.7500\n","benign precision: 88.8889  recall: 86.4865\n","benign sensitivity: 86.4865  specificity: 90.6977\n","benign FPR: 9.3023  NPV: 88.6364\n","benign TP: 32.0\n","benign TN: 39.0\n","benign FP: 4.0\n","benign FN: 5.0\n","malignant precision: 88.6364  recall: 90.6977\n","malignant sensitivity: 90.6977  specificity: 86.4865\n","malignant FPR: 13.5135  NPV: 88.8889\n","malignant TP: 39.0\n","malignant TN: 32.0\n","malignant FP: 5.0\n","malignant FN: 4.0\n","\n","\n","Testing complete in 0m 26s\n"]}],"source":["!python Test.py --model_idx efficientnet_b3_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oEQ8PIN4TUxi"},"outputs":[{"name":"stdout","output_type":"stream","text":["class_names: ['benign', 'malignant']\n","['swin_base_patch4_window7_224',\n"," 'swin_base_patch4_window7_224_in22k',\n"," 'swin_base_patch4_window12_384',\n"," 'swin_base_patch4_window12_384_in22k',\n"," 'swin_large_patch4_window7_224',\n"," 'swin_large_patch4_window7_224_in22k',\n"," 'swin_large_patch4_window12_384',\n"," 'swin_large_patch4_window12_384_in22k',\n"," 'swin_small_patch4_window7_224',\n"," 'swin_tiny_patch4_window7_224']\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","test model output： tensor([[ 0.1083, -0.0856]], grad_fn=\u003cAddmmBackward0\u003e)\n","model is ready now!\n","model loaded\n","model : swin_b_384_401_PT_lf05_b4_warwick_CLS\n","*********************************setting*************************************\n","Namespace(MIL_Stripe=False, att_module='SimAM', attn_drop_rate=0.0, batch_size=1, check_minibatch=5, cls_token_off=False, data_augmentation_mode=2, dataroot='/data/MIL_Experiment/dataset/warwick_CLS', draw_root='/home/MIL_Experiment/runs', drop_path_rate=0.0, drop_rate=0.0, edge_size=384, enable_attention_check=True, enable_notify=False, enable_tensorboard=False, enable_visualize_check=False, gpu_idx=0, model_idx='swin_b_384_401_PT_lf05_b4_warwick_CLS', model_path='/home/MIL_Experiment/saved_models', num_classes=0, paint=True, pos_embedding_off=False)\n","Epoch: Test\n","----------\n","Epoch: test     test index of 5 minibatch: 1      time used: 0.2797739505767822\n","minibatch AVG loss: 0.017978001246228816\n","/home/MIL_Experiment/code/utils/visual_usage.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return sof(x)\n","Epoch: test     test index of 5 minibatch: 2      time used: 0.16683244705200195\n","minibatch AVG loss: 0.0034912224975414572\n","Epoch: test     test index of 5 minibatch: 3      time used: 0.16642427444458008\n","minibatch AVG loss: 0.012001858046278357\n","Epoch: test     test index of 5 minibatch: 4      time used: 0.16638803482055664\n","minibatch AVG loss: 0.007737676613032818\n","Epoch: test     test index of 5 minibatch: 5      time used: 0.16611933708190918\n","minibatch AVG loss: 0.0053524697665125135\n","Epoch: test     test index of 5 minibatch: 6      time used: 0.16695046424865723\n","minibatch AVG loss: 0.0022519342368468643\n","Epoch: test     test index of 5 minibatch: 7      time used: 0.16624140739440918\n","minibatch AVG loss: 0.015454491134732962\n","Epoch: test     test index of 5 minibatch: 8      time used: 0.1668992042541504\n","minibatch AVG loss: 0.008493821043521166\n","Epoch: test     test index of 5 minibatch: 9      time used: 0.16627860069274902\n","minibatch AVG loss: 0.02898181991185993\n","Epoch: test     test index of 5 minibatch: 10      time used: 0.16598176956176758\n","minibatch AVG loss: 0.05005248293746263\n","Epoch: test     test index of 5 minibatch: 11      time used: 0.16707873344421387\n","minibatch AVG loss: 0.007098094467073679\n","Epoch: test     test index of 5 minibatch: 12      time used: 0.16675829887390137\n","minibatch AVG loss: 0.009018663596361876\n","Epoch: test     test index of 5 minibatch: 13      time used: 0.16646146774291992\n","minibatch AVG loss: 0.065934827120509\n","Epoch: test     test index of 5 minibatch: 14      time used: 0.1659698486328125\n","minibatch AVG loss: 0.631210764683783\n","Epoch: test     test index of 5 minibatch: 15      time used: 0.16660094261169434\n","minibatch AVG loss: 0.002017776819411665\n","Epoch: test     test index of 5 minibatch: 16      time used: 0.16611766815185547\n","minibatch AVG loss: 0.08343644351698458\n","\n","Epoch:  test \n","Loss: 0.0594  Acc: 98.7500\n","benign precision: 97.3684  recall: 100.0000\n","benign sensitivity: 100.0000  specificity: 97.6744\n","benign FPR: 2.3256  NPV: 100.0000\n","benign TP: 37.0\n","benign TN: 42.0\n","benign FP: 1.0\n","benign FN: 0.0\n","malignant precision: 100.0000  recall: 97.6744\n","malignant sensitivity: 97.6744  specificity: 100.0000\n","malignant FPR: 0.0000  NPV: 97.3684\n","malignant TP: 42.0\n","malignant TN: 37.0\n","malignant FP: 0.0\n","malignant FN: 1.0\n","\n","\n","Testing complete in 0m 29s\n"]}],"source":["!python Test.py --model_idx swin_b_384_401_PT_lf05_b4_warwick_CLS --edge_size 384 --data_augmentation_mode 2 --enable_attention_check --check_minibatch 5 --dataroot /data/MIL_Experiment/dataset/warwick_CLS --model_path /home/MIL_Experiment/saved_models --draw_root /home/MIL_Experiment/runs"]},{"cell_type":"markdown","metadata":{"id":"32lV43PnKVJx"},"source":["# Synchronize files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mCZDUffdQep4"},"outputs":[],"source":["# change working dir\n","import os\n","os.chdir(\"/home/MIL_Experiment/code/utils\")\n","!python check_log_json.py --draw_root /home/MIL_Experiment/runs --record_dir /home/MIL_Experiment/CSV_logs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_Wx0ymiiEuyS"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘/content/drive/MyDrive/MIL_SI_sample’: File exists\n","results copy completed!\n"]}],"source":["# create path on google drive\n","!mkdir /content/drive/MyDrive/MIL_SI_sample\n","# copy the results\n","!/bin/cp -rf /home/MIL_Experiment/* /content/drive/MyDrive/MIL_SI_sample/\n","print('results copy completed!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9lzAtLIhnGe5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Aug 15 19:05:26 UTC 2022\n"]}],"source":["!date --date='+8 hour'  # CST time zone"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"OpenSourse Sample CLS warwick_Experiment 384 401 lf05 Counterparts Train.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}